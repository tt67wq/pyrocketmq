import concurrent.futures
import threading
import time
from concurrent.futures import Future, ThreadPoolExecutor

# pyrocketmqå¯¼å…¥
from datetime import datetime
from typing import Any

from pyrocketmq.broker import BrokerClient, MessagePullError
from pyrocketmq.consumer.allocate_queue_strategy import (
    AllocateContext,
    AllocateQueueStrategyFactory,
)
from pyrocketmq.consumer.base_consumer import BaseConsumer
from pyrocketmq.consumer.config import ConsumerConfig
from pyrocketmq.consumer.consume_from_where_manager import ConsumeFromWhereManager
from pyrocketmq.consumer.errors import (
    ConsumerShutdownError,
    ConsumerStartError,
    MessageConsumeError,
)
from pyrocketmq.consumer.offset_store import ReadOffsetType
from pyrocketmq.consumer.process_queue import ProcessQueue
from pyrocketmq.logging import get_logger
from pyrocketmq.model import (
    ConsumeMessageDirectlyHeader,
    ConsumeMessageDirectlyResult,
    ConsumeResult,
    MessageExt,
    MessageModel,
    MessageQueue,
    PullMessageResult,
    RemotingCommand,
    RemotingCommandBuilder,
    RequestCode,
    ResponseCode,
    SubscriptionData,
    SubscriptionEntry,
)
from pyrocketmq.remote import ConnectionPool

logger = get_logger(__name__)


class OrderlyConsumer(BaseConsumer):
    def __init__(self, config: ConsumerConfig) -> None:
        super().__init__(config)

        # åˆ›å»ºæ¶ˆè´¹èµ·å§‹ä½ç½®ç®¡ç†å™¨
        self._consume_from_where_manager: ConsumeFromWhereManager = (
            ConsumeFromWhereManager(
                consume_group=self._config.consumer_group,
                namesrv_manager=self._name_server_manager,
                broker_manager=self._broker_manager,
            )
        )

        # åˆ›å»ºé˜Ÿåˆ—åˆ†é…ç­–ç•¥
        self._allocate_strategy = AllocateQueueStrategyFactory.create_strategy(
            self._config.allocate_queue_strategy
        )

        # çº¿ç¨‹æ± å’Œé˜Ÿåˆ—ç®¡ç†
        # é¡ºåºæ¶ˆæ¯æ¶ˆè´¹è€…æ¯ä¸ªmessage_queueä¸€ä¸ªæ¶ˆè´¹çº¿ç¨‹
        self._consume_tasks: dict[MessageQueue, list[Future[None]]] = {}
        self._consume_executor: ThreadPoolExecutor | None = None  # ç”¨äºç®¡ç†æ¶ˆè´¹ä»»åŠ¡
        self._pull_executor: ThreadPoolExecutor | None = None

        # åˆå§‹åŒ–æ¯ä¸ªmessage_queueçš„é”ç®¡ç†å­—æ®µ
        # æ¯ä¸ªqueueçš„é”éƒ½æœ‰æ—¶é—´é™åˆ¶ï¼Œæ”¯æŒis_lock_expiredæ–¹æ³•
        self._queue_locks: dict[MessageQueue, threading.RLock] = {}
        self._queue_lock_management_lock = threading.Lock()  # ğŸ”ä¿æŠ¤_queue_lockså­—å…¸

        # æ¶ˆæ¯ç¼“å­˜ç®¡ç† - ä½¿ç”¨ProcessQueueè§£å†³å¹¶å‘æ¶ˆè´¹åç§»é‡é—®é¢˜
        # ProcessQueueæ”¯æŒé«˜æ•ˆçš„insert/remove/min/max/countè®¡ç®—
        # è¿˜èƒ½ç»Ÿè®¡MessageExtçš„bodyæ€»ä½“ç§¯ï¼Œæä¾›æ›´å¥½çš„æ€§èƒ½
        self._msg_cache: dict[MessageQueue, ProcessQueue] = {}
        self._cache_lock = threading.Lock()  # ç”¨äºä¿æŠ¤_msg_cacheå­—å…¸

        # çŠ¶æ€ç®¡ç†
        self._pull_tasks: dict[MessageQueue, Future[None]] = {}
        self._assigned_queues: dict[MessageQueue, int] = {}  # queue -> last_offset
        self._assigned_queues_lock = (
            threading.RLock()
        )  # ğŸ”ä¿æŠ¤_assigned_queueså­—å…¸çš„å¹¶å‘è®¿é—®
        self._last_rebalance_time: float = 0.0

        # é‡å¹³è¡¡ä»»åŠ¡ç®¡ç†
        self._rebalance_thread: threading.Thread | None = None
        self._rebalance_interval: float = 20.0  # é‡å¹³è¡¡é—´éš”(ç§’)

        # çº¿ç¨‹åŒæ­¥äº‹ä»¶
        self._rebalance_event: threading.Event = (
            threading.Event()
        )  # ç”¨äºé‡å¹³è¡¡å¾ªç¯çš„äº‹ä»¶

        # çº¿ç¨‹åœæ­¢äº‹ä»¶ - ç”¨äºä¼˜é›…å…³é—­æ‹‰å–å’Œæ¶ˆè´¹å¾ªç¯
        self._pull_stop_events: dict[MessageQueue, threading.Event] = {}
        self._consume_stop_events: dict[MessageQueue, threading.Event] = {}
        self._stop_events_lock = threading.Lock()  # ä¿æŠ¤åœæ­¢äº‹ä»¶å­—å…¸

        # é‡å¹³è¡¡é‡å…¥ä¿æŠ¤
        self._rebalance_lock: threading.RLock = threading.RLock()  # é‡å¹³è¡¡é”ï¼Œé˜²æ­¢é‡å…¥

        # è¿œç¨‹é”ç¼“å­˜å’Œæœ‰æ•ˆæœŸç®¡ç†
        # é¿å…æ¯æ¬¡æ¶ˆè´¹å¾ªç¯éƒ½éœ€è¦è·å–è¿œç¨‹é”ï¼Œæå‡æ€§èƒ½
        self._remote_lock_cache: dict[
            MessageQueue, float
        ] = {}  # queue -> lock_expiry_time
        self._remote_lock_cache_lock = threading.Lock()  # ä¿æŠ¤è¿œç¨‹é”ç¼“å­˜
        self._remote_lock_expire_time: float = 30.0  # è¿œç¨‹é”æœ‰æ•ˆæœŸ30ç§’

        logger.info(
            "OrderlyConsumer initialized",
            extra={
                "consumer_group": self._config.consumer_group,
                "message_model": self._config.message_model,
                "consume_thread_max": self._config.consume_thread_max,
                "pull_batch_size": self._config.pull_batch_size,
                "remote_lock_expire_time": self._remote_lock_expire_time,
            },
        )

    def start(self) -> None:
        """å¯åŠ¨é¡ºåºæ¶ˆè´¹è€…ã€‚

        åˆå§‹åŒ–å¹¶å¯åŠ¨æ¶ˆè´¹è€…çš„æ‰€æœ‰ç»„ä»¶ï¼ŒåŒ…æ‹¬ï¼š
        - å»ºç«‹ä¸NameServerå’ŒBrokerçš„ç½‘ç»œè¿æ¥
        - åˆ›å»ºæ¶ˆæ¯æ‹‰å–å’Œå¤„ç†çº¿ç¨‹æ± 
        - æ‰§è¡Œåˆå§‹é˜Ÿåˆ—åˆ†é…å’Œé‡å¹³è¡¡
        - å¯åŠ¨å¿ƒè·³å’Œé‡å¹³è¡¡åå°ä»»åŠ¡

        å¯åŠ¨å¤±è´¥æ—¶ä¼šè‡ªåŠ¨æ¸…ç†å·²åˆ†é…çš„èµ„æºã€‚

        Raises:
            ConsumerStartError: å½“ä»¥ä¸‹æƒ…å†µå‘ç”Ÿæ—¶æŠ›å‡ºï¼š
                - æœªæ³¨å†Œæ¶ˆæ¯ç›‘å¬å™¨
                - æ¶ˆæ¯ç›‘å¬å™¨ç±»å‹ä¸åŒ¹é…ï¼ˆéœ€è¦MessageListenerï¼‰
                - ç½‘ç»œè¿æ¥å¤±è´¥
                - çº¿ç¨‹æ± åˆ›å»ºå¤±è´¥
                - å…¶ä»–åˆå§‹åŒ–é”™è¯¯

        Note:
            æ­¤æ–¹æ³•æ˜¯çº¿ç¨‹å®‰å…¨çš„ï¼Œå¤šæ¬¡è°ƒç”¨åªä¼šå¯åŠ¨ä¸€æ¬¡ã€‚
            å¯åŠ¨æˆåŠŸåï¼Œæ¶ˆè´¹è€…ä¼šè‡ªåŠ¨å¼€å§‹æ‹‰å–å’Œå¤„ç†æ¶ˆæ¯ã€‚
        """
        with self._lock:
            if self._is_running:
                logger.warning("Consumer is already running")
                return

            try:
                logger.info(
                    "Starting OrderlyConsumer",
                    extra={
                        "consumer_group": self._config.consumer_group,
                        "namesrv_addr": self._config.namesrv_addr,
                    },
                )

                # å¯åŠ¨BaseConsumer
                super().start()

                # åˆ›å»ºçº¿ç¨‹æ± 
                max_workers: int = self._config.consume_thread_max
                pull_workers: int = min(self._config.consume_thread_max, 10)
                self._consume_executor = ThreadPoolExecutor(
                    max_workers=max_workers,
                    thread_name_prefix=f"consume-{self._config.consumer_group}",
                )
                self._pull_executor = ThreadPoolExecutor(
                    max_workers=pull_workers,
                    thread_name_prefix=f"pull-{self._config.consumer_group}",
                )

                self._do_rebalance()

                # å¯åŠ¨é‡å¹³è¡¡ä»»åŠ¡
                self._start_rebalance_task()

                self._stats["start_time"] = time.time()

                with self._assigned_queues_lock:  # ğŸ”ä¿æŠ¤_assigned_queuesè®¿é—®
                    assigned_queues_count = len(self._assigned_queues)

                logger.info(
                    "OrderlyConsumer started successfully",
                    extra={
                        "consumer_group": self._config.consumer_group,
                        "assigned_queues": assigned_queues_count,
                        "consume_threads": self._config.consume_thread_max,
                        "pull_threads": min(self._config.consume_thread_max, 10),
                    },
                )

            except Exception as e:
                logger.error(
                    f"Failed to start OrderlyConsumer: {e}",
                    extra={
                        "consumer_group": self._config.consumer_group,
                        "error": str(e),
                    },
                    exc_info=True,
                )
                self._cleanup_on_start_failure()
                raise ConsumerStartError(
                    "Failed to start OrderlyConsumer",
                    cause=e,
                    context={"consumer_group": self._config.consumer_group},
                ) from e

    def shutdown(self) -> None:
        """ä¼˜é›…åœæ­¢é¡ºåºæ¶ˆè´¹è€…ã€‚

        æ‰§è¡Œä»¥ä¸‹å…³é—­æµç¨‹ï¼š
        1. åœæ­¢æ¥å—æ–°çš„æ¶ˆæ¯æ‹‰å–è¯·æ±‚
        2. ç­‰å¾…æ­£åœ¨å¤„ç†çš„æ¶ˆæ¯å®Œæˆï¼ˆæœ€å¤šç­‰å¾…30ç§’ï¼‰
        3. æŒä¹…åŒ–æ‰€æœ‰é˜Ÿåˆ—çš„æ¶ˆè´¹åç§»é‡
        4. å…³é—­æ‰€æœ‰çº¿ç¨‹æ± å’Œåå°ä»»åŠ¡
        5. æ¸…ç†ç½‘ç»œè¿æ¥å’Œèµ„æº

        Args:
            None

        Returns:
            None

        Raises:
            ConsumerShutdownError: å½“ä»¥ä¸‹æƒ…å†µå‘ç”Ÿæ—¶æŠ›å‡ºï¼š
                - åç§»é‡æŒä¹…åŒ–å¤±è´¥
                - çº¿ç¨‹æ± å…³é—­è¶…æ—¶
                - ç½‘ç»œè¿æ¥æ¸…ç†å¤±è´¥
                - å…¶ä»–æ¸…ç†è¿‡ç¨‹ä¸­çš„é”™è¯¯

        Note:
            - æ­¤æ–¹æ³•æ˜¯çº¿ç¨‹å®‰å…¨çš„ï¼Œå¯ä»¥å¤šæ¬¡è°ƒç”¨
            - ä¼šå°½åŠ›ç­‰å¾…æ­£åœ¨å¤„ç†çš„æ¶ˆæ¯å®Œæˆï¼Œä½†ä¸ä¼šæ— é™æœŸç­‰å¾…
            - å³ä½¿å…³é—­è¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯ï¼Œä¹Ÿä¼šç»§ç»­æ‰§è¡Œåç»­çš„æ¸…ç†æ­¥éª¤
            - å…³é—­åçš„æ¶ˆè´¹è€…ä¸èƒ½é‡æ–°å¯åŠ¨ï¼Œéœ€è¦åˆ›å»ºæ–°å®ä¾‹
        """
        with self._lock:
            if not self._is_running:
                logger.warning("Consumer is not running")
                return

            try:
                logger.info(
                    "Shutting down OrderlyConsumer",
                    extra={"consumer_group": self._config.consumer_group},
                )

                self._is_running = False

                # å…ˆè®¾ç½®Eventä»¥å”¤é†’å¯èƒ½é˜»å¡çš„çº¿ç¨‹
                self._rebalance_event.set()

                # åœæ­¢æ‹‰å–ä»»åŠ¡
                self._stop_pull_tasks()

                # åœæ­¢æ¶ˆè´¹ä»»åŠ¡
                self._stop_consume_tasks()

                # ç­‰å¾…å¤„ç†ä¸­çš„æ¶ˆæ¯å®Œæˆ
                self._wait_for_processing_completion()

                # æŒä¹…åŒ–åç§»é‡
                try:
                    self._offset_store.persist_all()
                except Exception as e:
                    logger.error(
                        f"Failed to persist offsets during shutdown: {e}",
                        extra={
                            "consumer_group": self._config.consumer_group,
                            "error": str(e),
                        },
                        exc_info=True,
                    )

                # åœæ­¢çº¿ç¨‹æ± 
                self._shutdown_thread_pools()

                # æ¸…ç†èµ„æº
                self._cleanup_resources()

                super().shutdown()

                logger.info(
                    "OrderlyConsumer shutdown completed",
                    extra={
                        "consumer_group": self._config.consumer_group,
                        "final_stats": self._get_final_stats(),
                    },
                )

            except Exception as e:
                logger.error(
                    f"Error during OrderlyConsumer shutdown: {e}",
                    extra={
                        "consumer_group": self._config.consumer_group,
                        "error": str(e),
                    },
                    exc_info=True,
                )
                raise ConsumerShutdownError(
                    "Error during consumer shutdown",
                    cause=e,
                    context={"consumer_group": self._config.consumer_group},
                ) from e

    # ==================== å†…éƒ¨æ–¹æ³•ï¼šé‡å¹³è¡¡ç®¡ç† ====================

    def _pre_rebalance_check(self) -> bool:
        """æ‰§è¡Œé‡å¹³è¡¡å‰ç½®æ£€æŸ¥ã€‚

        æ£€æŸ¥æ˜¯å¦å¯ä»¥æ‰§è¡Œé‡å¹³è¡¡æ“ä½œï¼ŒåŒ…æ‹¬é”è·å–å’Œè®¢é˜…çŠ¶æ€æ£€æŸ¥ã€‚

        Returns:
            bool: å¦‚æœå¯ä»¥æ‰§è¡Œé‡å¹³è¡¡è¿”å›Trueï¼Œå¦åˆ™è¿”å›False

        Raises:
            None: æ­¤æ–¹æ³•ä¸ä¼šæŠ›å‡ºå¼‚å¸¸
        """
        # å¤šä¸ªåœ°æ–¹éƒ½ä¼šè§¦å‘é‡å¹³è¡¡ï¼ŒåŠ å…¥ä¸€ä¸ªæ”¾ç½®é‡å…¥æœºåˆ¶ï¼Œå¦‚æœæ­£åœ¨æ‰§è¡Œrebalanceï¼Œå†æ¬¡è§¦å‘æ— æ•ˆ
        # ä½¿ç”¨å¯é‡å…¥é”ä¿æŠ¤é‡å¹³è¡¡æ“ä½œ
        if not self._rebalance_lock.acquire(blocking=False):
            # å¦‚æœæ— æ³•è·å–é”ï¼Œè¯´æ˜æ­£åœ¨æ‰§è¡Œé‡å¹³è¡¡ï¼Œè·³è¿‡æœ¬æ¬¡è¯·æ±‚
            self._stats["rebalance_skipped_count"] += 1
            logger.debug(
                "Rebalance already in progress, skipping",
                extra={
                    "consumer_group": self._config.consumer_group,
                    "skipped_count": self._stats["rebalance_skipped_count"],
                },
            )
            return False

        # æ£€æŸ¥æ˜¯å¦æœ‰è®¢é˜…çš„Topic
        topics: set[str] = self._subscription_manager.get_topics()
        if not topics:
            logger.debug("No topics subscribed, skipping rebalance")
            self._rebalance_lock.release()
            return False

        return True

    def _collect_and_allocate_queues(self) -> list[MessageQueue]:
        """æ”¶é›†æ‰€æœ‰Topicçš„å¯ç”¨é˜Ÿåˆ—å¹¶æ‰§è¡Œåˆ†é…ã€‚

        éå†æ‰€æœ‰è®¢é˜…çš„Topicï¼Œè·å–æ¯ä¸ªTopicçš„å¯ç”¨é˜Ÿåˆ—ï¼Œ
        å¹¶ä¸ºæ¯ä¸ªTopicæ‰§è¡Œé˜Ÿåˆ—åˆ†é…ç®—æ³•ã€‚

        Returns:
            list[MessageQueue]: åˆ†é…ç»™å½“å‰æ¶ˆè´¹è€…çš„æ‰€æœ‰é˜Ÿåˆ—åˆ—è¡¨

        Raises:
            Exception: è·¯ç”±ä¿¡æ¯æ›´æ–°æˆ–é˜Ÿåˆ—åˆ†é…å¤±è´¥æ—¶æŠ›å‡ºå¼‚å¸¸
        """
        allocated_queues: list[MessageQueue] = []
        topics = self._subscription_manager.get_topics()

        for topic in topics:
            try:
                # æ›´æ–°Topicè·¯ç”±ä¿¡æ¯
                _ = self._update_route_info(topic)

                # è·å–Topicçš„æ‰€æœ‰å¯ç”¨é˜Ÿåˆ—
                all_queues: list[MessageQueue] = [
                    x
                    for (x, _) in self._topic_broker_mapping.get_subscribe_queues(topic)
                ]

                if not all_queues:
                    logger.debug(
                        "No queues available for subscribed topic",
                        extra={"topic": topic},
                    )
                    continue

                # æ‰§è¡Œé˜Ÿåˆ—åˆ†é…
                topic_allocated_queues = self._allocate_queues(topic, all_queues)
                allocated_queues.extend(topic_allocated_queues)

                logger.debug(
                    "Topic queue allocation completed",
                    extra={
                        "topic": topic,
                        "total_queues": len(all_queues),
                        "allocated_queues": len(topic_allocated_queues),
                    },
                )

            except Exception as e:
                logger.warning(
                    f"Failed to allocate queues for topic {topic}: {e}",
                    extra={"topic": topic, "error": str(e)},
                )
                # ç»§ç»­å¤„ç†å…¶ä»–Topicï¼Œä¸ä¸­æ–­æ•´ä¸ªé‡å¹³è¡¡è¿‡ç¨‹
                continue

        return allocated_queues

    def _finalize_rebalance(self, total_topics: int, total_queues: int) -> None:
        """å®Œæˆé‡å¹³è¡¡åå¤„ç†ã€‚

        æ›´æ–°é‡å¹³è¡¡æ—¶é—´æˆ³ã€ç»Ÿè®¡ä¿¡æ¯ï¼Œå¹¶è®°å½•å®Œæˆæ—¥å¿—ã€‚

        Args:
            total_topics: é‡å¹³è¡¡å¤„ç†çš„Topicæ€»æ•°
            total_queues: åˆ†é…åˆ°çš„é˜Ÿåˆ—æ€»æ•°

        Raises:
            None: æ­¤æ–¹æ³•ä¸ä¼šæŠ›å‡ºå¼‚å¸¸
        """
        self._last_rebalance_time = time.time()

        # æ›´æ–°æˆåŠŸç»Ÿè®¡
        self._stats["rebalance_success_count"] += 1

        logger.info(
            "Rebalance completed",
            extra={
                "consumer_group": self._config.consumer_group,
                "total_topics": total_topics,
                "assigned_queues": total_queues,
                "success_count": self._stats["rebalance_success_count"],
            },
        )

    def _do_rebalance(self) -> None:
        """æ‰§è¡Œæ¶ˆè´¹è€…é‡å¹³è¡¡æ“ä½œã€‚

        æ ¹æ®å½“å‰è®¢é˜…çš„æ‰€æœ‰Topicï¼Œé‡æ–°è®¡ç®—å’Œåˆ†é…é˜Ÿåˆ—ç»™å½“å‰æ¶ˆè´¹è€…ã€‚
        é‡å¹³è¡¡æ˜¯RocketMQå®ç°è´Ÿè½½å‡è¡¡çš„æ ¸å¿ƒæœºåˆ¶ï¼Œç¡®ä¿æ¶ˆè´¹è€…ç»„å†…çš„é˜Ÿåˆ—åˆ†é…åˆç†ã€‚

        æ‰§è¡Œæµç¨‹ï¼š
        1. æ‰§è¡Œé‡å¹³è¡¡å‰ç½®æ£€æŸ¥
        2. æ”¶é›†æ‰€æœ‰Topicçš„å¯ç”¨é˜Ÿåˆ—
        3. æ‰§è¡Œé˜Ÿåˆ—åˆ†é…ç®—æ³•
        4. æ›´æ–°åˆ†é…çš„é˜Ÿåˆ—å¹¶å¯åŠ¨æ‹‰å–ä»»åŠ¡
        5. å®Œæˆé‡å¹³è¡¡åå¤„ç†

        é‡å¹³è¡¡è§¦å‘æ¡ä»¶ï¼š
        - æ¶ˆè´¹è€…å¯åŠ¨æ—¶
        - æ–°è®¢é˜…æˆ–å–æ¶ˆè®¢é˜…Topicæ—¶
        - å®šæœŸé‡å¹³è¡¡æ£€æŸ¥ï¼ˆé»˜è®¤20ç§’é—´éš”ï¼‰
        - æ”¶åˆ°æ¶ˆè´¹è€…ç»„å˜æ›´é€šçŸ¥æ—¶

        Returns:
            None

        Raises:
            None: æ­¤æ–¹æ³•ä¼šæ•è·æ‰€æœ‰å¼‚å¸¸å¹¶è®°å½•æ—¥å¿—ï¼Œä¸ä¼šå‘ä¸ŠæŠ›å‡º

        Note:
            - é‡å¹³è¡¡è¿‡ç¨‹ä¸­å¯èƒ½ä¼šçŸ­æš‚åœæ­¢æ¶ˆæ¯æ‹‰å–
            - æ–°åˆ†é…çš„é˜Ÿåˆ—ä¼šè‡ªåŠ¨å¼€å§‹æ‹‰å–æ¶ˆæ¯
            - è¢«å›æ”¶çš„é˜Ÿåˆ—ä¼šåœæ­¢æ‹‰å–å¹¶ç­‰å¾…å½“å‰æ¶ˆæ¯å¤„ç†å®Œæˆ
            - é‡å¹³è¡¡å¤±è´¥ä¸ä¼šå½±å“å·²è¿è¡Œçš„é˜Ÿåˆ—ï¼Œä¼šåœ¨ä¸‹æ¬¡é‡è¯•
        """
        # å‰ç½®æ£€æŸ¥
        if not self._pre_rebalance_check():
            return

        try:
            logger.debug(
                "Starting rebalance",
                extra={"consumer_group": self._config.consumer_group},
            )

            # æ›´æ–°ç»Ÿè®¡ä¿¡æ¯
            self._stats["rebalance_count"] += 1

            # æ”¶é›†æ‰€æœ‰å¯ç”¨é˜Ÿåˆ—å¹¶æ‰§è¡Œåˆ†é…
            allocated_queues = self._collect_and_allocate_queues()

            # æ›´æ–°åˆ†é…çš„é˜Ÿåˆ—
            if allocated_queues:
                self._update_assigned_queues(allocated_queues)

            # å®Œæˆé‡å¹³è¡¡å¤„ç†
            self._finalize_rebalance(
                len(self._subscription_manager.get_topics()), len(allocated_queues)
            )

        except Exception as e:
            logger.error(
                f"Rebalance failed: {e}",
                extra={
                    "consumer_group": self._config.consumer_group,
                    "error": str(e),
                },
                exc_info=True,
            )
            # æ›´æ–°å¤±è´¥ç»Ÿè®¡
            self._stats["rebalance_failure_count"] += 1

        finally:
            # é‡Šæ”¾é‡å¹³è¡¡é”
            self._rebalance_lock.release()
            logger.debug(
                "Rebalance lock released",
                extra={
                    "consumer_group": self._config.consumer_group,
                    "rebalance_count": self._stats["rebalance_count"],
                },
            )

    def _allocate_queues(
        self, topic: str, all_queues: list[MessageQueue]
    ) -> list[MessageQueue]:
        """
        ä¸ºå½“å‰æ¶ˆè´¹è€…åˆ†é…é˜Ÿåˆ—

        æ ¹æ®æ¶ˆæ¯æ¨¡å¼å’Œåˆ†é…ç­–ç•¥ï¼Œä»æ‰€æœ‰å¯ç”¨é˜Ÿåˆ—ä¸­é€‰æ‹©ä¸€éƒ¨åˆ†åˆ†é…ç»™å½“å‰æ¶ˆè´¹è€…å®ä¾‹ã€‚
        è¿™æ˜¯RocketMQæ¶ˆè´¹è€…è´Ÿè½½å‡è¡¡çš„æ ¸å¿ƒæœºåˆ¶ï¼Œç¡®ä¿å¤šä¸ªæ¶ˆè´¹è€…èƒ½å¤Ÿåˆç†åœ°æ¶ˆè´¹åŒä¸€ä¸ªTopicä¸‹çš„æ¶ˆæ¯ã€‚

        ## åˆ†é…ç­–ç•¥

        ### é›†ç¾¤æ¨¡å¼ (CLUSTERING)
        - åœ¨é›†ç¾¤æ¨¡å¼ä¸‹ï¼ŒåŒä¸€ä¸ªæ¶ˆè´¹è€…ç»„å†…çš„å¤šä¸ªæ¶ˆè´¹è€…ä¼šåˆ†æ‹…é˜Ÿåˆ—
        - æ¯ä¸ªé˜Ÿåˆ—åªèƒ½è¢«ä¸€ä¸ªæ¶ˆè´¹è€…æ¶ˆè´¹ï¼Œé¿å…é‡å¤æ¶ˆè´¹
        - ä½¿ç”¨åˆ†é…ç­–ç•¥ç®—æ³•ï¼ˆå¦‚å¹³å‡åˆ†é…ï¼‰æ¥å†³å®šå“ªä¸ªæ¶ˆè´¹è€…æ¶ˆè´¹å“ªäº›é˜Ÿåˆ—

        ### å¹¿æ’­æ¨¡å¼ (BROADCASTING)
        - åœ¨å¹¿æ’­æ¨¡å¼ä¸‹ï¼Œæ¯ä¸ªæ¶ˆè´¹è€…éƒ½ä¼šæ¶ˆè´¹æ‰€æœ‰é˜Ÿåˆ—
        - æ‰€æœ‰æ¶ˆè´¹è€…éƒ½ä¼šæ”¶åˆ°ç›¸åŒçš„æ¶ˆæ¯ï¼Œå®ç°å¹¿æ’­æ•ˆæœ
        - ä¸éœ€è¦è¿›è¡Œé˜Ÿåˆ—åˆ†é…ï¼Œç›´æ¥è¿”å›æ‰€æœ‰é˜Ÿåˆ—

        ## åˆ†é…æµç¨‹

        1. **æ£€æŸ¥æ¶ˆæ¯æ¨¡å¼**ï¼š
           - é›†ç¾¤æ¨¡å¼ï¼šæ‰§è¡Œè´Ÿè½½å‡è¡¡åˆ†é…
           - å¹¿æ’­æ¨¡å¼ï¼šè¿”å›æ‰€æœ‰é˜Ÿåˆ—

        2. **é›†ç¾¤æ¨¡å¼åˆ†é…**ï¼š
           - è·å–è®¢é˜…è¯¥Topicçš„æ‰€æœ‰æ¶ˆè´¹è€…IDåˆ—è¡¨
           - å¦‚æœæ²¡æœ‰å…¶ä»–æ¶ˆè´¹è€…ï¼Œè¿”å›ç©ºåˆ—è¡¨ï¼ˆé¿å…é‡å¤æ¶ˆè´¹ï¼‰
           - ä½¿ç”¨é…ç½®çš„åˆ†é…ç­–ç•¥è¿›è¡Œé˜Ÿåˆ—åˆ†é…

        3. **åˆ†é…ç­–ç•¥å‚æ•°**ï¼š
           - consumer_group: æ¶ˆè´¹è€…ç»„å
           - client_id: å½“å‰æ¶ˆè´¹è€…å®¢æˆ·ç«¯ID
           - consumer_ids: æ‰€æœ‰æ¶ˆè´¹è€…IDåˆ—è¡¨
           - all_queues: æ‰€æœ‰å¯ç”¨é˜Ÿåˆ—åˆ—è¡¨
           - message_queues: é˜Ÿåˆ—æ˜ å°„è¡¨

        Args:
            topic: è¦åˆ†é…é˜Ÿåˆ—çš„Topicåç§°
            all_queues: è¯¥Topicä¸‹æ‰€æœ‰å¯ç”¨çš„æ¶ˆæ¯é˜Ÿåˆ—åˆ—è¡¨

        Returns:
            list[MessageQueue]: åˆ†é…ç»™å½“å‰æ¶ˆè´¹è€…çš„é˜Ÿåˆ—åˆ—è¡¨

        ## ä½¿ç”¨åœºæ™¯

        - **æ¶ˆè´¹è€…å¯åŠ¨æ—¶**ï¼šåˆæ¬¡åˆ†é…é˜Ÿåˆ—
        - **é‡å¹³è¡¡æ—¶**ï¼šæ¶ˆè´¹è€…åŠ å…¥æˆ–ç¦»å¼€åé‡æ–°åˆ†é…
        - **è·¯ç”±å˜æ›´æ—¶**ï¼šTopicè·¯ç”±ä¿¡æ¯å˜åŒ–åé‡æ–°åˆ†é…

        ## æ³¨æ„äº‹é¡¹

        - é›†ç¾¤æ¨¡å¼ä¸‹ç¡®ä¿ä¸€ä¸ªé˜Ÿåˆ—åªè¢«ä¸€ä¸ªæ¶ˆè´¹è€…æ¶ˆè´¹
        - å¹¿æ’­æ¨¡å¼ä¸‹æ¯ä¸ªæ¶ˆè´¹è€…éƒ½èƒ½æ”¶åˆ°æ‰€æœ‰æ¶ˆæ¯
        - åˆ†é…ç»“æœä¼šå½±å“æ¶ˆæ¯çš„å¹¶å‘åº¦å’Œå¤„ç†æ€§èƒ½
        - åˆ†é…ç­–ç•¥çš„å˜æ›´å¯èƒ½å¯¼è‡´æ¶ˆæ¯é¡ºåºæ€§çš„å˜åŒ–

        ## ç¤ºä¾‹

        ```python
        # å‡è®¾æœ‰3ä¸ªé˜Ÿåˆ—å’Œ2ä¸ªæ¶ˆè´¹è€…
        all_queues = [queue1, queue2, queue3]

        # é›†ç¾¤æ¨¡å¼ä¸‹ï¼Œå¯èƒ½åˆ†é…ç»™å½“å‰æ¶ˆè´¹è€…ï¼š[queue1, queue3]
        # å¹¿æ’­æ¨¡å¼ä¸‹ï¼Œåˆ†é…ç»™å½“å‰æ¶ˆè´¹è€…ï¼š[queue1, queue2, queue3]
        allocated = self._allocate_queues("test_topic", all_queues)
        ```
        """
        if self._config.message_model == MessageModel.CLUSTERING:
            cids = self._find_consumer_list(topic)
            if not cids:
                return []

            return self._allocate_strategy.allocate(
                AllocateContext(
                    self._config.consumer_group,
                    self._config.client_id,
                    cids,
                    all_queues,
                    {},
                )
            )
        else:
            return all_queues.copy()

    def _update_assigned_queues(self, new_queues: list[MessageQueue]) -> None:
        """æ›´æ–°å½“å‰æ¶ˆè´¹è€…çš„åˆ†é…é˜Ÿåˆ—é›†åˆã€‚

        æ¯”è¾ƒæ–°æ—§é˜Ÿåˆ—åˆ†é…ï¼Œæ‰§è¡Œå¢é‡æ›´æ–°ï¼š
        - åœæ­¢è¢«å›æ”¶é˜Ÿåˆ—çš„æ‹‰å–ä»»åŠ¡
        - å¯åŠ¨æ–°åˆ†é…é˜Ÿåˆ—çš„æ‹‰å–ä»»åŠ¡
        - ç»´æŠ¤é˜Ÿåˆ—åç§»é‡ä¿¡æ¯
        - ç®¡ç†æ¯ä¸ªé˜Ÿåˆ—çš„æ¶ˆè´¹ä»»åŠ¡

        Args:
            new_queues (list[MessageQueue]): æ–°åˆ†é…ç»™å½“å‰æ¶ˆè´¹è€…çš„é˜Ÿåˆ—åˆ—è¡¨

        Returns:
            None

        Raises:
            None: æ­¤æ–¹æ³•ä¼šå¤„ç†æ‰€æœ‰å¼‚å¸¸æƒ…å†µ

        Note:
            - é˜Ÿåˆ—å˜æ›´ä¸ä¼šä¸­æ–­æ­£åœ¨å¤„ç†çš„æ¶ˆæ¯
            - è¢«å›æ”¶çš„é˜Ÿåˆ—ä¼šç­‰å¾…å½“å‰æ¶ˆæ¯å¤„ç†å®Œæˆåæ‰åœæ­¢
            - æ–°é˜Ÿåˆ—ä¼šç«‹å³å¼€å§‹æ‹‰å–æ¶ˆæ¯
            - åç§»é‡ä¿¡æ¯ä¼šåœ¨é˜Ÿåˆ—åˆ†é…å˜æ›´æ—¶ä¿ç•™
            - æ¯ä¸ªé˜Ÿåˆ—çš„æ¶ˆè´¹ä»»åŠ¡ä¼šåœ¨é˜Ÿåˆ—åˆ†é…å˜æ›´æ—¶è¿›è¡Œç®¡ç†
        """

        # ä½¿ç”¨_assigned_queues_lockä¿æŠ¤æ•´ä¸ªé˜Ÿåˆ—æ›´æ–°è¿‡ç¨‹
        with self._assigned_queues_lock:  # ğŸ”ä¿æŠ¤_assigned_queuesçš„å®Œæ•´æ“ä½œ
            old_queues: set[MessageQueue] = set(self._assigned_queues.keys())
            new_queue_set: set[MessageQueue] = set(new_queues)

            removed_queues: set[MessageQueue] = old_queues - new_queue_set
            added_queues: set[MessageQueue] = new_queue_set - old_queues

            # ç§»é™¤æ—§é˜Ÿåˆ—çš„åç§»é‡ä¿¡æ¯
            for q in removed_queues:
                _ = self._assigned_queues.pop(q, None)

            # æ·»åŠ æ–°é˜Ÿåˆ—çš„åç§»é‡åˆå§‹åŒ–
            for q in added_queues:
                self._assigned_queues[q] = 0  # åˆå§‹åŒ–åç§»é‡ä¸º0ï¼Œåç»­ä¼šæ›´æ–°

        # åœ¨é”å¤–å¤„ç†å…¶ä»–èµ„æºçš„æ¸…ç†å’Œåˆ›å»ºï¼Œé¿å…æ­»é”
        # åœæ­¢ä¸å†åˆ†é…çš„é˜Ÿåˆ—çš„æ‹‰å–ä»»åŠ¡å’Œæ¶ˆè´¹ä»»åŠ¡
        for q in removed_queues:
            if q in self._pull_tasks:
                future: Future[None] | None = self._pull_tasks.pop(q)
                if future and not future.done():
                    future.cancel()

            # åœæ­¢å¹¶ç§»é™¤è¯¥é˜Ÿåˆ—çš„æ¶ˆè´¹ä»»åŠ¡
            if q in self._consume_tasks:
                consume_futures = self._consume_tasks.pop(q)
                for future in consume_futures:
                    if future and not future.done():
                        future.cancel()

            # æ¸…ç†é˜Ÿåˆ—é”
            if q in self._queue_locks:
                del self._queue_locks[q]

        # ä¸ºæ–°åˆ†é…çš„é˜Ÿåˆ—åˆ›å»ºèµ„æº
        for q in added_queues:
            # ä¸ºæ–°é˜Ÿåˆ—åˆ›å»ºé”
            self._queue_locks[q] = threading.RLock()

            # ä¸ºæ–°é˜Ÿåˆ—åˆå§‹åŒ–æ¶ˆè´¹ä»»åŠ¡åˆ—è¡¨
            self._consume_tasks[q] = []

        # å¦‚æœæ¶ˆè´¹è€…æ­£åœ¨è¿è¡Œï¼Œå¯åŠ¨æ–°é˜Ÿåˆ—çš„æ‹‰å–ä»»åŠ¡å’Œæ¶ˆè´¹ä»»åŠ¡
        if self._is_running and added_queues:
            self._start_pull_tasks_for_queues(added_queues)
            self._start_consume_tasks_for_queues(added_queues)

    def _get_queue_lock(self, message_queue: MessageQueue) -> threading.RLock:
        """
        è·å–æŒ‡å®šæ¶ˆæ¯é˜Ÿåˆ—çš„é”

        Args:
            message_queue: æ¶ˆæ¯é˜Ÿåˆ—

        Returns:
            è¯¥é˜Ÿåˆ—çš„RLocké”å¯¹è±¡
        """

        # ä½¿ç”¨åŒé‡æ£€æŸ¥é”å®šæ¨¡å¼æ¥é¿å…ç«äº‰æ¡ä»¶
        # é¦–å…ˆè¿›è¡Œæ— é”æ£€æŸ¥ï¼Œæé«˜æ€§èƒ½
        if message_queue in self._queue_locks:
            return self._queue_locks[message_queue]

        # ä½¿ç”¨é”ä¿æŠ¤å­—å…¸æ“ä½œï¼Œé˜²æ­¢ç«äº‰æ¡ä»¶
        with self._queue_lock_management_lock:
            # å†æ¬¡æ£€æŸ¥ï¼Œé˜²æ­¢åœ¨ç­‰å¾…é”çš„è¿‡ç¨‹ä¸­å…¶ä»–çº¿ç¨‹å·²ç»åˆ›å»ºäº†é”
            if message_queue not in self._queue_locks:
                self._queue_locks[message_queue] = threading.RLock()

            return self._queue_locks[message_queue]

    def _is_locked(self, message_queue: MessageQueue) -> bool:
        """
        æ£€æŸ¥æŒ‡å®šé˜Ÿåˆ—æ˜¯å¦å·²é”å®š

        Args:
            message_queue: æ¶ˆæ¯é˜Ÿåˆ—

        Returns:
            Trueå¦‚æœé˜Ÿåˆ—å·²é”å®šï¼ŒFalseå¦‚æœé˜Ÿåˆ—æœªé”å®š
        """
        # ä½¿ç”¨é”ä¿æŠ¤å¯¹_queue_lockså­—å…¸çš„è®¿é—®ï¼Œé˜²æ­¢ç«äº‰æ¡ä»¶
        with self._queue_lock_management_lock:
            if message_queue not in self._queue_locks:
                return False

            return self._queue_locks[message_queue].locked()

    def _is_remote_lock_valid(self, message_queue: MessageQueue) -> bool:
        """
        æ£€æŸ¥æŒ‡å®šé˜Ÿåˆ—çš„è¿œç¨‹é”æ˜¯å¦ä»ç„¶æœ‰æ•ˆ

        Args:
            message_queue: æ¶ˆæ¯é˜Ÿåˆ—

        Returns:
            Trueå¦‚æœè¿œç¨‹é”ä»ç„¶æœ‰æ•ˆï¼ŒFalseå¦‚æœå·²è¿‡æœŸæˆ–ä¸å­˜åœ¨
        """
        with self._remote_lock_cache_lock:
            expiry_time = self._remote_lock_cache.get(message_queue)
            if expiry_time is None:
                return False

            current_time = time.time()
            return current_time < expiry_time

    def _set_remote_lock_expiry(self, message_queue: MessageQueue) -> None:
        """
        è®¾ç½®æŒ‡å®šé˜Ÿåˆ—çš„è¿œç¨‹é”è¿‡æœŸæ—¶é—´

        Args:
            message_queue: æ¶ˆæ¯é˜Ÿåˆ—
        """
        with self._remote_lock_cache_lock:
            expiry_time = time.time() + self._remote_lock_expire_time
            self._remote_lock_cache[message_queue] = expiry_time

    def _invalidate_remote_lock(self, message_queue: MessageQueue) -> None:
        """
        ä½¿æŒ‡å®šé˜Ÿåˆ—çš„è¿œç¨‹é”å¤±æ•ˆ

        Args:
            message_queue: æ¶ˆæ¯é˜Ÿåˆ—
        """
        with self._remote_lock_cache_lock:
            self._remote_lock_cache.pop(message_queue, None)

    def _lock_remote_queue(self, message_queue: MessageQueue) -> bool:
        """
        å°è¯•è¿œç¨‹é”å®šæŒ‡å®šé˜Ÿåˆ—

        Args:
            message_queue: æ¶ˆæ¯é˜Ÿåˆ—

        Returns:
            Trueå¦‚æœé”å®šæˆåŠŸï¼ŒFalseå¦‚æœé”å®šå¤±è´¥
        """
        try:
            # è·å–é˜Ÿåˆ—å¯¹åº”çš„Brokerè¿æ¥
            broker_address: str | None = self._name_server_manager.get_broker_address(
                message_queue.broker_name
            )
            if not broker_address:
                logger.warning(f"Broker address not found for queue: {message_queue}")
                return False

            connection_pool = self._broker_manager.must_connection_pool(broker_address)

            # åˆ›å»ºbrokerå®¢æˆ·ç«¯
            with connection_pool.get_connection() as conn:
                broker_client = BrokerClient(conn)

                # å°è¯•é”å®šé˜Ÿåˆ—
                locked_queues = broker_client.lock_batch_mq(
                    consumer_group=self._config.consumer_group,
                    client_id=self._config.client_id,
                    mqs=[message_queue],
                )

                # æ£€æŸ¥é”å®šæ˜¯å¦æˆåŠŸ
                if locked_queues and len(locked_queues) > 0:
                    # æˆåŠŸè·å–è¿œç¨‹é”ï¼Œè®¾ç½®è¿‡æœŸæ—¶é—´
                    self._set_remote_lock_expiry(message_queue)
                    logger.debug(
                        f"Successfully locked remote queue: {message_queue}",
                        extra={
                            "consumer_group": self._config.consumer_group,
                            "client_id": self._config.client_id,
                            "queue": str(message_queue),
                            "operation": "lock_remote_queue",
                            "expire_seconds": self._remote_lock_expire_time,
                        },
                    )
                    return True
                else:
                    logger.warning(
                        f"Failed to lock remote queue: {message_queue}",
                        extra={
                            "consumer_group": self._config.consumer_group,
                            "client_id": self._config.client_id,
                            "queue": str(message_queue),
                            "operation": "lock_remote_queue",
                        },
                    )
                    return False

        except Exception as e:
            logger.error(
                f"Exception occurred while locking remote queue {message_queue}: {e}",
                extra={
                    "consumer_group": self._config.consumer_group,
                    "client_id": self._config.client_id,
                    "queue": str(message_queue),
                    "error": str(e),
                    "operation": "lock_remote_queue",
                },
                exc_info=True,
            )
            return False

    def _unlock_remote_queue(self, message_queue: MessageQueue) -> bool:
        """
        å°è¯•è¿œç¨‹è§£é”æŒ‡å®šé˜Ÿåˆ—

        Args:
            message_queue: æ¶ˆæ¯é˜Ÿåˆ—

        Returns:
            Trueå¦‚æœè§£é”æˆåŠŸï¼ŒFalseå¦‚æœè§£é”å¤±è´¥
        """
        try:
            # è·å–é˜Ÿåˆ—å¯¹åº”çš„Brokerè¿æ¥
            broker_address: str | None = self._name_server_manager.get_broker_address(
                message_queue.broker_name
            )
            if not broker_address:
                logger.warning(f"Broker address not found for queue: {message_queue}")
                return False

            connection_pool = self._broker_manager.must_connection_pool(broker_address)

            with connection_pool.get_connection() as conn:
                broker_client = BrokerClient(conn)

                # å°è¯•è§£é”é˜Ÿåˆ—
                broker_client.unlock_batch_mq(
                    consumer_group=self._config.consumer_group,
                    client_id=self._config.client_id,
                    mqs=[message_queue],
                )

                # æ¸…é™¤è¿œç¨‹é”ç¼“å­˜
                self._invalidate_remote_lock(message_queue)

                logger.debug(
                    f"Successfully unlocked remote queue: {message_queue}",
                    extra={
                        "consumer_group": self._config.consumer_group,
                        "client_id": self._config.client_id,
                        "queue": str(message_queue),
                        "operation": "unlock_remote_queue",
                    },
                )
                return True

        except Exception as e:
            logger.error(
                f"Exception occurred while unlocking remote queue {message_queue}: {e}",
                extra={
                    "consumer_group": self._config.consumer_group,
                    "client_id": self._config.client_id,
                    "queue": str(message_queue),
                    "error": str(e),
                    "operation": "unlock_remote_queue",
                },
                exc_info=True,
            )
            return False

    def _trigger_rebalance(self) -> None:
        """
        è§¦å‘é‡å¹³è¡¡
        """
        if self._is_running:
            # å”¤é†’é‡å¹³è¡¡å¾ªç¯ï¼Œä½¿å…¶ç«‹å³æ‰§è¡Œé‡å¹³è¡¡
            self._rebalance_event.set()

    # ==================== å†…éƒ¨æ–¹æ³•ï¼šæ¶ˆæ¯æ‹‰å– ====================

    def _start_pull_tasks_for_queues(self, queues: set[MessageQueue]) -> None:
        """
        ä¸ºæŒ‡å®šé˜Ÿåˆ—å¯åŠ¨æ‹‰å–ä»»åŠ¡

        Args:
            queues: è¦å¯åŠ¨æ‹‰å–ä»»åŠ¡çš„é˜Ÿåˆ—é›†åˆ
        """
        if not self._pull_executor:
            raise ValueError("Pull executor is not initialized")

        for q in queues:
            if q not in self._pull_tasks:
                # ä¸ºæ¯ä¸ªé˜Ÿåˆ—åˆ›å»ºåœæ­¢äº‹ä»¶
                with self._stop_events_lock:
                    pull_stop_event = threading.Event()
                    consume_stop_event = threading.Event()
                    self._pull_stop_events[q] = pull_stop_event
                    self._consume_stop_events[q] = consume_stop_event

                # å¯åŠ¨æ‹‰å–ä»»åŠ¡ï¼Œä¼ å…¥åœæ­¢äº‹ä»¶
                future: Future[None] = self._pull_executor.submit(
                    self._pull_messages_loop, q, pull_stop_event
                )
                self._pull_tasks[q] = future

    def _stop_pull_tasks(self) -> None:
        """
        åœæ­¢æ‰€æœ‰æ¶ˆæ¯æ‹‰å–ä»»åŠ¡ - ä½¿ç”¨åœæ­¢äº‹ä»¶ä¼˜é›…å…³é—­
        """
        if not self._pull_tasks:
            return

        # é¦–å…ˆè®¾ç½®æ‰€æœ‰åœæ­¢äº‹ä»¶
        with self._stop_events_lock:
            for message_queue in self._pull_tasks.keys():
                # è®¾ç½®æ‹‰å–åœæ­¢äº‹ä»¶
                if message_queue in self._pull_stop_events:
                    self._pull_stop_events[message_queue].set()
                # è®¾ç½®æ¶ˆè´¹åœæ­¢äº‹ä»¶
                if message_queue in self._consume_stop_events:
                    self._consume_stop_events[message_queue].set()

        # ç„¶åå–æ¶ˆFutureä»»åŠ¡
        for _, future in self._pull_tasks.items():
            if future and not future.done():
                future.cancel()

        self._pull_tasks.clear()

        # ç­‰å¾…ä¸€æ®µæ—¶é—´è®©çº¿ç¨‹è‡ªç„¶é€€å‡º
        time.sleep(0.1)

    def _stop_consume_tasks(self) -> None:
        """
        åœæ­¢æ‰€æœ‰æ¶ˆæ¯æ¶ˆè´¹ä»»åŠ¡ - ä½¿ç”¨åœæ­¢äº‹ä»¶ä¼˜é›…å…³é—­
        """
        if not self._consume_tasks:
            return

        # é¦–å…ˆè®¾ç½®æ‰€æœ‰åœæ­¢äº‹ä»¶
        with self._stop_events_lock:
            for message_queue in self._consume_tasks.keys():
                if message_queue in self._consume_stop_events:
                    self._consume_stop_events[message_queue].set()

        # ç„¶åå–æ¶ˆFutureä»»åŠ¡
        for message_queue, futures in self._consume_tasks.items():
            for future in futures:
                if future and not future.done():
                    future.cancel()

        self._consume_tasks.clear()

        # ç­‰å¾…ä¸€æ®µæ—¶é—´è®©çº¿ç¨‹è‡ªç„¶é€€å‡º
        time.sleep(0.1)

    def _pull_messages_loop(
        self,
        message_queue: MessageQueue,
        pull_stop_event: threading.Event,
    ) -> None:
        """æŒç»­æ‹‰å–æŒ‡å®šé˜Ÿåˆ—çš„æ¶ˆæ¯ã€‚

        ä¸ºæ¯ä¸ªåˆ†é…çš„é˜Ÿåˆ—åˆ›å»ºç‹¬ç«‹çš„æ‹‰å–å¾ªç¯ï¼ŒæŒç»­ä»Brokeræ‹‰å–æ¶ˆæ¯
        å¹¶æ”¾å…¥å¤„ç†é˜Ÿåˆ—ã€‚è¿™æ˜¯æ¶ˆè´¹è€…æ¶ˆæ¯æ‹‰å–çš„æ ¸å¿ƒæ‰§è¡Œå¾ªç¯ã€‚

        æ‰§è¡Œæµç¨‹ï¼š
        1. ä»é˜Ÿåˆ—çš„å½“å‰åç§»é‡å¼€å§‹æ‹‰å–æ¶ˆæ¯
        2. å¦‚æœæ‹‰å–åˆ°æ¶ˆæ¯ï¼Œæ›´æ–°æœ¬åœ°åç§»é‡è®°å½•
        3. å°†æ¶ˆæ¯å’Œå¤„ç†é˜Ÿåˆ—ä¿¡æ¯æäº¤ç»™æ¶ˆè´¹çº¿ç¨‹æ± 
        4. æ ¹æ®é…ç½®çš„æ‹‰å–é—´éš”è¿›è¡Œä¼‘çœ æ§åˆ¶

        Args:
            message_queue (MessageQueue): è¦æŒç»­æ‹‰å–æ¶ˆæ¯çš„ç›®æ ‡é˜Ÿåˆ—
            pull_stop_event (threading.Event): æ‹‰å–çº¿ç¨‹åœæ­¢äº‹ä»¶

        Returns:
            None

        Raises:
            None: æ­¤æ–¹æ³•ä¼šæ•è·æ‰€æœ‰å¼‚å¸¸å¹¶è®°å½•æ—¥å¿—ï¼Œä¸ä¼šä¸­æ–­æ‹‰å–å¾ªç¯

        Note:
            - æ¯ä¸ªé˜Ÿåˆ—æœ‰ç‹¬ç«‹çš„æ‹‰å–çº¿ç¨‹ï¼Œé¿å…é˜Ÿåˆ—é—´ç›¸äº’å½±å“
            - åç§»é‡åœ¨æœ¬åœ°ç»´æŠ¤ï¼Œå®šæœŸæˆ–åœ¨æ¶ˆæ¯å¤„ç†æˆåŠŸåæ›´æ–°åˆ°Broker
            - æ‹‰å–å¤±è´¥ä¼šè®°å½•æ—¥å¿—å¹¶ç­‰å¾…é‡è¯•ï¼Œä¸ä¼šå½±å“å…¶ä»–é˜Ÿåˆ—
            - æ¶ˆè´¹è€…åœæ­¢æ—¶æ­¤å¾ªç¯ä¼šè‡ªåŠ¨é€€å‡º
            - æ”¯æŒé€šè¿‡é…ç½®æ§åˆ¶æ‹‰å–é¢‘ç‡
            - æ”¯æŒé€šè¿‡åœæ­¢äº‹ä»¶ä¼˜é›…å…³é—­
        """
        suggest_broker_id = 0
        while self._is_running and not pull_stop_event.is_set():
            pq: ProcessQueue = self._get_or_create_process_queue(message_queue)
            if pq.need_flow_control():
                # ä½¿ç”¨å¯ä¸­æ–­çš„ç­‰å¾…ï¼Œæ£€æŸ¥åœæ­¢äº‹ä»¶
                if pull_stop_event.wait(timeout=3.0):
                    break
                continue

            try:
                # æ‰§è¡Œå•æ¬¡æ‹‰å–æ“ä½œ
                pull_result: tuple[list[MessageExt], int, int] | None = (
                    self._perform_single_pull(message_queue, suggest_broker_id)
                )

                if pull_result is None:
                    # å¦‚æœè¿”å›Noneï¼Œè¯´æ˜æ²¡æœ‰è®¢é˜…ä¿¡æ¯ï¼Œåœæ­¢æ¶ˆè´¹
                    logger.warning(
                        "No subscription found for topic, stopping pull loop",
                        extra={
                            "consumer_group": self._config.consumer_group,
                            "topic": message_queue.topic,
                            "queue_id": message_queue.queue_id,
                        },
                    )
                    break

                messages, next_begin_offset, next_suggest_id = pull_result
                suggest_broker_id = next_suggest_id

                if messages:
                    # å¤„ç†æ‹‰å–åˆ°çš„æ¶ˆæ¯
                    self._handle_pulled_messages(
                        message_queue, messages, next_begin_offset
                    )
                else:
                    self._stats["pull_requests"] += 1

                # æ§åˆ¶æ‹‰å–é¢‘ç‡ - ä¼ å…¥æ˜¯å¦æœ‰æ¶ˆæ¯çš„æ ‡å¿—ï¼Œä½¿ç”¨å¯ä¸­æ–­ç­‰å¾…
                self._apply_pull_interval(len(messages) > 0, pull_stop_event)

            except MessagePullError as e:
                logger.warning(
                    "The pull request is illegal",
                    extra={
                        "consumer_group": self._config.consumer_group,
                        "topic": message_queue.topic,
                        "queue_id": message_queue.queue_id,
                        "error": str(e),
                    },
                )
                break
            except Exception as e:
                logger.error(
                    f"Error in pull messages loop for {message_queue}: {e}",
                    extra={
                        "consumer_group": self._config.consumer_group,
                        "topic": message_queue.topic,
                        "queue_id": message_queue.queue_id,
                        "error": str(e),
                    },
                    exc_info=True,
                )

                self._stats["pull_failures"] += 1

                # æ‹‰å–å¤±è´¥æ—¶ç­‰å¾…ä¸€æ®µæ—¶é—´å†é‡è¯•ï¼Œä½¿ç”¨å¯ä¸­æ–­ç­‰å¾…
                if pull_stop_event.wait(timeout=3.0):
                    break

    def _perform_single_pull(
        self, message_queue: MessageQueue, suggest_broker_id: int
    ) -> tuple[list[MessageExt], int, int] | None:
        """æ‰§è¡Œå•æ¬¡æ¶ˆæ¯æ‹‰å–æ“ä½œã€‚

        Args:
            message_queue: è¦æ‹‰å–æ¶ˆæ¯çš„é˜Ÿåˆ—
            suggest_broker_id: å»ºè®®çš„Broker ID

        Returns:
            tuple[list[MessageExt], int, int] | None:
                - messages: æ‹‰å–åˆ°çš„æ¶ˆæ¯åˆ—è¡¨
                - next_begin_offset: ä¸‹æ¬¡æ‹‰å–çš„èµ·å§‹åç§»é‡
                - next_suggest_id: ä¸‹æ¬¡å»ºè®®çš„Broker ID
            None: å¦‚æœæ²¡æœ‰è®¢é˜…ä¿¡æ¯

        Raises:
            MessagePullError: å½“æ‹‰å–è¯·æ±‚éæ³•æ—¶æŠ›å‡º
        """
        # è·å–å½“å‰åç§»é‡
        current_offset: int = self._get_or_initialize_offset(message_queue)

        # æ‹‰å–æ¶ˆæ¯
        messages, next_begin_offset, next_suggest_id = self._pull_messages(
            message_queue,
            current_offset,
            suggest_broker_id,
        )

        # æ£€æŸ¥è®¢é˜…ä¿¡æ¯
        sub: SubscriptionEntry | None = self._subscription_manager.get_subscription(
            message_queue.topic
        )
        if sub is None:
            # å¦‚æœæ²¡æœ‰è®¢é˜…ä¿¡æ¯ï¼Œåˆ™åœæ­¢æ¶ˆè´¹
            return None

        sub_data: SubscriptionData = sub.subscription_data

        # æ ¹æ®è®¢é˜…ä¿¡æ¯è¿‡æ»¤æ¶ˆæ¯
        if sub_data.tags_set:
            messages = self._filter_messages_by_tags(messages, sub_data.tags_set)

        return messages, next_begin_offset, next_suggest_id

    def _filter_messages_by_tags(
        self, messages: list[MessageExt], tags_set: list[str]
    ) -> list[MessageExt]:
        """æ ¹æ®æ ‡ç­¾è¿‡æ»¤æ¶ˆæ¯ã€‚

        Args:
            messages: å¾…è¿‡æ»¤çš„æ¶ˆæ¯åˆ—è¡¨
            tags_set: å…è®¸çš„æ ‡ç­¾é›†åˆ

        Returns:
            list[MessageExt]: è¿‡æ»¤åçš„æ¶ˆæ¯åˆ—è¡¨
        """
        filtered_messages: list[MessageExt] = []
        for message in messages:
            if message.get_tags() in tags_set:
                filtered_messages.append(message)

        return filtered_messages

    def _handle_pulled_messages(
        self,
        message_queue: MessageQueue,
        messages: list[MessageExt],
        next_begin_offset: int,
    ) -> None:
        """å¤„ç†æ‹‰å–åˆ°çš„æ¶ˆæ¯ã€‚

        åŒ…æ‹¬æ›´æ–°åç§»é‡ã€ç¼“å­˜æ¶ˆæ¯ã€åˆ†æ‰¹å¤„ç†ç­‰ã€‚

        Args:
            message_queue: æ¶ˆæ¯é˜Ÿåˆ—
            messages: æ‹‰å–åˆ°çš„æ¶ˆæ¯åˆ—è¡¨
            next_begin_offset: ä¸‹æ¬¡æ‹‰å–çš„èµ·å§‹åç§»é‡
        """
        # æ›´æ–°åç§»é‡
        with self._assigned_queues_lock:  # ğŸ”ä¿æŠ¤_assigned_queuesè®¿é—®
            self._assigned_queues[message_queue] = next_begin_offset

        # å°†æ¶ˆæ¯æ·»åŠ åˆ°ç¼“å­˜ä¸­ï¼ˆç”¨äºè§£å†³å¹¶å‘åç§»é‡é—®é¢˜ï¼‰
        self._add_messages_to_cache(message_queue, messages)

        # æ›´æ–°ç»Ÿè®¡ä¿¡æ¯
        message_count = len(messages)
        self._stats["pull_successes"] += 1
        self._stats["messages_consumed"] += message_count
        self._stats["pull_requests"] += 1

    def _apply_pull_interval(
        self, has_messages: bool = True, stop_event: threading.Event | None = None
    ) -> None:
        """åº”ç”¨æ™ºèƒ½æ‹‰å–é—´éš”æ§åˆ¶ã€‚

        æ ¹æ®ä¸Šæ¬¡æ‹‰å–ç»“æœæ™ºèƒ½è°ƒæ•´æ‹‰å–é—´éš”ï¼š
        - å¦‚æœä¸Šæ¬¡æ‹‰å–åˆ°äº†æ¶ˆæ¯ï¼Œç«‹å³ç»§ç»­æ‹‰å–ä»¥æé«˜æ¶ˆè´¹é€Ÿåº¦
        - å¦‚æœä¸Šæ¬¡æ‹‰å–ä¸ºç©ºï¼Œåˆ™ä¼‘çœ é…ç½®çš„é—´éš”æ—¶é—´ä»¥é¿å…ç©ºè½®è¯¢

        Args:
            has_messages: ä¸Šæ¬¡æ‹‰å–æ˜¯å¦è·å–åˆ°æ¶ˆæ¯ï¼Œé»˜è®¤ä¸ºTrue
            stop_event: åœæ­¢äº‹ä»¶ï¼Œç”¨äºæ”¯æŒä¼˜é›…å…³é—­ï¼Œé»˜è®¤ä¸ºNone
        """
        # TODO: å¦‚æœmsg_cacheä¸­æ¶ˆæ¯ä½“ç§¯å¤ªå¤§ï¼Œéœ€è¦è°ƒæ•´æ‹‰å–é—´éš”
        if self._config.pull_interval > 0:
            if has_messages:
                # æ‹‰å–åˆ°æ¶ˆæ¯ï¼Œä¸ä¼‘çœ ç»§ç»­æ‹‰å–
                logger.debug(
                    "Messages pulled, continuing without interval",
                    extra={
                        "consumer_group": self._config.consumer_group,
                    },
                )
            else:
                # æ‹‰å–ä¸ºç©ºï¼Œä¼‘çœ é…ç½®çš„é—´éš”æ—¶é—´ï¼Œä½¿ç”¨å¯ä¸­æ–­ç­‰å¾…
                sleep_time: float = self._config.pull_interval / 1000.0
                logger.debug(
                    f"No messages pulled, sleeping for {sleep_time}s",
                    extra={
                        "consumer_group": self._config.consumer_group,
                        "sleep_time": sleep_time,
                    },
                )
                if stop_event:
                    stop_event.wait(timeout=sleep_time)
                else:
                    time.sleep(sleep_time)

    def _get_or_create_process_queue(self, queue: MessageQueue) -> ProcessQueue:
        """è·å–æˆ–åˆ›å»ºæŒ‡å®šé˜Ÿåˆ—çš„ProcessQueueï¼ˆæ¶ˆæ¯ç¼“å­˜é˜Ÿåˆ—ï¼‰"""
        with self._cache_lock:
            if queue not in self._msg_cache:
                self._msg_cache[queue] = ProcessQueue(
                    max_cache_count=self._config.max_cache_count_per_queue,
                    max_cache_size_mb=self._config.max_cache_size_per_queue,
                )
            return self._msg_cache[queue]

    def _add_messages_to_cache(
        self, queue: MessageQueue, messages: list[MessageExt]
    ) -> None:
        """
        å°†æ¶ˆæ¯æ·»åŠ åˆ°ProcessQueueç¼“å­˜ä¸­

        æ­¤æ–¹æ³•ç”¨äºå°†ä»Brokeræ‹‰å–çš„æ¶ˆæ¯æ·»åŠ åˆ°ProcessQueueä¸­ï¼Œä¸ºåç»­æ¶ˆè´¹åšå‡†å¤‡ã€‚
        ProcessQueueè‡ªåŠ¨ä¿æŒæŒ‰queue_offsetæ’åºï¼Œå¹¶æä¾›é«˜æ•ˆçš„æ’å…¥ã€æŸ¥è¯¢å’Œç»Ÿè®¡åŠŸèƒ½ã€‚

        Args:
            queue (MessageQueue): ç›®æ ‡æ¶ˆæ¯é˜Ÿåˆ—
            messages (list[MessageExt]): è¦æ·»åŠ çš„æ¶ˆæ¯åˆ—è¡¨ï¼Œæ¶ˆæ¯åº”åŒ…å«æœ‰æ•ˆçš„queue_offset

        Note:
            - ä½¿ç”¨ProcessQueueå†…ç½®çš„çº¿ç¨‹å®‰å…¨æœºåˆ¶
            - æŒ‰queue_offsetå‡åºæ’åˆ—ï¼Œæ–¹ä¾¿åç»­æŒ‰åºæ¶ˆè´¹
            - è‡ªåŠ¨è¿‡æ»¤ç©ºæ¶ˆæ¯åˆ—è¡¨ï¼Œé¿å…ä¸å¿…è¦çš„æ“ä½œ
            - è‡ªåŠ¨å»é‡ï¼Œé¿å…é‡å¤ç¼“å­˜ç›¸åŒåç§»é‡çš„æ¶ˆæ¯
            - è‡ªåŠ¨æ£€æŸ¥ç¼“å­˜é™åˆ¶ï¼ˆæ•°é‡å’Œå¤§å°ï¼‰

        Raises:
            æ— å¼‚å¸¸æŠ›å‡ºï¼Œç¡®ä¿æ¶ˆæ¯æ·»åŠ æµç¨‹çš„ç¨³å®šæ€§

        See Also:
            _remove_messages_from_cache: ä»ç¼“å­˜ä¸­ç§»é™¤å·²å¤„ç†çš„æ¶ˆæ¯
            _get_or_create_process_queue: è·å–æˆ–åˆ›å»ºProcessQueue
            _is_message_cached: æ£€æŸ¥æ¶ˆæ¯æ˜¯å¦å·²åœ¨ç¼“å­˜ä¸­
        """
        if not messages:
            return

        process_queue: ProcessQueue = self._get_or_create_process_queue(queue)
        _ = process_queue.add_batch_messages(messages)

    def _is_message_cached(self, queue: MessageQueue, queue_offset: int) -> bool:
        """æ£€æŸ¥æŒ‡å®šåç§»é‡çš„æ¶ˆæ¯æ˜¯å¦å·²åœ¨ProcessQueueç¼“å­˜ä¸­ã€‚

        ä½¿ç”¨ProcessQueueçš„é«˜æ•ˆæŸ¥æ‰¾æœºåˆ¶æ£€æŸ¥æ¶ˆæ¯æ˜¯å¦å·²å­˜åœ¨ï¼Œé¿å…é‡å¤ç¼“å­˜ã€‚

        Args:
            queue: æ¶ˆæ¯é˜Ÿåˆ—
            queue_offset: è¦æ£€æŸ¥çš„æ¶ˆæ¯åç§»é‡

        Returns:
            bool: Trueè¡¨ç¤ºæ¶ˆæ¯å·²å­˜åœ¨ï¼ŒFalseè¡¨ç¤ºä¸å­˜åœ¨
        """
        with self._cache_lock:
            if queue not in self._msg_cache:
                return False

        # ä½¿ç”¨ProcessQueueçš„contains_messageæ–¹æ³•
        process_queue = self._msg_cache[queue]
        return process_queue.contains_message(queue_offset)

    def _remove_messages_from_cache(
        self, queue: MessageQueue, messages: list[MessageExt]
    ) -> int | None:
        """
        ä»ProcessQueueç¼“å­˜ä¸­ç§»é™¤å·²å¤„ç†çš„æ¶ˆæ¯ï¼Œå¹¶è¿”å›å½“å‰é˜Ÿåˆ—çš„æœ€å°offset

        æ­¤æ–¹æ³•ç”¨äºä»ProcessQueueä¸­ç§»é™¤å·²ç»æˆåŠŸå¤„ç†çš„æ¶ˆæ¯ï¼Œé‡Šæ”¾å†…å­˜ç©ºé—´ã€‚
        ProcessQueueæä¾›é«˜æ•ˆçš„ç§»é™¤æ“ä½œï¼Œç¡®ä¿åœ¨å¤§é‡æ¶ˆæ¯ç¼“å­˜ä¸­ä»èƒ½ä¿æŒè‰¯å¥½çš„æ€§èƒ½ã€‚
        ç§»é™¤å®Œæˆåç›´æ¥è¿”å›å½“å‰ç¼“å­˜ä¸­æœ€å°æ¶ˆæ¯çš„offsetï¼Œé¿å…é¢å¤–çš„æŸ¥è¯¢æ“ä½œã€‚

        Args:
            queue (MessageQueue): ç›®æ ‡æ¶ˆæ¯é˜Ÿåˆ—
            messages (list[MessageExt]): è¦ç§»é™¤çš„æ¶ˆæ¯åˆ—è¡¨ï¼Œæ¶ˆæ¯åº”åŒ…å«æœ‰æ•ˆçš„queue_offset

        Returns:
            int | None: ç§»é™¤å®Œæˆåç¼“å­˜ä¸­æœ€å°æ¶ˆæ¯çš„offsetï¼Œå¦‚æœç¼“å­˜ä¸ºç©ºåˆ™è¿”å›None

        Note:
            - ä½¿ç”¨ProcessQueueå†…ç½®çš„çº¿ç¨‹å®‰å…¨æœºåˆ¶
            - ProcessQueueæä¾›é«˜æ•ˆçš„remove_messageæ“ä½œ
            - åªç§»é™¤å®Œå…¨åŒ¹é…çš„æ¶ˆæ¯ï¼ˆqueue_offsetç›¸åŒï¼‰ï¼Œé¿å…è¯¯åˆ 
            - è‡ªåŠ¨è¿‡æ»¤ç©ºæ¶ˆæ¯åˆ—è¡¨ï¼Œå‡å°‘ä¸å¿…è¦çš„æ“ä½œ
            - å¦‚æœæ¶ˆæ¯æœªæ‰¾åˆ°ï¼Œé™é»˜è·³è¿‡ï¼Œä¸å½±å“å…¶ä»–æ¶ˆæ¯çš„å¤„ç†
            - ç§»é™¤å®Œæˆåç›´æ¥è¿”å›æœ€å°offsetï¼Œæé«˜æ€§èƒ½

        Performance:
            - æ—¶é—´å¤æ‚åº¦: O(m * log n)ï¼Œå…¶ä¸­mæ˜¯è¦ç§»é™¤çš„æ¶ˆæ¯æ•°ï¼Œnæ˜¯ç¼“å­˜ä¸­çš„æ¶ˆæ¯æ•°
            - ç©ºé—´å¤æ‚åº¦: O(1)ï¼Œé¢å¤–ç©ºé—´ä»…ç”¨äºä¸´æ—¶å˜é‡

        Raises:
            æ— å¼‚å¸¸æŠ›å‡ºï¼Œç¡®ä¿æ¶ˆæ¯ç§»é™¤æµç¨‹çš„ç¨³å®šæ€§

        See Also:
            _add_messages_to_cache: å‘ç¼“å­˜ä¸­æ·»åŠ æ¶ˆæ¯
            _update_offset_from_cache: æ›´æ–°æ¶ˆè´¹åç§»é‡ï¼ˆç‹¬ç«‹æ–¹æ³•ï¼‰
            _get_or_create_process_queue: è·å–æˆ–åˆ›å»ºProcessQueue
        """
        process_queue = self._get_or_create_process_queue(queue)

        if not messages:
            # å¦‚æœæ²¡æœ‰æ¶ˆæ¯è¦ç§»é™¤ï¼Œç›´æ¥è¿”å›å½“å‰æœ€å°offset
            return process_queue.get_min_offset()

        _ = process_queue.remove_batch_messages(
            [x.queue_offset for x in messages if x.queue_offset is not None]
        )

        # è¿”å›ç§»é™¤å®Œæˆåå½“å‰ç¼“å­˜ä¸­çš„æœ€å°offset
        return process_queue.get_min_offset()

    def _update_offset_from_cache(self, queue: MessageQueue) -> None:
        """ä»ProcessQueueç¼“å­˜ä¸­è·å–æœ€å°offsetå¹¶æ›´æ–°åˆ°offset_store"""
        with self._cache_lock:
            if queue not in self._msg_cache:
                # ProcessQueueä¸å­˜åœ¨ï¼Œä¸éœ€è¦æ›´æ–°
                return

        process_queue: ProcessQueue = self._msg_cache[queue]

        # è·å–ç¼“å­˜ä¸­æœ€å°çš„offset
        min_offset: int | None = process_queue.get_min_offset()
        if min_offset is None:
            # ç¼“å­˜ä¸ºç©ºï¼Œä¸éœ€è¦æ›´æ–°
            return

        # æ›´æ–°åˆ°offset_store
        try:
            self._offset_store.update_offset(queue, min_offset)
            logger.debug(
                f"Updated offset from cache: {min_offset}",
                extra={
                    "consumer_group": self._config.consumer_group,
                    "topic": queue.topic,
                    "queue_id": queue.queue_id,
                    "offset": min_offset,
                    "cache_stats": process_queue.get_stats(),
                },
            )
        except Exception as e:
            logger.warning(
                f"Failed to update offset from cache: {e}",
                extra={
                    "consumer_group": self._config.consumer_group,
                    "topic": queue.topic,
                    "queue_id": queue.queue_id,
                    "error": str(e),
                },
            )

    def _get_or_initialize_offset(self, message_queue: MessageQueue) -> int:
        """è·å–æˆ–åˆå§‹åŒ–æ¶ˆè´¹åç§»é‡ã€‚

        å¦‚æœæœ¬åœ°ç¼“å­˜çš„åç§»é‡ä¸º0ï¼ˆé¦–æ¬¡æ¶ˆè´¹ï¼‰ï¼Œåˆ™æ ¹æ®é…ç½®çš„æ¶ˆè´¹ç­–ç•¥
        ä»ConsumeFromWhereManagerè·å–æ­£ç¡®çš„åˆå§‹åç§»é‡ã€‚

        Args:
            message_queue (MessageQueue): è¦è·å–åç§»é‡çš„æ¶ˆæ¯é˜Ÿåˆ—

        Returns:
            int: æ¶ˆè´¹åç§»é‡

        Note:
            - å¦‚æœåç§»é‡ä¸ä¸º0ï¼Œç›´æ¥è¿”å›ç¼“å­˜çš„å€¼
            - å¦‚æœåç§»é‡ä¸º0ï¼Œæ ¹æ®consume_from_whereç­–ç•¥è·å–åˆå§‹åç§»é‡
            - è·å–å¤±è´¥æ—¶ä½¿ç”¨é»˜è®¤åç§»é‡0ï¼Œç¡®ä¿æ¶ˆè´¹æµç¨‹ä¸ä¸­æ–­
        """
        with self._assigned_queues_lock:  # ğŸ”ä¿æŠ¤_assigned_queuesè®¿é—®
            current_offset: int = self._assigned_queues.get(message_queue, 0)

            # å¦‚æœcurrent_offsetä¸º0ï¼ˆé¦–æ¬¡æ¶ˆè´¹ï¼‰ï¼Œåˆ™ä»_consume_from_where_managerä¸­è·å–æ­£ç¡®çš„åˆå§‹åç§»é‡
            if current_offset == 0:
                try:
                    current_offset = (
                        self._consume_from_where_manager.get_consume_offset(
                            message_queue,
                            self._config.consume_from_where,
                            self._config.consume_timestamp
                            if hasattr(self._config, "consume_timestamp")
                            else 0,
                        )
                    )
                    # æ›´æ–°æœ¬åœ°ç¼“å­˜çš„åç§»é‡
                    self._assigned_queues[message_queue] = current_offset

                    logger.info(
                        f"åˆå§‹åŒ–æ¶ˆè´¹åç§»é‡: {current_offset}",
                        extra={
                            "consumer_group": self._config.consumer_group,
                            "topic": message_queue.topic,
                            "queue_id": message_queue.queue_id,
                            "strategy": self._config.consume_from_where,
                            "offset": current_offset,
                        },
                    )

                except Exception as e:
                    logger.error(
                        f"è·å–åˆå§‹æ¶ˆè´¹åç§»é‡å¤±è´¥ï¼Œä½¿ç”¨é»˜è®¤åç§»é‡0: {e}",
                        extra={
                            "consumer_group": self._config.consumer_group,
                            "topic": message_queue.topic,
                            "queue_id": message_queue.queue_id,
                            "strategy": self._config.consume_from_where,
                            "error": str(e),
                        },
                        exc_info=True,
                    )
                    # ä½¿ç”¨é»˜è®¤åç§»é‡0
                    current_offset = 0

        return current_offset

    def _pull_messages(
        self, message_queue: MessageQueue, offset: int, suggest_id: int
    ) -> tuple[list[MessageExt], int, int]:
        """ä»æŒ‡å®šé˜Ÿåˆ—æ‹‰å–æ¶ˆæ¯ï¼Œæ”¯æŒåç§»é‡ç®¡ç†å’ŒBrokeré€‰æ‹©ã€‚

        è¯¥æ–¹æ³•æ˜¯é¡ºåºæ¶ˆè´¹è€…çš„æ ¸å¿ƒæ‹‰å–é€»è¾‘ï¼Œè´Ÿè´£ä»RocketMQ Brokeræ‹‰å–æ¶ˆæ¯ï¼Œ
        å¹¶å¤„ç†ç›¸å…³çš„ç³»ç»Ÿæ ‡å¿—ä½å’Œåç§»é‡ç®¡ç†ã€‚æ”¯æŒä¸»å¤‡Brokerçš„æ™ºèƒ½é€‰æ‹©
        å’Œæ•…éšœè½¬ç§»æœºåˆ¶ã€‚

        æ ¸å¿ƒåŠŸèƒ½:
        - é€šè¿‡NameServerManagerè·å–æœ€ä¼˜Brokeråœ°å€
        - æ„å»ºæ‹‰å–è¯·æ±‚çš„ç³»ç»Ÿæ ‡å¿—ä½
        - å¤„ç†commit offsetçš„æäº¤é€»è¾‘
        - æ”¯æŒæ‰¹é‡æ¶ˆæ¯æ‹‰å–ä»¥æé«˜æ•ˆç‡
        - å®Œå–„çš„é”™è¯¯å¤„ç†å’Œé‡è¯•æœºåˆ¶

        æ‹‰å–ç­–ç•¥:
        1. è·å–ç›®æ ‡Brokeråœ°å€ï¼Œä¼˜å…ˆè¿æ¥master
        2. è¯»å–å½“å‰commit offsetï¼ˆå¦‚æœæœ‰ï¼‰
        3. æ„å»ºåŒ…å«commitæ ‡å¿—çš„ç³»ç»Ÿæ ‡å¿—ä½
        4. å‘é€PULL_MESSAGEè¯·æ±‚åˆ°Broker
        5. è§£æå“åº”å¹¶è¿”å›æ¶ˆæ¯åˆ—è¡¨å’Œä¸‹æ¬¡æ‹‰å–ä½ç½®

        è¿”å›å€¼è¯´æ˜:
        - list[MessageExt]: æ‹‰å–åˆ°çš„æ¶ˆæ¯åˆ—è¡¨ï¼Œå¯èƒ½ä¸ºç©º
        - int: ä¸‹ä¸€æ¬¡æ‹‰å–çš„èµ·å§‹åç§»é‡
        - int: å»ºè®®ä¸‹æ¬¡è¿æ¥çš„Broker IDï¼ˆ0=master, å…¶ä»–=slaveï¼‰

        Args:
            message_queue (MessageQueue): ç›®æ ‡æ¶ˆæ¯é˜Ÿåˆ—ï¼ŒåŒ…å«topicã€brokeråç§°ã€é˜Ÿåˆ—IDç­‰ä¿¡æ¯
            offset (int): æœ¬æ¬¡æ‹‰å–çš„èµ·å§‹åç§»é‡ï¼Œä»è¯¥ä½ç½®å¼€å§‹æ‹‰å–æ¶ˆæ¯
            suggest_id (int): å»ºè®®çš„Broker IDï¼Œç”¨äºè¿æ¥é€‰æ‹©ä¼˜åŒ–ï¼Œ
                            é€šå¸¸ä¸ºä¸Šæ¬¡æ‹‰å–æ—¶è¿”å›çš„å»ºè®®ID

        Returns:
            tuple[list[MessageExt], int, int]: ä¸‰å…ƒç»„åŒ…å«ï¼š
                                            - æ¶ˆæ¯åˆ—è¡¨ï¼ˆå¯èƒ½ä¸ºç©ºï¼‰
                                            - ä¸‹æ¬¡æ‹‰å–çš„èµ·å§‹åç§»é‡
                                            - å»ºè®®çš„ä¸‹æ¬¡Broker ID

        Raises:
            MessageConsumeError: å½“æ‹‰å–è¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯æ—¶æŠ›å‡ºï¼ŒåŒ…å«è¯¦ç»†çš„é”™è¯¯ä¿¡æ¯
            ValueError: å½“æ— æ³•æ‰¾åˆ°æŒ‡å®šbrokerçš„åœ°å€æ—¶æŠ›å‡º

        Example:
            ```python
            # æ‹‰å–æ¶ˆæ¯ç¤ºä¾‹
            messages, next_offset, suggested_broker = consumer._pull_messages(
                message_queue=MessageQueue("test_topic", "broker-a", 0),
                offset=100,
                suggest_id=0
            )

            if messages:
                for msg in messages:
                    print(f"æ¶ˆæ¯å†…å®¹: {msg.body.decode()}")
                print(f"ä¸‹æ¬¡æ‹‰å–åç§»é‡: {next_offset}")
                if suggested_broker != 0:
                    print(f"å»ºè®®ä¸‹æ¬¡è¿æ¥slave broker: {suggested_broker}")
            ```

        Note:
            - è¯¥æ–¹æ³•ä¼šè¢«_pull_messages_loopå¾ªç¯è°ƒç”¨ï¼Œå®ç°æŒç»­çš„æ¶ˆæ¯æ‹‰å–
            - suggest_idå‚æ•°ç”¨äºBrokeré€‰æ‹©çš„ä¼˜åŒ–ï¼Œé€šå¸¸æ¥è‡ªä¸Šæ¬¡æ‹‰å–å“åº”
            - commit_offsetåªåœ¨è¿æ¥master brokerä¸”å­˜åœ¨å·²æäº¤åç§»é‡æ—¶ä½¿ç”¨
            - æ‹‰å–å¤±è´¥æ—¶ä¼šè®°å½•è¯¦ç»†çš„é”™è¯¯ä¿¡æ¯ï¼Œä¾¿äºé—®é¢˜è¯Šæ–­
            - è¿”å›çš„emptyæ¶ˆæ¯åˆ—è¡¨å¹¶ä¸æ„å‘³ç€é˜Ÿåˆ—ä¸­æ²¡æœ‰æ¶ˆæ¯ï¼Œå¯èƒ½æ˜¯ç”±äºç½‘ç»œå»¶è¿Ÿ
        """
        try:
            self._stats["pull_requests"] += 1

            broker_info: tuple[str, bool] | None = (
                self._name_server_manager.get_broker_address_in_subscription(
                    message_queue.broker_name, suggest_id
                )
            )
            if not broker_info:
                raise ValueError(
                    f"Broker address not found for {message_queue.broker_name}"
                )

            commit_offset: int = self._offset_store.read_offset(
                message_queue, ReadOffsetType.READ_FROM_MEMORY
            )

            broker_address, is_master = broker_info

            # ä½¿ç”¨BrokerManageræ‹‰å–æ¶ˆæ¯
            pool: ConnectionPool = self._broker_manager.must_connection_pool(
                broker_address
            )
            self._prepare_consumer_remote(pool)
            with pool.get_connection(usage="æ‹‰å–æ¶ˆæ¯") as conn:
                result: PullMessageResult = BrokerClient(conn).pull_message(
                    consumer_group=self._config.consumer_group,
                    topic=message_queue.topic,
                    queue_id=message_queue.queue_id,
                    queue_offset=offset,
                    max_msg_nums=self._config.pull_batch_size,
                    sys_flag=self._build_sys_flag(
                        commit_offset=commit_offset > 0 and is_master
                    ),
                    commit_offset=commit_offset,
                )

                if result.messages:
                    return (
                        result.messages,
                        result.next_begin_offset,
                        result.suggest_which_broker_id or 0,
                    )

                return [], offset, 0

        except MessagePullError as e:
            logger.warning(
                "The pull request is illegal",
                extra={
                    "consumer_group": self._config.consumer_group,
                    "topic": message_queue.topic,
                    "queue_id": message_queue.queue_id,
                    "offset": offset,
                    "error": str(e),
                },
            )
            raise e

        except Exception as e:
            logger.warning(
                "Failed to pull messages",
                extra={
                    "consumer_group": self._config.consumer_group,
                    "topic": message_queue.topic,
                    "queue_id": message_queue.queue_id,
                    "offset": offset,
                    "error": str(e),
                },
            )
            raise MessageConsumeError(
                message_queue.topic,
                "Failed to pull messages",
                offset,
                cause=e,
            ) from e

    def _build_sys_flag(self, commit_offset: bool):
        """æ„å»ºç³»ç»Ÿæ ‡å¿—ä½

        æ ¹æ®Goè¯­è¨€å®ç°ï¼š
        - bit 0 (0x1): commitOffset æ ‡å¿—
        - bit 1 (0x2): suspend æ ‡å¿—
        - bit 2 (0x4): subscription æ ‡å¿—
        - bit 3 (0x8): classFilter æ ‡å¿—

        Args:
            commit_offset (bool): æ˜¯å¦æäº¤åç§»é‡

        Returns:
            int: ç³»ç»Ÿæ ‡å¿—ä½
        """
        flag = 0

        if commit_offset:
            flag |= 0x1 << 0  # bit 0: 0x1

        # suspend: always true
        flag |= 0x1 << 1  # bit 1: 0x2

        # subscription: always true
        flag |= 0x1 << 2  # bit 2: 0x4

        # class_filter: always false
        # flag |= 0x1 << 3  # bit 3: 0x8

        return flag

    # ==================== å†…éƒ¨æ–¹æ³•ï¼šæ¶ˆæ¯å¤„ç† ====================

    def _start_consume_tasks_for_queues(self, queues: set[MessageQueue]) -> None:
        """
        ä¸ºæŒ‡å®šçš„é˜Ÿåˆ—é›†åˆå¯åŠ¨æ¶ˆè´¹ä»»åŠ¡

        Args:
            queues: éœ€è¦å¯åŠ¨æ¶ˆè´¹ä»»åŠ¡çš„é˜Ÿåˆ—é›†åˆ
        """
        if not self._consume_executor:
            return

        for message_queue in queues:
            # è·å–è¯¥é˜Ÿåˆ—çš„åœæ­¢äº‹ä»¶
            with self._stop_events_lock:
                if message_queue in self._consume_stop_events:
                    consume_stop_event = self._consume_stop_events[message_queue]
                else:
                    # å¦‚æœäº‹ä»¶ä¸å­˜åœ¨ï¼Œåˆ›å»ºä¸€ä¸ªï¼ˆè™½ç„¶æ­£å¸¸æƒ…å†µä¸‹åº”è¯¥å·²ç»å­˜åœ¨ï¼‰
                    consume_stop_event = threading.Event()
                    self._consume_stop_events[message_queue] = consume_stop_event

            # å¯åŠ¨æ¶ˆè´¹ä»»åŠ¡ï¼Œä¼ å…¥åœæ­¢äº‹ä»¶
            future = self._consume_executor.submit(
                self._consume_messages_loop, message_queue, consume_stop_event
            )
            if message_queue not in self._consume_tasks:
                self._consume_tasks[message_queue] = []
            self._consume_tasks[message_queue].append(future)

    def _consume_messages_loop(
        self, message_queue: MessageQueue, stop_event: threading.Event
    ) -> None:
        """
        æŒç»­å¤„ç†æŒ‡å®šMessageQueueçš„æ¶ˆæ¯æ¶ˆè´¹å¾ªç¯ã€‚

        è¿™æ˜¯é¡ºåºæ¶ˆè´¹è€…æ ¸å¿ƒçš„æ¶ˆæ¯å¤„ç†å¾ªç¯ï¼Œæ¯ä¸ªMessageQueueè¿è¡Œåœ¨ç‹¬ç«‹çš„çº¿ç¨‹ä¸­ã€‚
        è¯¥å¾ªç¯è´Ÿè´£ä»å¯¹åº”MessageQueueçš„å¤„ç†é˜Ÿåˆ—ä¸­è·å–æ¶ˆæ¯å¹¶è°ƒç”¨ç”¨æˆ·æ³¨å†Œçš„æ¶ˆæ¯ç›‘å¬å™¨è¿›è¡Œå¤„ç†ã€‚

        Args:
            message_queue: è¦å¤„ç†çš„æŒ‡å®šMessageQueue
            stop_event: æ¶ˆè´¹çº¿ç¨‹åœæ­¢äº‹ä»¶

        ä¸»è¦åŠŸèƒ½:
            - ä»MessageQueueä¸“å±çš„å¤„ç†é˜Ÿåˆ—é˜»å¡å¼è·å–æ¶ˆæ¯æ‰¹æ¬¡
            - è°ƒç”¨ç”¨æˆ·æ¶ˆæ¯ç›‘å¬å™¨è¿›è¡Œä¸šåŠ¡å¤„ç†ï¼Œä¿è¯åŒä¸€é˜Ÿåˆ—å†…çš„æ¶ˆæ¯é¡ºåºæ€§
            - æ ¹æ®å¤„ç†ç»“æœæ‰§è¡ŒæˆåŠŸ/å¤±è´¥åçš„å¤„ç†é€»è¾‘
            - æ›´æ–°æ¶ˆè´¹ç»Ÿè®¡ä¿¡æ¯
            - å¤„ç†é‡è¯•æœºåˆ¶å’Œå¼‚å¸¸æƒ…å†µ

        å¤„ç†æµç¨‹:
            1. ä»MessageQueueä¸“å±å¤„ç†é˜Ÿåˆ—è·å–æ¶ˆæ¯æ‰¹æ¬¡ (_get_messages_from_queue)
            2. è°ƒç”¨æ¶ˆæ¯ç›‘å¬å™¨å¤„ç†æ¶ˆæ¯ (_process_messages_with_timing)
            3. æ ¹æ®å¤„ç†ç»“æœæ‰§è¡Œåç»­æ“ä½œï¼š
               - æˆåŠŸ: æ›´æ–°åç§»é‡ï¼Œæ¸…ç†å¤„ç†é˜Ÿåˆ—
               - å¤±è´¥: å‘é€å›brokerè¿›è¡Œé‡è¯•
            4. æ›´æ–°æ¶ˆè´¹ç»Ÿè®¡ä¿¡æ¯ (_update_consume_stats)
            5. å¯¹å‘é€å›brokerå¤±è´¥çš„æ¶ˆæ¯è¿›è¡Œå»¶è¿Ÿé‡è¯•

        é¡ºåºæ€§ä¿è¯:
            - æ¯ä¸ªMessageQueueæœ‰ç‹¬ç«‹çš„å¤„ç†é˜Ÿåˆ—å’Œæ¶ˆè´¹çº¿ç¨‹
            - ç¡®ä¿åŒä¸€é˜Ÿåˆ—å†…çš„æ¶ˆæ¯ä¸¥æ ¼æŒ‰ç…§åç§»é‡é¡ºåºå¤„ç†
            - é¿å…ä¸åŒé˜Ÿåˆ—ä¹‹é—´çš„æ¶ˆæ¯äº¤å‰å¤„ç†

        é‡è¯•æœºåˆ¶:
            - æ¶ˆè´¹å¤±è´¥çš„æ¶ˆæ¯ä¼šå°è¯•å‘é€å›brokerè¿›è¡Œé‡è¯•
            - å‘é€å›brokerå¤±è´¥çš„æ¶ˆæ¯ä¼šåœ¨æœ¬åœ°å»¶è¿Ÿ5ç§’åé‡è¯•
            - é¿å…å› brokerè¿æ¥é—®é¢˜å¯¼è‡´çš„æ¶ˆæ¯ä¸¢å¤±

        çº¿ç¨‹æ¨¡å‹:
            - æ¯ä¸ªMessageQueueåˆ†é…ä¸€ä¸ªç‹¬ç«‹çš„æ¶ˆè´¹çº¿ç¨‹
            - ä¸åŒé˜Ÿåˆ—çš„æ¶ˆè´¹è€…å¹¶è¡Œè¿è¡Œï¼Œäº’ä¸å¹²æ‰°
            - åŒä¸€é˜Ÿåˆ—å†…çš„æ¶ˆæ¯ä¸¥æ ¼ä¸²è¡Œå¤„ç†

        å¼‚å¸¸å¤„ç†:
            - æ•è·æ‰€æœ‰å¼‚å¸¸ï¼Œç¡®ä¿å•ä¸ªæ¶ˆæ¯å¤„ç†å¤±è´¥ä¸å½±å“æ•´ä¸ªå¾ªç¯
            - è®°å½•è¯¦ç»†çš„é”™è¯¯æ—¥å¿—å’Œå¼‚å¸¸å †æ ˆä¿¡æ¯
            - ä¿æŒå¾ªç¯ç»§ç»­è¿è¡Œï¼Œç¡®ä¿æ¶ˆè´¹è€…æœåŠ¡å¯ç”¨

        Note:
            - è¯¥æ–¹æ³•æ˜¯æ¶ˆè´¹è€…æ¶ˆæ¯å¤„ç†çš„æ ¸å¿ƒé€»è¾‘ï¼Œä¸åº”è¢«å¤–éƒ¨è°ƒç”¨
            - å¾ªç¯ä¼šåœ¨æ¶ˆè´¹è€…å…³é—­æ—¶(_is_running=False)è‡ªåŠ¨é€€å‡º
            - æ”¯æŒé€šè¿‡åœæ­¢äº‹ä»¶ä¼˜é›…å…³é—­
            - æ¶ˆè´¹å»¶è¿Ÿä¸»è¦å–å†³äºæ¶ˆæ¯å¤„ç†æ—¶é—´å’Œé˜Ÿåˆ—æ·±åº¦
            - ç»Ÿè®¡ä¿¡æ¯ç”¨äºç›‘æ§æ¶ˆè´¹æ€§èƒ½å’Œå¥åº·çŠ¶æ€
        """
        logger.info(
            f"Starting consume message loop for queue {message_queue}",
            extra={
                "consumer_group": self._config.consumer_group,
                "topic": message_queue.topic,
                "queue_id": message_queue.queue_id,
            },
        )

        while self._is_running and not stop_event.is_set():
            queue_lock: threading.RLock = self._get_queue_lock(message_queue)
            try:
                # å¼€å§‹æ¶ˆè´¹è¿™ä¸ªmessage_queueä¹‹å‰ï¼Œå…ˆæŒæœ‰æœ¬åœ°é”ï¼Œå¦‚æœæŒæœ‰å¤±è´¥ï¼Œåˆ™ç­‰å¾…10ms
                lock_acquired = False

                # å°è¯•éé˜»å¡è·å–é”ï¼Œå¦‚æœå¤±è´¥åˆ™ç­‰å¾…10msåé‡è¯•
                while (
                    not lock_acquired and self._is_running and not stop_event.is_set()
                ):
                    lock_acquired = queue_lock.acquire(blocking=False)
                    if not lock_acquired:
                        # ç­‰å¾…10ms
                        if stop_event.wait(timeout=0.01):
                            break

                # å¦‚æœè·å–é”å¤±è´¥æˆ–æ¶ˆè´¹è€…åœæ­¢ï¼Œåˆ™ç»§ç»­ä¸‹ä¸€è½®å¾ªç¯
                if not lock_acquired or not self._is_running:
                    if lock_acquired:
                        queue_lock.release()
                    continue

                # æœ¬åœ°é”æŒæœ‰æˆåŠŸï¼Œæ£€æŸ¥è¿œç¨‹é”æ˜¯å¦éœ€è¦é‡æ–°è·å–
                if not self._is_remote_lock_valid(message_queue):
                    # è¿œç¨‹é”å·²è¿‡æœŸæˆ–ä¸å­˜åœ¨ï¼Œéœ€è¦é‡æ–°è·å–
                    if not self._lock_remote_queue(message_queue):
                        logger.debug(
                            f"Failed to acquire remote lock for queue {message_queue}, skipping this round",
                            extra={
                                "consumer_group": self._config.consumer_group,
                                "client_id": self._config.client_id,
                                "queue": str(message_queue),
                                "operation": "consume_messages_loop",
                            },
                        )
                        # é‡Šæ”¾æœ¬åœ°é”å¹¶ç»§ç»­ä¸‹ä¸€è½®å¾ªç¯
                        queue_lock.release()
                        # ç­‰å¾…3ç§’ä»¥å‡å°‘é”ç«äº‰é¢‘ç‡
                        if stop_event.wait(timeout=3.0):
                            break
                        continue
                else:
                    logger.debug(
                        f"Using cached remote lock for queue {message_queue}",
                        extra={
                            "consumer_group": self._config.consumer_group,
                            "client_id": self._config.client_id,
                            "queue": str(message_queue),
                            "operation": "consume_messages_loop",
                            "lock_cached": True,
                        },
                    )

                pq: ProcessQueue | None = self._get_or_create_process_queue(
                    message_queue
                )
                messages: list[MessageExt] = pq.take_messages(
                    self._config.consume_batch_size
                )
                if not messages:
                    stop_event.wait(timeout=3.0)

                for msg in messages:
                    self._reset_retry(msg)

                while True:
                    # while messages and self._is_running and not stop_event.is_set():
                    success, result, duration = self._process_messages_with_timing(
                        messages, message_queue
                    )
                    offset: int = -1
                    if self._config.enable_auto_commit:
                        # è‡ªåŠ¨æäº¤æ¨¡å¼
                        if success:
                            offset = pq.commit()
                            if offset:
                                self._offset_store.update_offset(message_queue, offset)
                            break
                        else:
                            if self.check_reconsume_times(message_queue, messages):
                                stop_event.wait(timeout=1.0)
                            else:
                                offset = pq.commit()
                                if offset:
                                    self._offset_store.update_offset(
                                        message_queue, offset
                                    )
                                break

                    else:
                        # æ‰‹åŠ¨æäº¤æ¨¡å¼
                        if success:
                            if result == ConsumeResult.SUCCESS:
                                # å•¥ä¹Ÿä¸åš, ç­‰å¾…ä¸‹æ¬¡ä¸€èµ·commit
                                pass
                            else:
                                # commit
                                offset = pq.commit()
                                if offset:
                                    self._offset_store.update_offset(
                                        message_queue, offset
                                    )
                            break
                        else:
                            if result == ConsumeResult.ROLLBACK:
                                _ = pq.rollback(messages)
                                stop_event.wait(timeout=1.0)
                                break
                            elif result == ConsumeResult.RECONSUME_LATER:
                                if self.check_reconsume_times(message_queue, messages):
                                    stop_event.wait(timeout=1.0)
                                else:
                                    break

                    self._update_consume_stats(success, duration, len(messages))

            except Exception as e:
                logger.error(
                    f"Error in consume messages loop for queue {message_queue}: {e}",
                    extra={
                        "consumer_group": self._config.consumer_group,
                        "topic": message_queue.topic,
                        "queue_id": message_queue.queue_id,
                        "error": str(e),
                    },
                    exc_info=True,
                )

            finally:
                # é‡Šæ”¾æœ¬åœ°é”
                if queue_lock.locked():
                    queue_lock.release()

    def _take_messages(self) -> list[MessageExt] | None:
        """ """
        pass

    def _process_messages_with_timing(
        self, messages: list[MessageExt], message_queue: MessageQueue
    ) -> tuple[bool, ConsumeResult, float]:
        """
        å¤„ç†æ¶ˆæ¯å¹¶è®¡æ—¶

        Args:
            messages: è¦å¤„ç†çš„æ¶ˆæ¯åˆ—è¡¨
            message_queue: æ¶ˆæ¯é˜Ÿåˆ—

        Returns:
            æ¶ˆè´¹ç»“æœï¼ŒåŒ…å«æˆåŠŸçŠ¶æ€å’Œè€—æ—¶
        """

        start_time: float = time.time()
        success, consume_result = self._orderly_consume_message(messages, message_queue)
        duration: float = time.time() - start_time

        return success, consume_result, duration

    def _update_offset_to_store(
        self, message_queue: MessageQueue, min_offset: int
    ) -> None:
        """
        æ›´æ–°åç§»é‡åˆ°å­˜å‚¨

        Args:
            message_queue: æ¶ˆæ¯é˜Ÿåˆ—
            min_offset: æœ€å°åç§»é‡
        """
        try:
            self._offset_store.update_offset(message_queue, min_offset)
            logger.debug(
                f"Updated offset from cache: {min_offset}",
                extra={
                    "consumer_group": self._config.consumer_group,
                    "topic": message_queue.topic,
                    "queue_id": message_queue.queue_id,
                    "offset": min_offset,
                    "cache_stats": self._msg_cache.get(
                        message_queue, ProcessQueue()
                    ).get_stats(),
                },
            )
        except Exception as e:
            logger.warning(
                f"Failed to update offset from cache: {e}",
                extra={
                    "consumer_group": self._config.consumer_group,
                    "topic": message_queue.topic,
                    "queue_id": message_queue.queue_id,
                    "offset": min_offset,
                    "error": str(e),
                },
            )

    def _update_consume_stats(
        self, success: bool, duration: float, message_count: int
    ) -> None:
        """
        æ›´æ–°æ¶ˆè´¹ç»Ÿè®¡ä¿¡æ¯

        Args:
            consume_result: æ¶ˆè´¹ç»“æœ
            message_count: æ¶ˆæ¯æ•°é‡
        """
        # æ›´æ–°æ€»æ¶ˆè´¹æ—¶é•¿
        self._stats["consume_duration_total"] += duration

        # æ›´æ–°å¤±è´¥æ¶ˆæ¯æ•°
        if not success:
            self._stats["messages_failed"] += message_count

    def _wait_for_processing_completion(self) -> None:
        """
        ç­‰å¾…æ­£åœ¨å¤„ç†çš„æ¶ˆæ¯å®Œæˆ
        """
        try:
            # ç­‰å¾…æ‰€æœ‰æ¶ˆè´¹ä»»åŠ¡å®Œæˆ
            timeout: int = 30  # 30ç§’è¶…æ—¶

            # æ”¶é›†æ‰€æœ‰æœªå®Œæˆçš„æ¶ˆè´¹ä»»åŠ¡
            all_futures: list[Future[None]] = []
            for futures in self._consume_tasks.values():
                for future in futures:
                    if future and not future.done():
                        all_futures.append(future)

            # ç­‰å¾…æ‰€æœ‰ä»»åŠ¡å®Œæˆæˆ–è¶…æ—¶
            if all_futures and self._consume_executor:
                _, not_done_futures = concurrent.futures.wait(
                    all_futures, timeout=timeout
                )

                if not_done_futures:
                    logger.warning(
                        f"Timeout waiting for {len(not_done_futures)} consume tasks to complete",
                        extra={
                            "consumer_group": self._config.consumer_group,
                            "timeout": timeout,
                            "remaining_tasks": len(not_done_futures),
                        },
                    )

        except Exception as e:
            logger.warning(
                f"Error waiting for processing completion: {e}",
                extra={
                    "consumer_group": self._config.consumer_group,
                    "error": str(e),
                },
            )

    # ==================== å†…éƒ¨æ–¹æ³•ï¼šé‡å¹³è¡¡ä»»åŠ¡ ====================

    def _start_rebalance_task(self) -> None:
        """
        å¯åŠ¨å®šæœŸé‡å¹³è¡¡ä»»åŠ¡
        """
        self._rebalance_thread = threading.Thread(
            target=self._rebalance_loop,
            name=f"{self._config.consumer_group}-rebalance-thread",
            daemon=True,
        )
        self._rebalance_thread.start()

    def _rebalance_loop(self) -> None:
        """
        å®šæœŸé‡å¹³è¡¡å¾ªç¯
        """
        while self._is_running:
            try:
                # ä½¿ç”¨Event.wait()æ›¿ä»£time.sleep()
                if self._rebalance_event.wait(timeout=self._rebalance_interval):
                    # Eventè¢«è§¦å‘ï¼Œæ£€æŸ¥æ˜¯å¦éœ€è¦é€€å‡º
                    if not self._is_running:
                        break
                    # é‡ç½®äº‹ä»¶çŠ¶æ€
                    self._rebalance_event.clear()

                if self._is_running:
                    self._do_rebalance()

            except Exception as e:
                logger.error(
                    f"Error in rebalance loop: {e}",
                    extra={
                        "consumer_group": self._config.consumer_group,
                        "error": str(e),
                    },
                    exc_info=True,
                )

    def _on_notify_consumer_ids_changed(
        self, _remoting_cmd: RemotingCommand, _remote_addr: tuple[str, int]
    ) -> None:
        logger.info("Received notification of consumer IDs changed")
        self._do_rebalance()

    def _find_consumer_list(self, topic: str) -> list[str]:
        """
        æŸ¥æ‰¾æ¶ˆè´¹è€…åˆ—è¡¨

        Args:
            topic: ä¸»é¢˜åç§°

        Returns:
            æ¶ˆè´¹è€…åˆ—è¡¨
        """

        addresses: list[str] = self._name_server_manager.get_all_broker_addresses(topic)
        if not addresses:
            logger.warning(
                "No broker addresses found for topic", extra={"topic": topic}
            )
            return []

        pool: ConnectionPool = self._broker_manager.must_connection_pool(addresses[0])
        with pool.get_connection(usage="æŸ¥æ‰¾æ¶ˆè´¹è€…åˆ—è¡¨") as conn:
            return BrokerClient(conn).get_consumers_by_group(
                self._config.consumer_group
            )

    # ==================== å†…éƒ¨æ–¹æ³•ï¼šèµ„æºæ¸…ç† ====================

    def _cleanup_on_start_failure(self) -> None:
        """å¯åŠ¨å¤±è´¥æ—¶çš„èµ„æºæ¸…ç†æ“ä½œã€‚

        å½“æ¶ˆè´¹è€…å¯åŠ¨è¿‡ç¨‹ä¸­å‘ç”Ÿå¼‚å¸¸æ—¶ï¼Œè°ƒç”¨æ­¤æ–¹æ³•æ¸…ç†å·²åˆ†é…çš„èµ„æºï¼Œ
        ç¡®ä¿æ¶ˆè´¹è€…çŠ¶æ€ä¸€è‡´ï¼Œé¿å…èµ„æºæ³„æ¼ã€‚

        æ¸…ç†æµç¨‹ï¼š
        1. å…³é—­çº¿ç¨‹æ± ï¼ˆæ‹‰å–çº¿ç¨‹æ± ã€æ¶ˆè´¹çº¿ç¨‹æ± ï¼‰
        2. åœæ­¢æ ¸å¿ƒç»„ä»¶ï¼ˆNameServerã€BrokerManagerã€åç§»é‡å­˜å‚¨ï¼‰
        3. æ¸…ç†å†…å­˜èµ„æºå’Œé˜Ÿåˆ—

        Args:
            None

        Returns:
            None

        Raises:
            None: æ­¤æ–¹æ³•ä¼šæ•è·æ‰€æœ‰å¼‚å¸¸å¹¶è®°å½•æ—¥å¿—

        Note:
            - ä»…åœ¨å¯åŠ¨å¤±è´¥æ—¶è°ƒç”¨ï¼Œæ­£å¸¸å…³é—­ä½¿ç”¨shutdown()æ–¹æ³•
            - æ¸…ç†è¿‡ç¨‹ä¸­çš„å¼‚å¸¸ä¸ä¼šä¸­æ–­æ¸…ç†æµç¨‹
            - ç¡®ä¿æ¶ˆè´¹è€…å¤„äºå®Œå…¨åœæ­¢çŠ¶æ€
            - æ‰€æœ‰æ¸…ç†æ“ä½œéƒ½ä¼šè®°å½•è¯¦ç»†æ—¥å¿—
        """
        try:
            self._shutdown_thread_pools()
            self._cleanup_resources()
        except Exception as e:
            logger.error(
                f"Error during startup failure cleanup: {e}",
                extra={
                    "consumer_group": self._config.consumer_group,
                    "error": str(e),
                },
                exc_info=True,
            )

    def _shutdown_thread_pools(self) -> None:
        """
        å…³é—­çº¿ç¨‹æ± å’Œä¸“ç”¨çº¿ç¨‹
        """
        try:
            # å–æ¶ˆæ‰€æœ‰æ¶ˆè´¹ä»»åŠ¡
            for _, futures in self._consume_tasks.items():
                for future in futures:
                    if future and not future.done():
                        future.cancel()
            self._consume_tasks.clear()

            # å…³é—­çº¿ç¨‹æ± 
            if self._pull_executor:
                self._pull_executor.shutdown(wait=False)
                self._pull_executor = None

            if self._consume_executor:
                self._consume_executor.shutdown(wait=False)
                self._consume_executor = None

            # ç­‰å¾…ä¸“ç”¨çº¿ç¨‹ç»“æŸ
            self._rebalance_event.set()  # å”¤é†’é‡å¹³è¡¡çº¿ç¨‹

            # ç­‰å¾…çº¿ç¨‹ç»“æŸ
            threads_to_join: list[threading.Thread] = []
            if self._rebalance_thread and self._rebalance_thread.is_alive():
                threads_to_join.append(self._rebalance_thread)

            # å¹¶å‘ç­‰å¾…æ‰€æœ‰çº¿ç¨‹ç»“æŸ
            for thread in threads_to_join:
                try:
                    thread.join(timeout=5.0)
                    if thread.is_alive():
                        logger.warning(
                            f"Thread did not stop gracefully: {thread.name}",
                            extra={
                                "consumer_group": self._config.consumer_group,
                                "thread_name": thread.name,
                            },
                        )
                except Exception as e:
                    logger.error(
                        f"Error joining thread {thread.name}: {e}",
                        extra={
                            "consumer_group": self._config.consumer_group,
                            "thread_name": thread.name,
                            "error": str(e),
                        },
                    )

        except Exception as e:
            logger.warning(
                f"Error shutting down thread pools and threads: {e}",
                extra={
                    "consumer_group": self._config.consumer_group,
                    "error": str(e),
                },
            )

    def _cleanup_resources(self) -> None:
        """
        æ¸…ç†èµ„æº
        """

        # æ¸…ç†ProcessQueueæ¶ˆæ¯ç¼“å­˜
        for process_queue in self._msg_cache.values():
            _ = process_queue.clear()
        self._msg_cache.clear()

        # æ¸…ç†åœæ­¢äº‹ä»¶
        with self._stop_events_lock:
            self._pull_stop_events.clear()
            self._consume_stop_events.clear()

        # æ¸…ç†çŠ¶æ€
        self._pull_tasks.clear()

        # è¿œç¨‹è§£é”æ‰€æœ‰å·²åˆ†é…çš„é˜Ÿåˆ—
        with self._assigned_queues_lock:  # ğŸ”ä¿æŠ¤_assigned_queuesè®¿é—®
            assigned_queues = list(self._assigned_queues.keys())  # å¤åˆ¶ä¸€ä»½é¿å…å¹¶å‘ä¿®æ”¹
            self._assigned_queues.clear()

        for message_queue in assigned_queues:
            try:
                self._unlock_remote_queue(message_queue)
            except Exception as e:
                logger.warning(
                    f"Failed to unlock remote queue {message_queue} during cleanup: {e}",
                    extra={
                        "consumer_group": self._config.consumer_group,
                        "client_id": self._config.client_id,
                        "queue": str(message_queue),
                        "error": str(e),
                        "operation": "cleanup_resources",
                    },
                )

        # æ¸…ç†é˜Ÿåˆ—é”
        self._queue_locks.clear()

        # æ¸…ç†è¿œç¨‹é”ç¼“å­˜
        with self._remote_lock_cache_lock:
            self._remote_lock_cache.clear()

    # ==================== ç»Ÿè®¡å’Œç›‘æ§æ–¹æ³• ====================

    def _get_final_stats(self) -> dict[str, Any]:
        """
        è·å–æœ€ç»ˆç»Ÿè®¡ä¿¡æ¯

        Returns:
            dict: ç»Ÿè®¡ä¿¡æ¯å­—å…¸
        """
        uptime: float = time.time() - self._stats.get("start_time", time.time())
        messages_consumed: int = self._stats["messages_consumed"]
        pull_requests: int = self._stats["pull_requests"]
        route_refresh_count: int = self._stats.get("route_refresh_count", 0)

        # è®¡ç®—è·¯ç”±åˆ·æ–°æˆåŠŸç‡
        route_refresh_success_rate: float = 0.0
        if route_refresh_count > 0:
            route_refresh_success_rate = (
                self._stats.get("route_refresh_success_count", 0) / route_refresh_count
            )

        with self._assigned_queues_lock:  # ğŸ”ä¿æŠ¤_assigned_queuesè®¿é—®
            assigned_queues_count = len(self._assigned_queues)

        # è¿œç¨‹é”ç¼“å­˜ç»Ÿè®¡
        with self._remote_lock_cache_lock:
            remote_lock_cache_size = len(self._remote_lock_cache)
            valid_remote_locks = sum(
                1
                for _, expiry_time in self._remote_lock_cache.items()
                if time.time() < expiry_time
            )

        return {
            **self._stats,
            "uptime_seconds": uptime,
            "assigned_queues": assigned_queues_count,
            "avg_consume_duration": (
                self._stats["consume_duration_total"] / max(messages_consumed, 1)
            ),
            "pull_success_rate": (
                self._stats["pull_successes"] / max(pull_requests, 1)
            ),
            "consume_success_rate": (
                (messages_consumed - self._stats["messages_failed"])
                / max(messages_consumed, 1)
            ),
            # è¿œç¨‹é”ç›¸å…³ç»Ÿè®¡
            "remote_lock_cache_size": remote_lock_cache_size,
            "valid_remote_locks": valid_remote_locks,
            "remote_lock_expire_time": self._remote_lock_expire_time,
            # è·¯ç”±åˆ·æ–°ç›¸å…³ç»Ÿè®¡
            "route_refresh_success_rate": route_refresh_success_rate,
            "route_refresh_avg_interval": (uptime / max(route_refresh_count, 1))
            if route_refresh_count > 0
            else 0.0,
            # é‡å¹³è¡¡ç›¸å…³ç»Ÿè®¡
            "rebalance_success_rate": (
                self._stats.get("rebalance_success_count", 0)
                / max(self._stats.get("rebalance_count", 1), 1)
            ),
            "rebalance_avg_interval": (
                uptime / max(self._stats.get("rebalance_count", 1), 1)
            )
            if self._stats.get("rebalance_count", 0) > 0
            else 0.0,
            "rebalance_skipped_rate": (
                self._stats.get("rebalance_skipped_count", 0)
                / max(
                    self._stats.get("rebalance_count", 1)
                    + self._stats.get("rebalance_skipped_count", 1),
                    1,
                )
            ),
        }

    def get_stats(self) -> dict[str, Any]:
        """è·å–æ¶ˆè´¹è€…çš„å®æ—¶ç»Ÿè®¡ä¿¡æ¯ã€‚

        è¿”å›åŒ…å«æ¶ˆè´¹è€…è¿è¡ŒçŠ¶æ€ã€æ€§èƒ½æŒ‡æ ‡å’Œèµ„æºä½¿ç”¨æƒ…å†µçš„è¯¦ç»†ç»Ÿè®¡æ•°æ®ã€‚

        Returns:
            dict[str, Any]: ç»Ÿè®¡ä¿¡æ¯å­—å…¸ï¼ŒåŒ…å«ä»¥ä¸‹å­—æ®µï¼š
                - messages_consumed (int): å·²æ¶ˆè´¹çš„æ¶ˆæ¯æ€»æ•°
                - messages_failed (int): æ¶ˆè´¹å¤±è´¥çš„æ¶ˆæ¯æ€»æ•°
                - pull_requests (int): æ‹‰å–è¯·æ±‚æ€»æ•°
                - pull_successes (int): æ‹‰å–æˆåŠŸçš„æ¬¡æ•°
                - pull_failures (int): æ‹‰å–å¤±è´¥çš„æ¬¡æ•°
                - consume_duration_total (float): æ€»æ¶ˆè´¹è€—æ—¶ï¼ˆç§’ï¼‰
                - start_time (float): å¯åŠ¨æ—¶é—´æˆ³
                - heartbeat_success_count (int): å¿ƒè·³æˆåŠŸæ¬¡æ•°
                - heartbeat_failure_count (int): å¿ƒè·³å¤±è´¥æ¬¡æ•°
                - uptime_seconds (float): è¿è¡Œæ—¶é•¿ï¼ˆç§’ï¼‰
                - assigned_queues (int): å½“å‰åˆ†é…çš„é˜Ÿåˆ—æ•°é‡
                - avg_consume_duration (float): å¹³å‡æ¶ˆè´¹è€—æ—¶ï¼ˆç§’ï¼‰
                - pull_success_rate (float): æ‹‰å–æˆåŠŸç‡ï¼ˆ0-1ä¹‹é—´ï¼‰
                - consume_success_rate (float): æ¶ˆè´¹æˆåŠŸç‡ï¼ˆ0-1ä¹‹é—´ï¼‰

        Raises:
            None: æ­¤æ–¹æ³•ä¸ä¼šæŠ›å‡ºå¼‚å¸¸

        Note:
            - ç»Ÿè®¡æ•°æ®åœ¨æ¶ˆè´¹è€…ç”Ÿå‘½å‘¨æœŸå†…æŒç»­ç´¯ç§¯
            - å¹³å‡å€¼åŸºäºå®é™…æ¶ˆè´¹æ¬¡æ•°è®¡ç®—
            - æˆåŠŸç‡åŸºäºå®é™…æ“ä½œæ¬¡æ•°è®¡ç®—
            - å¯ç”¨äºç›‘æ§æ¶ˆè´¹è€…å¥åº·çŠ¶æ€å’Œæ€§èƒ½è°ƒä¼˜

        Example:
            >>> stats = consumer.get_stats()
            >>> print(f"æ¶ˆè´¹æˆåŠŸç‡: {stats['consume_success_rate']:.2%}")
            >>> print(f"å¹³å‡æ¶ˆè´¹è€—æ—¶: {stats['avg_consume_duration']:.3f}ç§’")
        """
        return self._get_final_stats()

    # ==================== å…¶ä»–æ–¹æ³• ====================
    def _prepare_consumer_remote(self, pool: ConnectionPool) -> None:
        pool.register_request_processor(
            RequestCode.NOTIFY_CONSUMER_IDS_CHANGED,
            self._on_notify_consumer_ids_changed,
        )
        pool.register_request_processor(
            RequestCode.CONSUME_MESSAGE_DIRECTLY,
            self._on_notify_consume_message_directly,
        )

    def _on_notify_consume_message_directly(
        self, command: RemotingCommand, _addr: tuple[str, int]
    ) -> RemotingCommand:
        header: ConsumeMessageDirectlyHeader = ConsumeMessageDirectlyHeader.decode(
            command.ext_fields
        )
        if header.client_id == self._config.client_id:
            return self._on_notify_consume_message_directly_internal(header, command)
        else:
            return (
                RemotingCommandBuilder(ResponseCode.ERROR)
                .with_remark(f"Can't find client ID {header.client_id}")
                .build()
            )

    def _on_notify_consume_message_directly_internal(
        self, header: ConsumeMessageDirectlyHeader, command: RemotingCommand
    ) -> RemotingCommand:
        if not command.body:
            return (
                RemotingCommandBuilder(ResponseCode.ERROR)
                .with_remark("No message body")
                .build()
            )

        msgs = MessageExt.decode_messages(command.body)
        if len(msgs) == 0:
            return (
                RemotingCommandBuilder(ResponseCode.ERROR)
                .with_remark("No message")
                .build()
            )

        msg: MessageExt = msgs[0]

        q: MessageQueue
        if msg.queue:
            q = MessageQueue(msg.topic, header.broker_name, msg.queue.queue_id)
        else:
            q = MessageQueue(msg.topic, header.broker_name, 0)

        now = datetime.now()

        for msg in msgs:
            self._reset_retry(msg)

        if self._concurrent_consume_message(msgs, q):
            res: ConsumeMessageDirectlyResult = ConsumeMessageDirectlyResult(
                order=False,
                auto_commit=True,
                consume_result=ConsumeResult.SUCCESS,
                remark="Message consumed",
                spent_time_mills=int((datetime.now() - now).total_seconds() * 1000),
            )
            return (
                RemotingCommandBuilder(ResponseCode.SUCCESS)
                .with_remark("Message consumed")
                .with_body(res.encode())
                .build()
            )
        else:
            return (
                RemotingCommandBuilder(ResponseCode.ERROR)
                .with_remark("Failed to consume message")
                .build()
            )
