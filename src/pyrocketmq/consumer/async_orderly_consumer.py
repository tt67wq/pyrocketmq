# Licensed to the Apache Software Foundation (ASF) under one or more
# contributor license agreements.  See the NOTICE file distributed with
# this work for additional information regarding copyright ownership.
# The ASF licenses this file to You under the Apache License, Version 2.0
# (the "License"); you may not use this file except in compliance with
# the License.  You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

"""
å¼‚æ­¥é¡ºåºæ¶ˆè´¹è€…å®ç°

åŸºäºAsyncBaseConsumerå®ç°çš„å¼‚æ­¥é¡ºåºæ¶ˆè´¹è€…ï¼Œä¿è¯åŒä¸€æ¶ˆæ¯é˜Ÿåˆ—ä¸­çš„æ¶ˆæ¯
æŒ‰ç…§åç§»é‡é¡ºåºè¿›è¡Œæ¶ˆè´¹ã€‚
"""

import asyncio
import time
from typing import Any

from pyrocketmq.broker import AsyncBrokerClient, MessagePullError
from pyrocketmq.consumer.allocate_queue_strategy import AllocateContext
from pyrocketmq.consumer.async_base_consumer import AsyncBaseConsumer
from pyrocketmq.consumer.config import ConsumerConfig
from pyrocketmq.consumer.errors import MessageConsumeError
from pyrocketmq.consumer.offset_store import ReadOffsetType
from pyrocketmq.consumer.process_queue import ProcessQueue
from pyrocketmq.logging import get_logger
from pyrocketmq.model import (
    MessageExt,
    MessageModel,
    MessageQueue,
    SubscriptionData,
    SubscriptionEntry,
)
from pyrocketmq.remote import AsyncConnectionPool


class AsyncOrderlyConsumer(AsyncBaseConsumer):
    """
    å¼‚æ­¥é¡ºåºæ¶ˆè´¹è€…

    ç»§æ‰¿AsyncBaseConsumerï¼Œä¸“æ³¨äºé¡ºåºæ¶ˆè´¹é€»è¾‘ã€‚ä¿è¯åŒä¸€æ¶ˆæ¯é˜Ÿåˆ—ä¸­çš„æ¶ˆæ¯
    æŒ‰ç…§åç§»é‡ä¸¥æ ¼é¡ºåºå¤„ç†ï¼Œæ¯ä¸ªé˜Ÿåˆ—åŒæ—¶åªèƒ½æœ‰ä¸€ä¸ªæ¶ˆè´¹ä»»åŠ¡ã€‚
    """

    def __init__(self, config: ConsumerConfig):
        """
        åˆå§‹åŒ–å¼‚æ­¥é¡ºåºæ¶ˆè´¹è€…

        Args:
            config: æ¶ˆè´¹è€…é…ç½®
        """

        # è°ƒç”¨çˆ¶ç±»åˆå§‹åŒ–
        super().__init__(config)

        self.logger = get_logger(__name__)
        # é¡ºåºæ¶ˆè´¹ç‰¹æœ‰å­—æ®µ
        self._queue_locks: dict[
            MessageQueue, asyncio.Semaphore
        ] = {}  # é˜Ÿåˆ—çº§é”ä¿¡å·é‡ï¼Œç¡®ä¿é¡ºåºæ¶ˆè´¹
        self._consume_tasks: dict[str, asyncio.Task[None]] = {}  # é˜Ÿåˆ—æ¶ˆè´¹ä»»åŠ¡
        self._pull_tasks: dict[str, asyncio.Task[None]] = {}  # é˜Ÿåˆ—æ‹‰å–ä»»åŠ¡

        self._msg_cache: dict[MessageQueue, ProcessQueue] = {}
        self._cache_lock = asyncio.Lock()  # ç”¨äºä¿æŠ¤_msg_cacheå­—å…¸

        # çŠ¶æ€ç®¡ç†
        self._assigned_queues: dict[MessageQueue, int] = {}  # queue -> last_offset
        self._assigned_queues_lock = (
            asyncio.Lock()
        )  # ğŸ”ä¿æŠ¤_assigned_queueså­—å…¸çš„å¹¶å‘è®¿é—®
        self._last_rebalance_time: float = 0.0

        # é‡å¹³è¡¡ä»»åŠ¡ç®¡ç†
        self._rebalance_task: asyncio.Task[None] | None = None
        self._rebalance_interval: float = 20.0  # é‡å¹³è¡¡é—´éš”(ç§’)

        # çº¿ç¨‹åŒæ­¥äº‹ä»¶
        self._rebalance_event: asyncio.Event = asyncio.Event()  # ç”¨äºé‡å¹³è¡¡å¾ªç¯çš„äº‹ä»¶

        # çº¿ç¨‹åœæ­¢äº‹ä»¶ - ç”¨äºä¼˜é›…å…³é—­æ‹‰å–å’Œæ¶ˆè´¹å¾ªç¯
        self._pull_stop_events: dict[str, asyncio.Event] = {}
        self._consume_stop_events: dict[str, asyncio.Event] = {}
        self._stop_events_lock = asyncio.Lock()  # ä¿æŠ¤åœæ­¢äº‹ä»¶å­—å…¸

        # é‡å¹³è¡¡é‡å…¥ä¿æŠ¤
        self._rebalance_lock = asyncio.Lock()  # é‡å¹³è¡¡é”ï¼Œé˜²æ­¢é‡å…¥

        # è¿œç¨‹é”ç¼“å­˜å’Œæœ‰æ•ˆæœŸç®¡ç†
        # é¿å…æ¯æ¬¡æ¶ˆè´¹å¾ªç¯éƒ½éœ€è¦è·å–è¿œç¨‹é”ï¼Œæå‡æ€§èƒ½
        self._remote_lock_cache: dict[
            MessageQueue, float
        ] = {}  # queue -> lock_expiry_time
        self._remote_lock_cache_lock = asyncio.Lock()  # ä¿æŠ¤è¿œç¨‹é”ç¼“å­˜
        self._remote_lock_expire_time: float = 30.0  # è¿œç¨‹é”æœ‰æ•ˆæœŸ30ç§’

        # ç»Ÿè®¡ä¿¡æ¯æ‰©å±•
        self._stats.update(
            {
                "queue_lock_wait_count": 0,
                "queue_lock_wait_total_time": 0.0,
                "orderly_consume_success_count": 0,
                "orderly_consume_fail_count": 0,
                "orderly_consume_rt_total": 0.0,
                "orderly_consume_rt_count": 0,
            }
        )

        self.logger.info(
            "AsyncOrderlyConsumer initialized",
            extra={
                "consumer_group": self._config.consumer_group,
                "message_model": self._config.message_model,
                "consume_thread_max": self._config.consume_thread_max,
                "pull_batch_size": self._config.pull_batch_size,
                "remote_lock_expire_time": self._remote_lock_expire_time,
            },
        )

    async def start(self) -> None:
        """å¼‚æ­¥å¯åŠ¨é¡ºåºæ¶ˆè´¹è€…ã€‚

        åˆå§‹åŒ–å¹¶å¯åŠ¨æ¶ˆè´¹è€…çš„æ‰€æœ‰ç»„ä»¶ï¼ŒåŒ…æ‹¬ï¼š
        - å»ºç«‹ä¸NameServerå’ŒBrokerçš„ç½‘ç»œè¿æ¥
        - åˆ›å»ºå¼‚æ­¥ä»»åŠ¡ç®¡ç†å™¨
        - æ‰§è¡Œåˆå§‹é˜Ÿåˆ—åˆ†é…å’Œé‡å¹³è¡¡
        - å¯åŠ¨å¿ƒè·³å’Œé‡å¹³è¡¡åå°ä»»åŠ¡

        å¯åŠ¨å¤±è´¥æ—¶ä¼šè‡ªåŠ¨æ¸…ç†å·²åˆ†é…çš„èµ„æºã€‚

        Raises:
            ConsumerStartError: å½“ä»¥ä¸‹æƒ…å†µå‘ç”Ÿæ—¶æŠ›å‡ºï¼š
                - æœªæ³¨å†Œæ¶ˆæ¯ç›‘å¬å™¨
                - æ¶ˆæ¯ç›‘å¬å™¨ç±»å‹ä¸åŒ¹é…ï¼ˆéœ€è¦AsyncMessageListenerï¼‰
                - ç½‘ç»œè¿æ¥å¤±è´¥
                - å¼‚æ­¥ä»»åŠ¡åˆ›å»ºå¤±è´¥
                - å…¶ä»–åˆå§‹åŒ–é”™è¯¯

        Note:
            æ­¤æ–¹æ³•æ˜¯çº¿ç¨‹å®‰å…¨çš„ï¼Œå¤šæ¬¡è°ƒç”¨åªä¼šå¯åŠ¨ä¸€æ¬¡ã€‚
            å¯åŠ¨æˆåŠŸåï¼Œæ¶ˆè´¹è€…ä¼šè‡ªåŠ¨å¼€å§‹æ‹‰å–å’Œå¤„ç†æ¶ˆæ¯ã€‚
        """
        async with self._lock:
            if self._is_running:
                self.logger.warning("AsyncOrderlyConsumer is already running")
                return

            try:
                self.logger.info(
                    "Starting AsyncOrderlyConsumer",
                    extra={
                        "consumer_group": self._config.consumer_group,
                        "namesrv_addr": self._config.namesrv_addr,
                    },
                )

                # å¯åŠ¨AsyncBaseConsumer
                await super().start()

                # æ‰§è¡Œåˆå§‹é‡å¹³è¡¡
                await self._do_rebalance()

                # å¯åŠ¨é‡å¹³è¡¡ä»»åŠ¡
                await self._start_rebalance_task()

                self._stats["start_time"] = time.time()

                async with self._assigned_queues_lock:
                    assigned_queues_count = len(self._assigned_queues)

                self.logger.info(
                    "AsyncOrderlyConsumer started successfully",
                    extra={
                        "consumer_group": self._config.consumer_group,
                        "assigned_queues": assigned_queues_count,
                        "rebalance_interval": self._rebalance_interval,
                        "remote_lock_expire_time": self._remote_lock_expire_time,
                    },
                )

            except Exception as e:
                self.logger.error(
                    f"Failed to start AsyncOrderlyConsumer: {e}",
                    extra={
                        "consumer_group": self._config.consumer_group,
                        "error": str(e),
                    },
                    exc_info=True,
                )
                await self._cleanup_on_start_failure()
                from .errors import ConsumerStartError

                raise ConsumerStartError(
                    "Failed to start AsyncOrderlyConsumer",
                    cause=e,
                    context={"consumer_group": self._config.consumer_group},
                ) from e

    async def shutdown(self) -> None:
        """å¼‚æ­¥ä¼˜é›…åœæ­¢é¡ºåºæ¶ˆè´¹è€…ã€‚

        æ‰§è¡Œä»¥ä¸‹å…³é—­æµç¨‹ï¼š
        1. åœæ­¢æ¥å—æ–°çš„æ¶ˆæ¯æ‹‰å–è¯·æ±‚
        2. ç­‰å¾…æ­£åœ¨å¤„ç†çš„æ¶ˆæ¯å®Œæˆï¼ˆæœ€å¤šç­‰å¾…30ç§’ï¼‰
        3. æŒä¹…åŒ–æ‰€æœ‰é˜Ÿåˆ—çš„æ¶ˆè´¹åç§»é‡
        4. å…³é—­æ‰€æœ‰å¼‚æ­¥ä»»åŠ¡å’Œåå°ä»»åŠ¡
        5. æ¸…ç†ç½‘ç»œè¿æ¥å’Œèµ„æº

        Args:
            None

        Returns:
            None

        Raises:
            ConsumerShutdownError: å½“ä»¥ä¸‹æƒ…å†µå‘ç”Ÿæ—¶æŠ›å‡ºï¼š
                - åç§»é‡æŒä¹…åŒ–å¤±è´¥
                - å¼‚æ­¥ä»»åŠ¡å…³é—­è¶…æ—¶
                - ç½‘ç»œè¿æ¥æ¸…ç†å¤±è´¥
                - å…¶ä»–æ¸…ç†è¿‡ç¨‹ä¸­çš„é”™è¯¯

        Note:
            - æ­¤æ–¹æ³•æ˜¯çº¿ç¨‹å®‰å…¨çš„ï¼Œå¯ä»¥å¤šæ¬¡è°ƒç”¨
            - ä¼šå°½åŠ›ç­‰å¾…æ­£åœ¨å¤„ç†çš„æ¶ˆæ¯å®Œæˆï¼Œä½†ä¸ä¼šæ— é™æœŸç­‰å¾…
            - å³ä½¿å…³é—­è¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯ï¼Œä¹Ÿä¼šç»§ç»­æ‰§è¡Œåç»­çš„æ¸…ç†æ­¥éª¤
            - å…³é—­åçš„æ¶ˆè´¹è€…ä¸èƒ½é‡æ–°å¯åŠ¨ï¼Œéœ€è¦åˆ›å»ºæ–°å®ä¾‹
        """
        async with self._lock:
            if not self._is_running:
                self.logger.warning("AsyncOrderlyConsumer is not running")
                return

            try:
                self.logger.info(
                    "Shutting down AsyncOrderlyConsumer",
                    extra={"consumer_group": self._config.consumer_group},
                )

                self._is_running = False

                # å…ˆè®¾ç½®Eventä»¥å”¤é†’å¯èƒ½é˜»å¡çš„å¼‚æ­¥ä»»åŠ¡
                self._rebalance_event.set()

                # åœæ­¢æ‹‰å–ä»»åŠ¡
                await self._stop_pull_tasks()

                # åœæ­¢æ¶ˆè´¹ä»»åŠ¡
                await self._stop_consume_tasks()

                # ç­‰å¾…å¤„ç†ä¸­çš„æ¶ˆæ¯å®Œæˆ
                await self._wait_for_processing_completion()

                # æŒä¹…åŒ–åç§»é‡
                try:
                    await self._offset_store.persist_all()
                except Exception as e:
                    self.logger.error(
                        f"Failed to persist offsets during shutdown: {e}",
                        extra={
                            "consumer_group": self._config.consumer_group,
                            "error": str(e),
                        },
                        exc_info=True,
                    )

                # åœæ­¢å¼‚æ­¥ä»»åŠ¡
                await self._shutdown_async_tasks()

                # æ¸…ç†èµ„æº
                await self._cleanup_resources()

                # è°ƒç”¨çˆ¶ç±»å…³é—­
                await super().shutdown()

                self.logger.info(
                    "AsyncOrderlyConsumer shutdown completed",
                    extra={
                        "consumer_group": self._config.consumer_group,
                        "final_stats": await self._get_final_stats(),
                    },
                )

            except Exception as e:
                self.logger.error(
                    f"Error during AsyncOrderlyConsumer shutdown: {e}",
                    extra={
                        "consumer_group": self._config.consumer_group,
                        "error": str(e),
                    },
                    exc_info=True,
                )
                from .errors import ConsumerShutdownError

                raise ConsumerShutdownError(
                    "Error during async consumer shutdown",
                    cause=e,
                    context={"consumer_group": self._config.consumer_group},
                ) from e

    async def _pre_rebalance_check(self) -> bool:
        """æ‰§è¡Œé‡å¹³è¡¡å‰ç½®æ£€æŸ¥ã€‚

        æ£€æŸ¥æ˜¯å¦å¯ä»¥æ‰§è¡Œé‡å¹³è¡¡æ“ä½œï¼ŒåŒ…æ‹¬é”è·å–å’Œè®¢é˜…çŠ¶æ€æ£€æŸ¥ã€‚

        Returns:
            bool: å¦‚æœå¯ä»¥æ‰§è¡Œé‡å¹³è¡¡è¿”å›Trueï¼Œå¦åˆ™è¿”å›False

        Raises:
            None: æ­¤æ–¹æ³•ä¸ä¼šæŠ›å‡ºå¼‚å¸¸
        """
        # å¤šä¸ªåœ°æ–¹éƒ½ä¼šè§¦å‘é‡å¹³è¡¡ï¼ŒåŠ å…¥ä¸€ä¸ªæ”¾ç½®é‡å…¥æœºåˆ¶ï¼Œå¦‚æœæ­£åœ¨æ‰§è¡Œrebalanceï¼Œå†æ¬¡è§¦å‘æ— æ•ˆ
        # ä½¿ç”¨å¯é‡å…¥é”ä¿æŠ¤é‡å¹³è¡¡æ“ä½œ
        if not self._rebalance_lock.acquire():
            # å¦‚æœæ— æ³•è·å–é”ï¼Œè¯´æ˜æ­£åœ¨æ‰§è¡Œé‡å¹³è¡¡ï¼Œè·³è¿‡æœ¬æ¬¡è¯·æ±‚
            self._stats["rebalance_skipped_count"] += 1
            self.logger.debug(
                "Rebalance already in progress, skipping",
                extra={
                    "consumer_group": self._config.consumer_group,
                    "skipped_count": self._stats["rebalance_skipped_count"],
                },
            )
            return False

        # æ£€æŸ¥æ˜¯å¦æœ‰è®¢é˜…çš„Topic
        topics: set[str] = self._subscription_manager.get_topics()
        if not topics:
            self.logger.debug("No topics subscribed, skipping rebalance")
            self._rebalance_lock.release()
            return False

        return True

    async def _collect_and_allocate_queues(self) -> list[MessageQueue]:
        """æ”¶é›†æ‰€æœ‰Topicçš„å¯ç”¨é˜Ÿåˆ—å¹¶æ‰§è¡Œåˆ†é…ã€‚

        éå†æ‰€æœ‰è®¢é˜…çš„Topicï¼Œè·å–æ¯ä¸ªTopicçš„å¯ç”¨é˜Ÿåˆ—ï¼Œ
        å¹¶ä¸ºæ¯ä¸ªTopicæ‰§è¡Œé˜Ÿåˆ—åˆ†é…ç®—æ³•ã€‚

        Returns:
            list[MessageQueue]: åˆ†é…ç»™å½“å‰æ¶ˆè´¹è€…çš„æ‰€æœ‰é˜Ÿåˆ—åˆ—è¡¨

        Raises:
            Exception: è·¯ç”±ä¿¡æ¯æ›´æ–°æˆ–é˜Ÿåˆ—åˆ†é…å¤±è´¥æ—¶æŠ›å‡ºå¼‚å¸¸
        """
        allocated_queues: list[MessageQueue] = []
        topics = self._subscription_manager.get_topics()

        for topic in topics:
            try:
                # å¼‚æ­¥æ›´æ–°Topicè·¯ç”±ä¿¡æ¯
                _ = await self._update_route_info(topic)

                # è·å–Topicçš„æ‰€æœ‰å¯ç”¨é˜Ÿåˆ—
                all_queues: list[MessageQueue] = [
                    x
                    for (x, _) in self._topic_broker_mapping.get_subscribe_queues(topic)
                ]

                if not all_queues:
                    self.logger.debug(
                        "No queues available for subscribed topic",
                        extra={"topic": topic},
                    )
                    continue

                # å¼‚æ­¥æ‰§è¡Œé˜Ÿåˆ—åˆ†é…
                topic_allocated_queues = await self._allocate_queues(topic, all_queues)
                allocated_queues.extend(topic_allocated_queues)

                self.logger.debug(
                    "Topic queue allocation completed",
                    extra={
                        "topic": topic,
                        "total_queues": len(all_queues),
                        "allocated_queues": len(topic_allocated_queues),
                    },
                )

            except Exception as e:
                self.logger.warning(
                    f"Failed to allocate queues for topic {topic}: {e}",
                    extra={"topic": topic, "error": str(e)},
                )
                # ç»§ç»­å¤„ç†å…¶ä»–Topicï¼Œä¸ä¸­æ–­æ•´ä¸ªé‡å¹³è¡¡è¿‡ç¨‹
                continue

        return allocated_queues

    async def _allocate_queues(
        self, topic: str, all_queues: list[MessageQueue]
    ) -> list[MessageQueue]:
        """
        ä¸ºå½“å‰æ¶ˆè´¹è€…åˆ†é…é˜Ÿåˆ—

        æ ¹æ®æ¶ˆæ¯æ¨¡å¼å’Œåˆ†é…ç­–ç•¥ï¼Œä»æ‰€æœ‰å¯ç”¨é˜Ÿåˆ—ä¸­é€‰æ‹©ä¸€éƒ¨åˆ†åˆ†é…ç»™å½“å‰æ¶ˆè´¹è€…å®ä¾‹ã€‚
        è¿™æ˜¯RocketMQæ¶ˆè´¹è€…è´Ÿè½½å‡è¡¡çš„æ ¸å¿ƒæœºåˆ¶ï¼Œç¡®ä¿å¤šä¸ªæ¶ˆè´¹è€…èƒ½å¤Ÿåˆç†åœ°æ¶ˆè´¹åŒä¸€ä¸ªTopicä¸‹çš„æ¶ˆæ¯ã€‚

        Args:
            topic: è¦åˆ†é…é˜Ÿåˆ—çš„Topicåç§°
            all_queues: è¯¥Topicä¸‹æ‰€æœ‰å¯ç”¨çš„æ¶ˆæ¯é˜Ÿåˆ—åˆ—è¡¨

        Returns:
            list[MessageQueue]: åˆ†é…ç»™å½“å‰æ¶ˆè´¹è€…çš„é˜Ÿåˆ—åˆ—è¡¨
        """
        if self._config.message_model == MessageModel.CLUSTERING:
            cids = await self._find_consumer_list(topic)
            if not cids:
                return []

            return self._allocate_strategy.allocate(
                AllocateContext(
                    self._config.consumer_group,
                    self._config.client_id,
                    cids,
                    all_queues,
                    {},
                )
            )
        else:
            return all_queues.copy()

    async def _finalize_rebalance(self, total_topics: int, total_queues: int) -> None:
        """å®Œæˆé‡å¹³è¡¡åå¤„ç†ã€‚

        æ›´æ–°é‡å¹³è¡¡æ—¶é—´æˆ³ã€ç»Ÿè®¡ä¿¡æ¯ï¼Œå¹¶è®°å½•å®Œæˆæ—¥å¿—ã€‚

        Args:
            total_topics: é‡å¹³è¡¡å¤„ç†çš„Topicæ€»æ•°
            total_queues: åˆ†é…åˆ°çš„é˜Ÿåˆ—æ€»æ•°

        Raises:
            None: æ­¤æ–¹æ³•ä¸ä¼šæŠ›å‡ºå¼‚å¸¸
        """
        self._last_rebalance_time = time.time()

        # æ›´æ–°æˆåŠŸç»Ÿè®¡
        self._stats["rebalance_success_count"] = (
            self._stats.get("rebalance_success_count", 0) + 1
        )

        self.logger.info(
            "Rebalance completed",
            extra={
                "consumer_group": self._config.consumer_group,
                "total_topics": total_topics,
                "assigned_queues": total_queues,
                "success_count": self._stats["rebalance_success_count"],
            },
        )

    async def _find_consumer_list(self, topic: str) -> list[str]:
        """
        æŸ¥æ‰¾æ¶ˆè´¹è€…åˆ—è¡¨

        Args:
            topic: ä¸»é¢˜åç§°

        Returns:
            æ¶ˆè´¹è€…åˆ—è¡¨
        """
        addresses: list[str] = await self._nameserver_manager.get_all_broker_addresses(
            topic
        )
        if not addresses:
            self.logger.warning(
                "No broker addresses found for topic", extra={"topic": topic}
            )
            return []

        pool: AsyncConnectionPool = await self._broker_manager.must_connection_pool(
            addresses[0]
        )
        async with pool.get_connection(usage="æŸ¥æ‰¾æ¶ˆè´¹è€…åˆ—è¡¨") as conn:
            return await AsyncBrokerClient(conn).get_consumers_by_group(
                self._config.consumer_group
            )

    async def _get_queue_lock(self, message_queue: MessageQueue) -> asyncio.Semaphore:
        """è·å–æŒ‡å®šæ¶ˆæ¯é˜Ÿåˆ—çš„é”ä¿¡å·é‡

        ä½¿ç”¨åŒé‡æ£€æŸ¥é”å®šæ¨¡å¼æ¥é¿å…ç«äº‰æ¡ä»¶

        Args:
            message_queue: æ¶ˆæ¯é˜Ÿåˆ—

        Returns:
            asyncio.Semaphore: è¯¥é˜Ÿåˆ—çš„å¼‚æ­¥é”ä¿¡å·é‡å¯¹è±¡ï¼ˆå€¼ä¸º1çš„ä¿¡å·é‡ï¼‰
        """
        # é¦–å…ˆè¿›è¡Œæ— é”æ£€æŸ¥ï¼Œæé«˜æ€§èƒ½
        if message_queue in self._queue_locks:
            return self._queue_locks[message_queue]

        # ç”±äºå­—å…¸æ“ä½œæœ¬èº«æ˜¯åŸå­çš„ï¼Œä¸”æˆ‘ä»¬åœ¨å•çº¿ç¨‹äº‹ä»¶å¾ªç¯ä¸­è¿è¡Œï¼Œ
        # ä¸éœ€è¦é¢å¤–çš„é”ä¿æŠ¤
        if message_queue not in self._queue_locks:
            self._queue_locks[message_queue] = asyncio.Semaphore(1)

        return self._queue_locks[message_queue]

    async def _is_locked(self, message_queue: MessageQueue) -> bool:
        """æ£€æŸ¥æŒ‡å®šé˜Ÿåˆ—æ˜¯å¦å·²é”å®š

        Args:
            message_queue: æ¶ˆæ¯é˜Ÿåˆ—

        Returns:
            bool: Trueå¦‚æœé˜Ÿåˆ—å·²é”å®šï¼ŒFalseå¦‚æœé˜Ÿåˆ—æœªé”å®š
        """
        if message_queue not in self._queue_locks:
            return False

        # asyncio.Semaphoreæœ‰locked()æ–¹æ³•ï¼Œå¯ä»¥ç›´æ¥æ£€æŸ¥çŠ¶æ€
        return self._queue_locks[message_queue].locked()

    async def _is_remote_lock_valid(self, message_queue: MessageQueue) -> bool:
        """æ£€æŸ¥æŒ‡å®šé˜Ÿåˆ—çš„è¿œç¨‹é”æ˜¯å¦ä»ç„¶æœ‰æ•ˆ

        Args:
            message_queue: æ¶ˆæ¯é˜Ÿåˆ—

        Returns:
            Trueå¦‚æœè¿œç¨‹é”ä»ç„¶æœ‰æ•ˆï¼ŒFalseå¦‚æœå·²è¿‡æœŸæˆ–ä¸å­˜åœ¨
        """
        async with self._remote_lock_cache_lock:
            expiry_time = self._remote_lock_cache.get(message_queue)
            if expiry_time is None:
                return False

            current_time = time.time()
            return current_time < expiry_time

    async def _set_remote_lock_expiry(self, message_queue: MessageQueue) -> None:
        """è®¾ç½®æŒ‡å®šé˜Ÿåˆ—çš„è¿œç¨‹é”è¿‡æœŸæ—¶é—´

        Args:
            message_queue: æ¶ˆæ¯é˜Ÿåˆ—
        """
        async with self._remote_lock_cache_lock:
            expiry_time = time.time() + self._remote_lock_expire_time
            self._remote_lock_cache[message_queue] = expiry_time

    async def _invalidate_remote_lock(self, message_queue: MessageQueue) -> None:
        """ä½¿æŒ‡å®šé˜Ÿåˆ—çš„è¿œç¨‹é”å¤±æ•ˆ

        Args:
            message_queue: æ¶ˆæ¯é˜Ÿåˆ—
        """
        async with self._remote_lock_cache_lock:
            self._remote_lock_cache.pop(message_queue, None)

    async def _lock_remote_queue(self, message_queue: MessageQueue) -> bool:
        """å°è¯•è¿œç¨‹é”å®šæŒ‡å®šé˜Ÿåˆ—

        Args:
            message_queue: æ¶ˆæ¯é˜Ÿåˆ—

        Returns:
            Trueå¦‚æœé”å®šæˆåŠŸï¼ŒFalseå¦‚æœé”å®šå¤±è´¥
        """
        try:
            # è·å–é˜Ÿåˆ—å¯¹åº”çš„Brokeråœ°å€
            broker_address: (
                str | None
            ) = await self._nameserver_manager.get_broker_address(
                message_queue.broker_name
            )
            if not broker_address:
                self.logger.warning(
                    f"Broker address not found for queue: {message_queue}"
                )
                return False

            connection_pool = await self._broker_manager.must_connection_pool(
                broker_address
            )

            # åˆ›å»ºå¼‚æ­¥brokerå®¢æˆ·ç«¯
            async with connection_pool.get_connection() as conn:
                broker_client = AsyncBrokerClient(conn)

                # å°è¯•é”å®šé˜Ÿåˆ—
                locked_queues = await broker_client.lock_batch_mq(
                    consumer_group=self._config.consumer_group,
                    client_id=self._config.client_id,
                    mqs=[message_queue],
                )

                # æ£€æŸ¥é”å®šæ˜¯å¦æˆåŠŸ
                if locked_queues and len(locked_queues) > 0:
                    # æˆåŠŸè·å–è¿œç¨‹é”ï¼Œè®¾ç½®è¿‡æœŸæ—¶é—´
                    await self._set_remote_lock_expiry(message_queue)
                    self.logger.debug(
                        f"Successfully locked remote queue: {message_queue}",
                        extra={
                            "consumer_group": self._config.consumer_group,
                            "client_id": self._config.client_id,
                            "queue": str(message_queue),
                            "operation": "lock_remote_queue",
                            "expire_seconds": self._remote_lock_expire_time,
                        },
                    )
                    return True
                else:
                    self.logger.warning(
                        f"Failed to lock remote queue: {message_queue}",
                        extra={
                            "consumer_group": self._config.consumer_group,
                            "client_id": self._config.client_id,
                            "queue": str(message_queue),
                            "operation": "lock_remote_queue",
                        },
                    )
                    return False

        except Exception as e:
            self.logger.error(
                f"Exception occurred while locking remote queue {message_queue}: {e}",
                extra={
                    "consumer_group": self._config.consumer_group,
                    "client_id": self._config.client_id,
                    "queue": str(message_queue),
                    "error": str(e),
                    "operation": "lock_remote_queue",
                },
                exc_info=True,
            )
            return False

    async def _unlock_remote_queue(self, message_queue: MessageQueue) -> bool:
        """å°è¯•è¿œç¨‹è§£é”æŒ‡å®šé˜Ÿåˆ—

        Args:
            message_queue: æ¶ˆæ¯é˜Ÿåˆ—

        Returns:
            Trueå¦‚æœè§£é”æˆåŠŸï¼ŒFalseå¦‚æœè§£é”å¤±è´¥
        """
        try:
            # è·å–é˜Ÿåˆ—å¯¹åº”çš„Brokeråœ°å€
            broker_address: (
                str | None
            ) = await self._nameserver_manager.get_broker_address(
                message_queue.broker_name
            )
            if not broker_address:
                self.logger.warning(
                    f"Broker address not found for queue: {message_queue}"
                )
                return False

            connection_pool = await self._broker_manager.must_connection_pool(
                broker_address
            )

            async with connection_pool.get_connection() as conn:
                broker_client = AsyncBrokerClient(conn)

                # å°è¯•è§£é”é˜Ÿåˆ—
                await broker_client.unlock_batch_mq(
                    consumer_group=self._config.consumer_group,
                    client_id=self._config.client_id,
                    mqs=[message_queue],
                )

                # æ¸…é™¤è¿œç¨‹é”ç¼“å­˜
                await self._invalidate_remote_lock(message_queue)

                self.logger.debug(
                    f"Successfully unlocked remote queue: {message_queue}",
                    extra={
                        "consumer_group": self._config.consumer_group,
                        "client_id": self._config.client_id,
                        "queue": str(message_queue),
                        "operation": "unlock_remote_queue",
                    },
                )
                return True

        except Exception as e:
            self.logger.error(
                f"Exception occurred while unlocking remote queue {message_queue}: {e}",
                extra={
                    "consumer_group": self._config.consumer_group,
                    "client_id": self._config.client_id,
                    "queue": str(message_queue),
                    "error": str(e),
                    "operation": "unlock_remote_queue",
                },
                exc_info=True,
            )
            return False

    async def _start_pull_tasks_for_queues(self, queues: set[MessageQueue]) -> None:
        """ä¸ºæŒ‡å®šé˜Ÿåˆ—å¯åŠ¨æ‹‰å–ä»»åŠ¡

        Args:
            queues: è¦å¯åŠ¨æ‹‰å–ä»»åŠ¡çš„é˜Ÿåˆ—é›†åˆ
        """
        for queue in queues:
            if queue not in self._pull_tasks:
                # ä¸ºæ¯ä¸ªé˜Ÿåˆ—åˆ›å»ºåœæ­¢äº‹ä»¶
                async with self._stop_events_lock:
                    pull_stop_event = asyncio.Event()
                    consume_stop_event = asyncio.Event()
                    self._pull_stop_events[str(queue)] = pull_stop_event
                    self._consume_stop_events[str(queue)] = consume_stop_event

                # å¯åŠ¨æ‹‰å–ä»»åŠ¡ï¼Œä¼ å…¥åœæ­¢äº‹ä»¶
                task = asyncio.create_task(
                    self._pull_messages_loop(queue, pull_stop_event)
                )
                self._pull_tasks[str(queue)] = task

                self.logger.debug(
                    f"Started pull task for queue: {queue}",
                    extra={
                        "consumer_group": self._config.consumer_group,
                        "topic": queue.topic,
                        "queue_id": queue.queue_id,
                    },
                )

    async def _stop_pull_tasks(self) -> None:
        """åœæ­¢æ‰€æœ‰æ¶ˆæ¯æ‹‰å–ä»»åŠ¡ - ä½¿ç”¨åœæ­¢äº‹ä»¶ä¼˜é›…å…³é—­"""
        if not self._pull_tasks:
            return

        # è®¾ç½®æ‰€æœ‰åœæ­¢äº‹ä»¶
        async with self._stop_events_lock:
            for queue_key in self._pull_tasks.keys():
                # è®¾ç½®æ‹‰å–åœæ­¢äº‹ä»¶
                if queue_key in self._pull_stop_events:
                    self._pull_stop_events[queue_key].set()
                # è®¾ç½®æ¶ˆè´¹åœæ­¢äº‹ä»¶
                if queue_key in self._consume_stop_events:
                    self._consume_stop_events[queue_key].set()

        # å–æ¶ˆæ‰€æœ‰å¼‚æ­¥ä»»åŠ¡
        for queue_key, task in self._pull_tasks.items():
            if task and not task.done():
                task.cancel()
                try:
                    await task
                except asyncio.CancelledError:
                    self.logger.debug(
                        f"Pull task cancelled for queue: {queue_key}",
                        extra={"consumer_group": self._config.consumer_group},
                    )

        self._pull_tasks.clear()

        # æ¸…ç†åœæ­¢äº‹ä»¶
        async with self._stop_events_lock:
            self._pull_stop_events.clear()
            self._consume_stop_events.clear()

        self.logger.debug(
            "All pull tasks stopped",
            extra={"consumer_group": self._config.consumer_group},
        )

    async def _perform_single_pull(
        self, message_queue: MessageQueue, suggest_broker_id: int
    ) -> tuple[list[MessageExt], int, int] | None:
        """æ‰§è¡Œå•æ¬¡æ¶ˆæ¯æ‹‰å–æ“ä½œã€‚

        Args:
            message_queue: è¦æ‹‰å–æ¶ˆæ¯çš„é˜Ÿåˆ—
            suggest_broker_id: å»ºè®®çš„Broker ID

        Returns:
            tuple[list[MessageExt], int, int] | None:
                - messages: æ‹‰å–åˆ°çš„æ¶ˆæ¯åˆ—è¡¨
                - next_begin_offset: ä¸‹æ¬¡æ‹‰å–çš„èµ·å§‹åç§»é‡
                - next_suggest_id: ä¸‹æ¬¡å»ºè®®çš„Broker ID
            None: å¦‚æœæ²¡æœ‰è®¢é˜…ä¿¡æ¯

        Raises:
            MessagePullError: å½“æ‹‰å–è¯·æ±‚éæ³•æ—¶æŠ›å‡º
        """
        # è·å–å½“å‰åç§»é‡
        current_offset: int = await self._get_or_initialize_offset(message_queue)

        # æ‹‰å–æ¶ˆæ¯
        messages, next_begin_offset, next_suggest_id = await self._pull_messages(
            message_queue,
            current_offset,
            suggest_broker_id,
        )

        # æ£€æŸ¥è®¢é˜…ä¿¡æ¯
        sub: SubscriptionEntry | None = self._subscription_manager.get_subscription(
            message_queue.topic
        )
        if sub is None:
            # å¦‚æœæ²¡æœ‰è®¢é˜…ä¿¡æ¯ï¼Œåˆ™åœæ­¢æ¶ˆè´¹
            return None

        sub_data: SubscriptionData = sub.subscription_data

        # æ ¹æ®è®¢é˜…ä¿¡æ¯è¿‡æ»¤æ¶ˆæ¯
        if sub_data.tags_set:
            messages = self._filter_messages_by_tags(messages, sub_data.tags_set)

        return messages, next_begin_offset, next_suggest_id

    async def _pull_messages(
        self, message_queue: MessageQueue, offset: int, suggest_id: int
    ) -> tuple[list[MessageExt], int, int]:
        """ä»æŒ‡å®šé˜Ÿåˆ—æ‹‰å–æ¶ˆæ¯ï¼Œæ”¯æŒåç§»é‡ç®¡ç†å’ŒBrokeré€‰æ‹©ã€‚

        è¯¥æ–¹æ³•æ˜¯é¡ºåºæ¶ˆè´¹è€…çš„æ ¸å¿ƒæ‹‰å–é€»è¾‘ï¼Œè´Ÿè´£ä»RocketMQ Brokeræ‹‰å–æ¶ˆæ¯ï¼Œ
        å¹¶å¤„ç†ç›¸å…³çš„ç³»ç»Ÿæ ‡å¿—ä½å’Œåç§»é‡ç®¡ç†ã€‚æ”¯æŒä¸»å¤‡Brokerçš„æ™ºèƒ½é€‰æ‹©
        å’Œæ•…éšœè½¬ç§»æœºåˆ¶ã€‚

        æ ¸å¿ƒåŠŸèƒ½:
        - é€šè¿‡NameServerManagerè·å–æœ€ä¼˜Brokeråœ°å€
        - æ„å»ºæ‹‰å–è¯·æ±‚çš„ç³»ç»Ÿæ ‡å¿—ä½
        - å¤„ç†commit offsetçš„æäº¤é€»è¾‘
        - æ”¯æŒæ‰¹é‡æ¶ˆæ¯æ‹‰å–ä»¥æé«˜æ•ˆç‡
        - å®Œå–„çš„é”™è¯¯å¤„ç†å’Œé‡è¯•æœºåˆ¶

        æ‹‰å–ç­–ç•¥:
        1. è·å–ç›®æ ‡Brokeråœ°å€ï¼Œä¼˜å…ˆè¿æ¥master
        2. è¯»å–å½“å‰commit offsetï¼ˆå¦‚æœæœ‰ï¼‰
        3. æ„å»ºåŒ…å«commitæ ‡å¿—çš„ç³»ç»Ÿæ ‡å¿—ä½
        4. å‘é€PULL_MESSAGEè¯·æ±‚åˆ°Broker
        5. è§£æå“åº”å¹¶è¿”å›æ¶ˆæ¯åˆ—è¡¨å’Œä¸‹æ¬¡æ‹‰å–ä½ç½®

        è¿”å›å€¼è¯´æ˜:
        - list[MessageExt]: æ‹‰å–åˆ°çš„æ¶ˆæ¯åˆ—è¡¨ï¼Œå¯èƒ½ä¸ºç©º
        - int: ä¸‹ä¸€æ¬¡æ‹‰å–çš„èµ·å§‹åç§»é‡
        - int: å»ºè®®ä¸‹æ¬¡è¿æ¥çš„Broker IDï¼ˆ0=master, å…¶ä»–=slaveï¼‰

        Args:
            message_queue (MessageQueue): ç›®æ ‡æ¶ˆæ¯é˜Ÿåˆ—ï¼ŒåŒ…å«topicã€brokeråç§°ã€é˜Ÿåˆ—IDç­‰ä¿¡æ¯
            offset (int): æœ¬æ¬¡æ‹‰å–çš„èµ·å§‹åç§»é‡ï¼Œä»è¯¥ä½ç½®å¼€å§‹æ‹‰å–æ¶ˆæ¯
            suggest_id (int): å»ºè®®çš„Broker IDï¼Œç”¨äºè¿æ¥é€‰æ‹©ä¼˜åŒ–ï¼Œ
                            é€šå¸¸ä¸ºä¸Šæ¬¡æ‹‰å–æ—¶è¿”å›çš„å»ºè®®ID

        Returns:
            tuple[list[MessageExt], int, int]: ä¸‰å…ƒç»„åŒ…å«ï¼š
                                            - æ¶ˆæ¯åˆ—è¡¨ï¼ˆå¯èƒ½ä¸ºç©ºï¼‰
                                            - ä¸‹æ¬¡æ‹‰å–çš„èµ·å§‹åç§»é‡
                                            - å»ºè®®çš„ä¸‹æ¬¡Broker ID

        Raises:
            MessageConsumeError: å½“æ‹‰å–è¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯æ—¶æŠ›å‡ºï¼ŒåŒ…å«è¯¦ç»†çš„é”™è¯¯ä¿¡æ¯
            ValueError: å½“æ— æ³•æ‰¾åˆ°æŒ‡å®šbrokerçš„åœ°å€æ—¶æŠ›å‡º
        """
        try:
            self._stats["pull_requests"] = self._stats.get("pull_requests", 0) + 1

            broker_info: (
                tuple[str, bool] | None
            ) = await self._nameserver_manager.get_broker_address_in_subscription(
                message_queue.broker_name, suggest_id
            )
            if not broker_info:
                raise ValueError(
                    f"Broker address not found for {message_queue.broker_name}"
                )

            commit_offset: int = await self._offset_store.read_offset(
                message_queue, ReadOffsetType.READ_FROM_MEMORY
            )

            broker_address, is_master = broker_info

            pool: AsyncConnectionPool = await self._broker_manager.must_connection_pool(
                broker_address
            )

            # ä½¿ç”¨å¼‚æ­¥è¿æ¥æ± æ‹‰å–æ¶ˆæ¯
            async with pool.get_connection() as conn:
                result = await conn.async_pull_message(
                    consumer_group=self._config.consumer_group,
                    topic=message_queue.topic,
                    queue_id=message_queue.queue_id,
                    offset=offset,
                    max_nums=self._config.pull_batch_size,
                    sys_flag=await self._build_sys_flag(
                        commit_offset=commit_offset > 0 and is_master
                    ),
                    commit_offset=commit_offset,
                    timeout=30,
                )

                if result.messages:
                    self._stats["pull_success"] = self._stats.get("pull_success", 0) + 1
                    return (
                        result.messages,
                        result.next_begin_offset,
                        result.suggest_which_broker_id or 0,
                    )

                self._stats["pull_success"] = self._stats.get("pull_success", 0) + 1
                return [], offset, 0

        except MessagePullError as e:
            self._stats["pull_fail"] = self._stats.get("pull_fail", 0) + 1
            self.logger.warning(
                "The pull request is illegal",
                extra={
                    "consumer_group": self._config.consumer_group,
                    "topic": message_queue.topic,
                    "queue_id": message_queue.queue_id,
                    "offset": offset,
                    "error": str(e),
                },
            )
            raise e

        except Exception as e:
            self._stats["pull_fail"] = self._stats.get("pull_fail", 0) + 1
            self.logger.warning(
                "Failed to pull messages",
                extra={
                    "consumer_group": self._config.consumer_group,
                    "topic": message_queue.topic,
                    "queue_id": message_queue.queue_id,
                    "offset": offset,
                    "error": str(e),
                },
            )
            raise MessageConsumeError(
                message_queue.topic,
                "Failed to pull messages",
                offset,
                cause=e,
            ) from e

    async def _build_sys_flag(self, commit_offset: bool) -> int:
        """æ„å»ºç³»ç»Ÿæ ‡å¿—ä½

        æ ¹æ®Goè¯­è¨€å®ç°ï¼š
        - bit 0 (0x1): commitOffset æ ‡å¿—
        - bit 1 (0x2): suspend æ ‡å¿—
        - bit 2 (0x4): subscription æ ‡å¿—
        - bit 3 (0x8): classFilter æ ‡å¿—

        Args:
            commit_offset (bool): æ˜¯å¦æäº¤åç§»é‡

        Returns:
            int: ç³»ç»Ÿæ ‡å¿—ä½
        """
        flag = 0

        if commit_offset:
            flag |= 0x1 << 0  # bit 0: 0x1

        # suspend: always true
        flag |= 0x1 << 1  # bit 1: 0x2

        # subscription: always true
        flag |= 0x1 << 2  # bit 2: 0x4

        # class_filter: always false
        # flag |= 0x1 << 3  # bit 3: 0x8

        return flag

    async def _start_consume_tasks_for_queues(self, queues: set[MessageQueue]) -> None:
        """ä¸ºæŒ‡å®šçš„é˜Ÿåˆ—é›†åˆå¯åŠ¨æ¶ˆè´¹ä»»åŠ¡

        Args:
            queues: éœ€è¦å¯åŠ¨æ¶ˆè´¹ä»»åŠ¡çš„é˜Ÿåˆ—é›†åˆ
        """
        for message_queue in queues:
            queue_key = str(message_queue)

            # è·å–æˆ–åˆ›å»ºåœæ­¢äº‹ä»¶
            async with self._stop_events_lock:
                if queue_key not in self._consume_stop_events:
                    self._consume_stop_events[queue_key] = asyncio.Event()
                consume_stop_event = self._consume_stop_events[queue_key]

            # å¯åŠ¨æ¶ˆè´¹ä»»åŠ¡
            if (
                queue_key not in self._consume_tasks
                or self._consume_tasks[queue_key].done()
            ):
                task = asyncio.create_task(
                    self._consume_messages_loop(message_queue, consume_stop_event)
                )
                self._consume_tasks[queue_key] = task

                self.logger.debug(
                    f"Started consume task for queue: {message_queue}",
                    extra={
                        "consumer_group": self._config.consumer_group,
                        "topic": message_queue.topic,
                        "queue_id": message_queue.queue_id,
                    },
                )

    async def _stop_consume_tasks(self) -> None:
        """åœæ­¢æ‰€æœ‰æ¶ˆæ¯æ¶ˆè´¹ä»»åŠ¡ - ä½¿ç”¨åœæ­¢äº‹ä»¶ä¼˜é›…å…³é—­"""
        if not self._consume_tasks:
            return

        # è®¾ç½®æ‰€æœ‰åœæ­¢äº‹ä»¶
        async with self._stop_events_lock:
            for queue_key in self._consume_stop_events:
                self._consume_stop_events[queue_key].set()

        # ç­‰å¾…æ‰€æœ‰ä»»åŠ¡å®Œæˆ
        tasks_to_cancel = []
        for queue_key, task in self._consume_tasks.items():
            if not task.done():
                # ç»™ä»»åŠ¡ä¸€äº›æ—¶é—´æ¥ä¼˜é›…é€€å‡º
                try:
                    await asyncio.wait_for(task, timeout=1.0)
                except asyncio.TimeoutError:
                    # è¶…æ—¶åˆ™å–æ¶ˆä»»åŠ¡
                    tasks_to_cancel.append(task)

        # å–æ¶ˆè¶…æ—¶çš„ä»»åŠ¡
        for task in tasks_to_cancel:
            task.cancel()

        # ç­‰å¾…å–æ¶ˆå®Œæˆ
        if tasks_to_cancel:
            await asyncio.gather(*tasks_to_cancel, return_exceptions=True)

        self._consume_tasks.clear()

        # æ¸…ç†åœæ­¢äº‹ä»¶
        async with self._stop_events_lock:
            self._consume_stop_events.clear()

        self.logger.debug(
            "All consume tasks stopped",
            extra={"consumer_group": self._config.consumer_group},
        )

    async def _acquire_consume_lock(
        self, message_queue: MessageQueue, stop_event: asyncio.Event
    ) -> tuple[asyncio.Semaphore, bool]:
        """
        è·å–æ¶ˆè´¹é”ï¼ˆæœ¬åœ°é” + è¿œç¨‹é”éªŒè¯ï¼‰ã€‚

        Args:
            message_queue: è¦å¤„ç†çš„æ¶ˆæ¯é˜Ÿåˆ—
            stop_event: åœæ­¢äº‹ä»¶

        Returns:
            tuple[asyncio.Semaphore, bool]: (é˜Ÿåˆ—é”, æ˜¯å¦æˆåŠŸè·å–é”)
        """
        queue_semaphore = self._get_queue_lock(message_queue)
        lock_acquired = False

        # å°è¯•éé˜»å¡è·å–é”ï¼Œå¦‚æœå¤±è´¥åˆ™ç­‰å¾…10msåé‡è¯•
        while not lock_acquired and self._is_running and not stop_event.is_set():
            try:
                lock_acquired = queue_semaphore.acquire_nowait()
            except:
                lock_acquired = False

            if not lock_acquired:
                # ç­‰å¾…10ms
                try:
                    await asyncio.wait_for(stop_event.wait(), timeout=0.01)
                    break  # å¦‚æœæ”¶åˆ°åœæ­¢ä¿¡å·ï¼Œé€€å‡ºå¾ªç¯
                except asyncio.TimeoutError:
                    pass  # è¶…æ—¶æ˜¯æ­£å¸¸çš„ï¼Œç»§ç»­å°è¯•è·å–é”

        # å¦‚æœè·å–é”å¤±è´¥æˆ–æ¶ˆè´¹è€…åœæ­¢ï¼Œåˆ™è¿”å›
        if not lock_acquired or not self._is_running:
            if lock_acquired:
                queue_semaphore.release()
            return queue_semaphore, False

        # æœ¬åœ°é”æŒæœ‰æˆåŠŸï¼Œæ£€æŸ¥è¿œç¨‹é”æ˜¯å¦éœ€è¦é‡æ–°è·å–
        # å¹¿æ’­æ¨¡å¼ä¸‹ä¸éœ€è¦è¿œç¨‹é”ï¼Œæ¯ä¸ªæ¶ˆè´¹è€…ç‹¬ç«‹å¤„ç†æ‰€æœ‰æ¶ˆæ¯
        if self._config.message_model == MessageModel.BROADCASTING:
            self.logger.debug(
                f"Broadcast mode - skipping remote lock for queue {message_queue}",
                extra={
                    "consumer_group": self._config.consumer_group,
                    "client_id": self._config.client_id,
                    "queue": str(message_queue),
                    "operation": "consume_messages_loop",
                    "message_model": "BROADCASTING",
                },
            )
            return queue_semaphore, True

        # é›†ç¾¤æ¨¡å¼ä¸‹éœ€è¦è¿œç¨‹é”æ¥ä¿è¯æ¶ˆæ¯çš„é¡ºåºæ€§
        if not await self._is_remote_lock_valid(message_queue):
            # è¿œç¨‹é”å·²è¿‡æœŸæˆ–ä¸å­˜åœ¨ï¼Œéœ€è¦é‡æ–°è·å–
            if not await self._lock_remote_queue(message_queue):
                self.logger.debug(
                    f"Failed to acquire remote lock for queue {message_queue}, skipping this round",
                    extra={
                        "consumer_group": self._config.consumer_group,
                        "client_id": self._config.client_id,
                        "queue": str(message_queue),
                        "operation": "consume_messages_loop",
                    },
                )
                # é‡Šæ”¾æœ¬åœ°é”å¹¶ç»§ç»­ä¸‹ä¸€è½®å¾ªç¯
                queue_semaphore.release()
                return queue_semaphore, False
        else:
            self.logger.debug(
                f"Using cached remote lock for queue {message_queue}",
                extra={
                    "consumer_group": self._config.consumer_group,
                    "client_id": self._config.client_id,
                    "queue": str(message_queue),
                    "operation": "consume_messages_loop",
                    "lock_cached": True,
                },
            )

        return queue_semaphore, True

    async def _fetch_messages_from_queue(
        self, message_queue: MessageQueue, stop_event: asyncio.Event
    ) -> tuple[ProcessQueue, list[MessageExt]]:
        """
        ä»å¤„ç†é˜Ÿåˆ—è·å–æ¶ˆæ¯ã€‚

        Args:
            message_queue: æ¶ˆæ¯é˜Ÿåˆ—
            stop_event: åœæ­¢äº‹ä»¶

        Returns:
            tuple[ProcessQueue, list[MessageExt]]: (å¤„ç†é˜Ÿåˆ—, æ¶ˆæ¯åˆ—è¡¨)
        """
        pq: ProcessQueue = await self._get_or_create_process_queue(message_queue)

        # å°è¯•è·å–æ¶ˆæ¯
        messages = []

        # ä½¿ç”¨å¼‚æ­¥æ–¹å¼ä»é˜Ÿåˆ—è·å–æ¶ˆæ¯
        while len(messages) < self._config.consume_message_batch_max_size:
            if stop_event.is_set():
                break

            try:
                # éé˜»å¡è·å–æ¶ˆæ¯
                msg = pq.get_message(blocking=False)
                if msg is None:
                    break
                messages.append(msg)
            except:
                break

        return pq, messages

    async def _consume_messages_loop(
        self, message_queue: MessageQueue, stop_event: asyncio.Event
    ) -> None:
        """æ¶ˆè´¹æ¶ˆæ¯çš„ä¸»å¾ªç¯

        Args:
            message_queue: è¦æ¶ˆè´¹çš„é˜Ÿåˆ—
            stop_event: åœæ­¢äº‹ä»¶
        """
        self.logger.debug(
            f"Starting consume messages loop for queue: {message_queue}",
            extra={
                "consumer_group": self._config.consumer_group,
                "topic": message_queue.topic,
                "queue_id": message_queue.queue_id,
            },
        )

        while self._is_running and not stop_event.is_set():
            try:
                # è·å–æ¶ˆè´¹é”
                queue_semaphore, lock_acquired = await self._acquire_consume_lock(
                    message_queue, stop_event
                )

                if not lock_acquired:
                    continue

                try:
                    # ä»é˜Ÿåˆ—è·å–æ¶ˆæ¯
                    pq, messages = await self._fetch_messages_from_queue(
                        message_queue, stop_event
                    )

                    if messages:
                        # å¤„ç†æ¶ˆæ¯
                        await self._process_messages_with_timing(
                            message_queue, pq, messages
                        )
                    else:
                        # æ²¡æœ‰æ¶ˆæ¯ï¼ŒçŸ­æš‚ä¼‘çœ 
                        try:
                            await asyncio.wait_for(stop_event.wait(), timeout=0.1)
                        except asyncio.TimeoutError:
                            pass

                finally:
                    # é‡Šæ”¾é”
                    queue_semaphore.release()

            except asyncio.CancelledError:
                break
            except Exception as e:
                self.logger.error(
                    f"Error in consume messages loop for {message_queue}: {e}",
                    extra={
                        "consumer_group": self._config.consumer_group,
                        "topic": message_queue.topic,
                        "queue_id": message_queue.queue_id,
                        "error": str(e),
                    },
                    exc_info=True,
                )

                # é”™è¯¯æ—¶çŸ­æš‚ç­‰å¾…åç»§ç»­
                try:
                    await asyncio.wait_for(stop_event.wait(), timeout=1.0)
                except asyncio.TimeoutError:
                    continue

        self.logger.debug(
            f"Consume messages loop ended for queue: {message_queue}",
            extra={
                "consumer_group": self._config.consumer_group,
                "topic": message_queue.topic,
                "queue_id": message_queue.queue_id,
            },
        )

    async def _process_messages_with_timing(
        self, message_queue: MessageQueue, pq: ProcessQueue, messages: list[MessageExt]
    ) -> None:
        """å¤„ç†æ¶ˆæ¯å¹¶è®°å½•æ—¶é—´

        Args:
            message_queue: æ¶ˆæ¯é˜Ÿåˆ—
            pq: å¤„ç†é˜Ÿåˆ—
            messages: è¦å¤„ç†çš„æ¶ˆæ¯åˆ—è¡¨
        """
        start_time = time.time()

        try:
            # è°ƒç”¨æ¶ˆæ¯å¤„ç†é€»è¾‘
            await self._process_messages_with_retry(message_queue, pq, messages)

            # æ›´æ–°ç»Ÿè®¡
            process_time = (time.time() - start_time) * 1000  # è½¬æ¢ä¸ºæ¯«ç§’
            self._stats["orderly_consume_rt_total"] = (
                self._stats.get("orderly_consume_rt_total", 0.0) + process_time
            )
            self._stats["orderly_consume_rt_count"] = (
                self._stats.get("orderly_consume_rt_count", 0) + 1
            )
            self._stats["orderly_consume_success_count"] = self._stats.get(
                "orderly_consume_success_count", 0
            ) + len(messages)

        except Exception as e:
            # æ›´æ–°å¤±è´¥ç»Ÿè®¡
            self._stats["orderly_consume_fail_count"] = self._stats.get(
                "orderly_consume_fail_count", 0
            ) + len(messages)
            raise

    async def _process_messages_with_retry(
        self, message_queue: MessageQueue, pq: ProcessQueue, messages: list[MessageExt]
    ) -> None:
        """å¸¦é‡è¯•æœºåˆ¶çš„æ¶ˆæ¯å¤„ç†

        Args:
            message_queue: æ¶ˆæ¯é˜Ÿåˆ—
            pq: å¤„ç†é˜Ÿåˆ—
            messages: è¦å¤„ç†çš„æ¶ˆæ¯åˆ—è¡¨
        """
        # è¿™é‡Œåº”è¯¥è°ƒç”¨å…·ä½“çš„æ¶ˆæ¯å¤„ç†é€»è¾‘
        # æš‚æ—¶ä½¿ç”¨ç®€å•çš„æ—¥å¿—è®°å½•
        self.logger.debug(
            f"Processing {len(messages)} messages from queue: {message_queue}",
            extra={
                "consumer_group": self._config.consumer_group,
                "topic": message_queue.topic,
                "queue_id": message_queue.queue_id,
                "message_count": len(messages),
            },
        )

        # TODO: å®ç°å…·ä½“çš„æ¶ˆæ¯å¤„ç†é€»è¾‘
        # è¿™é‡Œéœ€è¦è°ƒç”¨ç”¨æˆ·çš„MessageListeneræ¥å¤„ç†æ¶ˆæ¯

    async def _pull_messages_loop(
        self,
        message_queue: MessageQueue,
        pull_stop_event: asyncio.Event,
    ) -> None:
        """æŒç»­æ‹‰å–æŒ‡å®šé˜Ÿåˆ—çš„æ¶ˆæ¯ã€‚

        ä¸ºæ¯ä¸ªåˆ†é…çš„é˜Ÿåˆ—åˆ›å»ºç‹¬ç«‹çš„æ‹‰å–å¾ªç¯ï¼ŒæŒç»­ä»Brokeræ‹‰å–æ¶ˆæ¯
        å¹¶æ”¾å…¥å¤„ç†é˜Ÿåˆ—ã€‚è¿™æ˜¯æ¶ˆè´¹è€…æ¶ˆæ¯æ‹‰å–çš„æ ¸å¿ƒæ‰§è¡Œå¾ªç¯ã€‚

        æ‰§è¡Œæµç¨‹ï¼š
        1. ä»é˜Ÿåˆ—çš„å½“å‰åç§»é‡å¼€å§‹æ‹‰å–æ¶ˆæ¯
        2. å¦‚æœæ‹‰å–åˆ°æ¶ˆæ¯ï¼Œæ›´æ–°æœ¬åœ°åç§»é‡è®°å½•
        3. å°†æ¶ˆæ¯æ·»åŠ åˆ°å¤„ç†é˜Ÿåˆ—ç¼“å­˜
        4. æ ¹æ®é…ç½®çš„æ‹‰å–é—´éš”è¿›è¡Œä¼‘çœ æ§åˆ¶

        Args:
            message_queue: è¦æŒç»­æ‹‰å–æ¶ˆæ¯çš„ç›®æ ‡é˜Ÿåˆ—
            pull_stop_event: æ‹‰å–ä»»åŠ¡åœæ­¢äº‹ä»¶

        Note:
            - æ¯ä¸ªé˜Ÿåˆ—æœ‰ç‹¬ç«‹çš„æ‹‰å–ä»»åŠ¡ï¼Œé¿å…é˜Ÿåˆ—é—´ç›¸äº’å½±å“
            - åç§»é‡åœ¨æœ¬åœ°ç»´æŠ¤ï¼Œå®šæœŸæˆ–åœ¨æ¶ˆæ¯å¤„ç†æˆåŠŸåæ›´æ–°åˆ°Broker
            - æ‹‰å–å¤±è´¥ä¼šè®°å½•æ—¥å¿—å¹¶ç­‰å¾…é‡è¯•ï¼Œä¸ä¼šå½±å“å…¶ä»–é˜Ÿåˆ—
            - æ¶ˆè´¹è€…åœæ­¢æ—¶æ­¤å¾ªç¯ä¼šè‡ªåŠ¨é€€å‡º
            - æ”¯æŒé€šè¿‡é…ç½®æ§åˆ¶æ‹‰å–é¢‘ç‡
            - æ”¯æŒé€šè¿‡åœæ­¢äº‹ä»¶ä¼˜é›…å…³é—­
        """
        suggest_broker_id = 0
        while self._is_running and not pull_stop_event.is_set():
            # æ£€æŸ¥æ˜¯å¦éœ€è¦æµé‡æ§åˆ¶
            pq = await self._get_or_create_process_queue(message_queue)
            if pq.need_flow_control():
                # ä½¿ç”¨å¯ä¸­æ–­çš„ç­‰å¾…ï¼Œæ£€æŸ¥åœæ­¢äº‹ä»¶
                try:
                    await asyncio.wait_for(pull_stop_event.wait(), timeout=3.0)
                    break  # æ”¶åˆ°åœæ­¢ä¿¡å·
                except asyncio.TimeoutError:
                    continue  # è¶…æ—¶æ˜¯æ­£å¸¸çš„ï¼Œç»§ç»­æ£€æŸ¥æµé‡æ§åˆ¶

            try:
                # æ‰§è¡Œå•æ¬¡æ‹‰å–æ“ä½œ
                pull_result = await self._perform_single_pull(
                    message_queue, suggest_broker_id
                )

                if pull_result is None:
                    # å¦‚æœè¿”å›Noneï¼Œè¯´æ˜æ²¡æœ‰è®¢é˜…ä¿¡æ¯ï¼Œåœæ­¢æ¶ˆè´¹
                    self.logger.warning(
                        "No subscription found for topic, stopping pull loop",
                        extra={
                            "consumer_group": self._config.consumer_group,
                            "topic": message_queue.topic,
                            "queue_id": message_queue.queue_id,
                        },
                    )
                    break

                messages, next_begin_offset, next_suggest_id = pull_result
                suggest_broker_id = next_suggest_id

                if messages:
                    # å¤„ç†æ‹‰å–åˆ°çš„æ¶ˆæ¯
                    await self._handle_pulled_messages(
                        message_queue, messages, next_begin_offset
                    )
                else:
                    # æ²¡æœ‰æ‹‰å–åˆ°æ¶ˆæ¯ï¼Œåªæ›´æ–°è¯·æ±‚ç»Ÿè®¡
                    self._stats["pull_requests"] = (
                        self._stats.get("pull_requests", 0) + 1
                    )

                # æ§åˆ¶æ‹‰å–é¢‘ç‡ - ä¼ å…¥æ˜¯å¦æœ‰æ¶ˆæ¯çš„æ ‡å¿—ï¼Œä½¿ç”¨å¯ä¸­æ–­ç­‰å¾…
                await self._apply_pull_interval(
                    has_messages=bool(messages), stop_event=pull_stop_event
                )

            except MessagePullError as e:
                self.logger.warning(
                    "The pull request is illegal",
                    extra={
                        "consumer_group": self._config.consumer_group,
                        "topic": message_queue.topic,
                        "queue_id": message_queue.queue_id,
                        "error": str(e),
                    },
                )
                break
            except asyncio.CancelledError:
                break
            except Exception as e:
                self.logger.error(
                    f"Error in pull messages loop for {message_queue}: {e}",
                    extra={
                        "consumer_group": self._config.consumer_group,
                        "topic": message_queue.topic,
                        "queue_id": message_queue.queue_id,
                        "error": str(e),
                    },
                    exc_info=True,
                )

                self._stats["pull_fail"] = self._stats.get("pull_fail", 0) + 1

                # æ‹‰å–å¤±è´¥æ—¶ç­‰å¾…ä¸€æ®µæ—¶é—´å†é‡è¯•ï¼Œä½¿ç”¨å¯ä¸­æ–­ç­‰å¾…
                try:
                    await asyncio.wait_for(pull_stop_event.wait(), timeout=3.0)
                    break  # æ”¶åˆ°åœæ­¢ä¿¡å·
                except asyncio.TimeoutError:
                    continue  # è¶…æ—¶æ˜¯æ­£å¸¸çš„ï¼Œç»§ç»­é‡è¯•

    async def _get_or_create_process_queue(self, queue: MessageQueue) -> ProcessQueue:
        """è·å–æˆ–åˆ›å»ºæŒ‡å®šé˜Ÿåˆ—çš„ProcessQueueï¼ˆæ¶ˆæ¯ç¼“å­˜é˜Ÿåˆ—ï¼‰

        Args:
            queue: æ¶ˆæ¯é˜Ÿåˆ—

        Returns:
            ProcessQueue: æŒ‡å®šé˜Ÿåˆ—çš„å¤„ç†é˜Ÿåˆ—å¯¹è±¡
        """
        async with self._cache_lock:
            if queue not in self._msg_cache:
                self._msg_cache[queue] = ProcessQueue(
                    max_cache_count=self._config.max_cache_count_per_queue,
                    max_cache_size_mb=self._config.max_cache_size_per_queue,
                )
            return self._msg_cache[queue]

    async def _get_or_initialize_offset(self, message_queue: MessageQueue) -> int:
        """è·å–æˆ–åˆå§‹åŒ–æ¶ˆè´¹åç§»é‡ã€‚

        å¦‚æœæœ¬åœ°ç¼“å­˜çš„åç§»é‡ä¸º0ï¼ˆé¦–æ¬¡æ¶ˆè´¹ï¼‰ï¼Œåˆ™æ ¹æ®é…ç½®çš„æ¶ˆè´¹ç­–ç•¥
        ä»ConsumeFromWhereManagerè·å–æ­£ç¡®çš„åˆå§‹åç§»é‡ã€‚

        Args:
            message_queue: è¦è·å–åç§»é‡çš„æ¶ˆæ¯é˜Ÿåˆ—

        Returns:
            int: æ¶ˆè´¹åç§»é‡

        Note:
            - å¦‚æœåç§»é‡ä¸ä¸º0ï¼Œç›´æ¥è¿”å›ç¼“å­˜çš„å€¼
            - å¦‚æœåç§»é‡ä¸º0ï¼Œæ ¹æ®consume_from_whereç­–ç•¥è·å–åˆå§‹åç§»é‡
            - è·å–å¤±è´¥æ—¶ä½¿ç”¨é»˜è®¤åç§»é‡0ï¼Œç¡®ä¿æ¶ˆè´¹æµç¨‹ä¸ä¸­æ–­
        """
        async with self._assigned_queues_lock:  # ğŸ”ä¿æŠ¤_assigned_queuesè®¿é—®
            current_offset: int = self._assigned_queues.get(message_queue, 0)

            # å¦‚æœcurrent_offsetä¸º0ï¼ˆé¦–æ¬¡æ¶ˆè´¹ï¼‰ï¼Œåˆ™ä»_consume_from_where_managerä¸­è·å–æ­£ç¡®çš„åˆå§‹åç§»é‡
            if current_offset == 0:
                try:
                    # è°ƒç”¨å¼‚æ­¥ç‰ˆæœ¬çš„åç§»é‡è·å–
                    current_offset = (
                        await self._consume_from_where_manager.get_consume_offset(
                            message_queue,
                            self._config.consume_from_where,
                            self._config.consume_timestamp
                            if hasattr(self._config, "consume_timestamp")
                            else 0,
                        )
                    )
                    # æ›´æ–°æœ¬åœ°ç¼“å­˜çš„åç§»é‡
                    self._assigned_queues[message_queue] = current_offset

                    self.logger.info(
                        f"åˆå§‹åŒ–æ¶ˆè´¹åç§»é‡: {current_offset}",
                        extra={
                            "consumer_group": self._config.consumer_group,
                            "topic": message_queue.topic,
                            "queue_id": message_queue.queue_id,
                            "strategy": self._config.consume_from_where,
                            "offset": current_offset,
                        },
                    )

                except Exception as e:
                    self.logger.error(
                        f"è·å–åˆå§‹æ¶ˆè´¹åç§»é‡å¤±è´¥ï¼Œä½¿ç”¨é»˜è®¤åç§»é‡0: {e}",
                        extra={
                            "consumer_group": self._config.consumer_group,
                            "topic": message_queue.topic,
                            "queue_id": message_queue.queue_id,
                            "strategy": self._config.consume_from_where,
                            "error": str(e),
                        },
                        exc_info=True,
                    )
                    # ä½¿ç”¨é»˜è®¤åç§»é‡0
                    current_offset = 0

        return current_offset

    async def _handle_pulled_messages(
        self,
        message_queue: MessageQueue,
        messages: list[MessageExt],
        next_begin_offset: int,
    ) -> None:
        """å¤„ç†æ‹‰å–åˆ°çš„æ¶ˆæ¯ã€‚

        åŒ…æ‹¬æ›´æ–°åç§»é‡ã€ç¼“å­˜æ¶ˆæ¯ã€ç»Ÿè®¡ä¿¡æ¯ç­‰ã€‚

        Args:
            message_queue: æ¶ˆæ¯é˜Ÿåˆ—
            messages: æ‹‰å–åˆ°çš„æ¶ˆæ¯åˆ—è¡¨
            next_begin_offset: ä¸‹æ¬¡æ‹‰å–çš„èµ·å§‹åç§»é‡
        """
        # æ›´æ–°åç§»é‡
        async with self._assigned_queues_lock:  # ğŸ”ä¿æŠ¤_assigned_queuesè®¿é—®
            self._assigned_queues[message_queue] = next_begin_offset

        # å°†æ¶ˆæ¯æ·»åŠ åˆ°ç¼“å­˜ä¸­ï¼ˆç”¨äºè§£å†³å¹¶å‘åç§»é‡é—®é¢˜ï¼‰
        await self._add_messages_to_cache(message_queue, messages)

        # æ›´æ–°ç»Ÿè®¡ä¿¡æ¯
        message_count = len(messages)
        self._stats["pull_success"] = self._stats.get("pull_success", 0) + 1
        self._stats["messages_consumed"] = (
            self._stats.get("messages_consumed", 0) + message_count
        )
        self._stats["pull_requests"] = self._stats.get("pull_requests", 0) + 1

        self.logger.debug(
            f"Pulled {len(messages)} messages from queue: {message_queue}",
            extra={
                "consumer_group": self._config.consumer_group,
                "topic": message_queue.topic,
                "queue_id": message_queue.queue_id,
                "message_count": len(messages),
                "next_begin_offset": next_begin_offset,
            },
        )

    async def _apply_pull_interval(
        self, has_messages: bool = True, stop_event: asyncio.Event | None = None
    ) -> None:
        """åº”ç”¨æ™ºèƒ½æ‹‰å–é—´éš”æ§åˆ¶ã€‚

        æ ¹æ®ä¸Šæ¬¡æ‹‰å–ç»“æœæ™ºèƒ½è°ƒæ•´æ‹‰å–é—´éš”ï¼š
        - å¦‚æœä¸Šæ¬¡æ‹‰å–åˆ°äº†æ¶ˆæ¯ï¼Œç«‹å³ç»§ç»­æ‹‰å–ä»¥æé«˜æ¶ˆè´¹é€Ÿåº¦
        - å¦‚æœä¸Šæ¬¡æ‹‰å–ä¸ºç©ºï¼Œåˆ™ä¼‘çœ é…ç½®çš„é—´éš”æ—¶é—´ä»¥é¿å…ç©ºè½®è¯¢

        Args:
            has_messages: ä¸Šæ¬¡æ‹‰å–æ˜¯å¦è·å–åˆ°æ¶ˆæ¯ï¼Œé»˜è®¤ä¸ºTrue
            stop_event: åœæ­¢äº‹ä»¶ï¼Œç”¨äºæ”¯æŒä¼˜é›…å…³é—­ï¼Œé»˜è®¤ä¸ºNone
        """
        if self._config.pull_interval > 0:
            if has_messages:
                # æ‹‰å–åˆ°æ¶ˆæ¯ï¼Œä¸ä¼‘çœ ç»§ç»­æ‹‰å–
                self.logger.debug(
                    "Messages pulled, continuing without interval",
                    extra={
                        "consumer_group": self._config.consumer_group,
                    },
                )
            else:
                # æ‹‰å–ä¸ºç©ºï¼Œä¼‘çœ é…ç½®çš„é—´éš”æ—¶é—´ï¼Œä½¿ç”¨å¯ä¸­æ–­ç­‰å¾…
                sleep_time: float = self._config.pull_interval / 1000.0
                self.logger.debug(
                    f"No messages pulled, sleeping for {sleep_time}s",
                    extra={
                        "consumer_group": self._config.consumer_group,
                        "sleep_time": sleep_time,
                    },
                )
                if stop_event:
                    try:
                        await asyncio.wait_for(stop_event.wait(), timeout=sleep_time)
                    except asyncio.TimeoutError:
                        pass  # è¶…æ—¶æ˜¯æ­£å¸¸çš„ï¼Œç»§ç»­æ‹‰å–
                else:
                    await asyncio.sleep(sleep_time)

    async def _add_messages_to_cache(
        self, queue: MessageQueue, messages: list[MessageExt]
    ) -> None:
        """å°†æ¶ˆæ¯æ·»åŠ åˆ°ProcessQueueç¼“å­˜ä¸­

        æ­¤æ–¹æ³•ç”¨äºå°†ä»Brokeræ‹‰å–çš„æ¶ˆæ¯æ·»åŠ åˆ°ProcessQueueä¸­ï¼Œä¸ºåç»­æ¶ˆè´¹åšå‡†å¤‡ã€‚
        ProcessQueueè‡ªåŠ¨ä¿æŒæŒ‰queue_offsetæ’åºï¼Œå¹¶æä¾›é«˜æ•ˆçš„æ’å…¥ã€æŸ¥è¯¢å’Œç»Ÿè®¡åŠŸèƒ½ã€‚

        Args:
            queue: ç›®æ ‡æ¶ˆæ¯é˜Ÿåˆ—
            messages: è¦æ·»åŠ çš„æ¶ˆæ¯åˆ—è¡¨ï¼Œæ¶ˆæ¯åº”åŒ…å«æœ‰æ•ˆçš„queue_offset

        Note:
            - ä½¿ç”¨ProcessQueueå†…ç½®çš„çº¿ç¨‹å®‰å…¨æœºåˆ¶
            - æŒ‰queue_offsetå‡åºæ’åˆ—ï¼Œæ–¹ä¾¿åç»­æŒ‰åºæ¶ˆè´¹
            - è‡ªåŠ¨è¿‡æ»¤ç©ºæ¶ˆæ¯åˆ—è¡¨ï¼Œé¿å…ä¸å¿…è¦çš„æ“ä½œ
            - è‡ªåŠ¨å»é‡ï¼Œé¿å…é‡å¤ç¼“å­˜ç›¸åŒåç§»é‡çš„æ¶ˆæ¯
            - è‡ªåŠ¨æ£€æŸ¥ç¼“å­˜é™åˆ¶ï¼ˆæ•°é‡å’Œå¤§å°ï¼‰

        See Also:
            _get_or_create_process_queue: è·å–æˆ–åˆ›å»ºProcessQueue
        """
        if not messages:
            return

        process_queue: ProcessQueue = await self._get_or_create_process_queue(queue)
        _ = process_queue.add_batch_messages(messages)

        self.logger.debug(
            f"Added {len(messages)} messages to cache for queue: {queue}",
            extra={
                "consumer_group": self._config.consumer_group,
                "topic": queue.topic,
                "queue_id": queue.queue_id,
                "message_count": len(messages),
            },
        )

    async def _cleanup_on_start_failure(self) -> None:
        """å¼‚æ­¥å¯åŠ¨å¤±è´¥æ—¶çš„èµ„æºæ¸…ç†æ“ä½œã€‚

        å½“æ¶ˆè´¹è€…å¯åŠ¨è¿‡ç¨‹ä¸­å‘ç”Ÿå¼‚å¸¸æ—¶ï¼Œè°ƒç”¨æ­¤æ–¹æ³•æ¸…ç†å·²åˆ†é…çš„èµ„æºï¼Œ
        ç¡®ä¿æ¶ˆè´¹è€…çŠ¶æ€ä¸€è‡´ï¼Œé¿å…èµ„æºæ³„æ¼ã€‚

        æ¸…ç†æµç¨‹ï¼š
        1. åœæ­¢å¼‚æ­¥ä»»åŠ¡ï¼ˆæ‹‰å–ä»»åŠ¡ã€æ¶ˆè´¹ä»»åŠ¡ã€é‡å¹³è¡¡ä»»åŠ¡ï¼‰
        2. åœæ­¢æ ¸å¿ƒç»„ä»¶ï¼ˆNameServerã€BrokerManagerã€åç§»é‡å­˜å‚¨ï¼‰
        3. æ¸…ç†å†…å­˜èµ„æºå’Œé˜Ÿåˆ—

        Args:
            None

        Returns:
            None

        Raises:
            None: æ­¤æ–¹æ³•ä¼šæ•è·æ‰€æœ‰å¼‚å¸¸å¹¶è®°å½•æ—¥å¿—

        Note:
            æ­¤æ–¹æ³•æ˜¯å¼‚æ­¥çš„ï¼Œç¡®ä¿æ‰€æœ‰IOæ“ä½œéƒ½ä¸é˜»å¡äº‹ä»¶å¾ªç¯
        """
        try:
            self.logger.info(
                "Cleaning up resources after startup failure",
                extra={"consumer_group": self._config.consumer_group},
            )

            # åœæ­¢å¼‚æ­¥ä»»åŠ¡
            await self._shutdown_async_tasks()

            # åœæ­¢æ ¸å¿ƒç»„ä»¶
            await self._async_cleanup_resources()

            self.logger.info(
                "Startup failure cleanup completed",
                extra={"consumer_group": self._config.consumer_group},
            )

        except Exception as e:
            self.logger.error(
                f"Error during startup failure cleanup: {e}",
                extra={
                    "consumer_group": self._config.consumer_group,
                    "error": str(e),
                },
                exc_info=True,
            )

    async def _wait_for_processing_completion(self) -> None:
        """å¼‚æ­¥ç­‰å¾…æ­£åœ¨å¤„ç†çš„æ¶ˆæ¯å®Œæˆ

        ç­‰å¾…æ‰€æœ‰æ¶ˆè´¹ä»»åŠ¡å®Œæˆï¼Œæœ€å¤šç­‰å¾…30ç§’ã€‚å¦‚æœè¶…æ—¶ï¼Œä¼šè®°å½•è­¦å‘Šæ—¥å¿—
        ä½†ä¸ä¼šé˜»å¡å…³é—­æµç¨‹ã€‚
        """
        try:
            timeout: int = 30  # 30ç§’è¶…æ—¶

            # æ”¶é›†æ‰€æœ‰æœªå®Œæˆçš„æ¶ˆè´¹ä»»åŠ¡
            all_tasks: list[asyncio.Task[None]] = []
            for task in self._consume_tasks.values():
                if task and not task.done():
                    all_tasks.append(task)

            # ç­‰å¾…æ‰€æœ‰ä»»åŠ¡å®Œæˆæˆ–è¶…æ—¶
            if all_tasks:
                try:
                    await asyncio.wait_for(
                        asyncio.gather(*all_tasks, return_exceptions=True),
                        timeout=timeout,
                    )
                except asyncio.TimeoutError:
                    # å–æ¶ˆæœªå®Œæˆçš„ä»»åŠ¡
                    for task in all_tasks:
                        if not task.done():
                            task.cancel()

                    # ç­‰å¾…å–æ¶ˆå®Œæˆï¼ˆçŸ­æš‚ç­‰å¾…ï¼‰
                    try:
                        await asyncio.wait_for(
                            asyncio.gather(*all_tasks, return_exceptions=True),
                            timeout=5.0,
                        )
                    except asyncio.TimeoutError:
                        pass

                    self.logger.warning(
                        "Timeout waiting for consume tasks to complete during shutdown",
                        extra={
                            "consumer_group": self._config.consumer_group,
                            "timeout": timeout,
                            "cancelled_tasks": len(all_tasks),
                        },
                    )

        except Exception as e:
            self.logger.error(
                f"Error waiting for processing completion: {e}",
                extra={
                    "consumer_group": self._config.consumer_group,
                    "error": str(e),
                },
                exc_info=True,
            )

    async def _shutdown_async_tasks(self) -> None:
        """å¼‚æ­¥å…³é—­æ‰€æœ‰å¼‚æ­¥ä»»åŠ¡"""
        try:
            # å–æ¶ˆé‡å¹³è¡¡ä»»åŠ¡
            if self._rebalance_task and not self._rebalance_task.done():
                self._rebalance_task.cancel()
                try:
                    await self._rebalance_task
                except asyncio.CancelledError:
                    pass
                self._rebalance_task = None

            # å–æ¶ˆæ‰€æœ‰æ‹‰å–ä»»åŠ¡
            for task in self._pull_tasks.values():
                if task and not task.done():
                    task.cancel()
            self._pull_tasks.clear()

            # å–æ¶ˆæ‰€æœ‰æ¶ˆè´¹ä»»åŠ¡
            for task in self._consume_tasks.values():
                if task and not task.done():
                    task.cancel()
            self._consume_tasks.clear()

            self.logger.info(
                "Async tasks shutdown completed",
                extra={"consumer_group": self._config.consumer_group},
            )

        except Exception as e:
            self.logger.error(
                f"Error shutting down async tasks: {e}",
                extra={
                    "consumer_group": self._config.consumer_group,
                    "error": str(e),
                },
                exc_info=True,
            )

    async def _cleanup_resources(self) -> None:
        """å¼‚æ­¥æ¸…ç†èµ„æº"""
        try:
            # æ¸…ç†ProcessQueueæ¶ˆæ¯ç¼“å­˜
            for process_queue in self._msg_cache.values():
                _ = process_queue.clear()
            self._msg_cache.clear()

            # æ¸…ç†åœæ­¢äº‹ä»¶
            async with self._stop_events_lock:
                self._pull_stop_events.clear()
                self._consume_stop_events.clear()

            # æ¸…ç†çŠ¶æ€
            self._pull_tasks.clear()
            self._consume_tasks.clear()

            # è¿œç¨‹è§£é”æ‰€æœ‰å·²åˆ†é…çš„é˜Ÿåˆ—
            async with self._assigned_queues_lock:
                assigned_queues = list(
                    self._assigned_queues.keys()
                )  # å¤åˆ¶ä¸€ä»½é¿å…å¹¶å‘ä¿®æ”¹

            for queue in assigned_queues:
                try:
                    await self._unlock_remote_queue(queue)
                except Exception as e:
                    self.logger.warning(
                        f"Failed to unlock remote queue {queue} during cleanup: {e}",
                        extra={
                            "consumer_group": self._config.consumer_group,
                            "topic": queue.topic,
                            "queue_id": queue.queue_id,
                            "broker_name": queue.broker_name,
                            "error": str(e),
                        },
                    )

            # æ¸…ç†é˜Ÿåˆ—é”
            self._queue_locks.clear()

            # æ¸…ç†è¿œç¨‹é”ç¼“å­˜
            async with self._remote_lock_cache_lock:
                self._remote_lock_cache.clear()

            # æ¸…ç†åˆ†é…çš„é˜Ÿåˆ—
            async with self._assigned_queues_lock:
                self._assigned_queues.clear()

            self.logger.info(
                "Async resources cleanup completed",
                extra={"consumer_group": self._config.consumer_group},
            )

        except Exception as e:
            self.logger.error(
                f"Error during async resource cleanup: {e}",
                extra={
                    "consumer_group": self._config.consumer_group,
                    "error": str(e),
                },
                exc_info=True,
            )

    async def _get_final_stats(self) -> dict[str, Any]:
        """å¼‚æ­¥è·å–æœ€ç»ˆç»Ÿè®¡ä¿¡æ¯"""
        stats = {
            "consumer_group": self._config.consumer_group,
            "shutdown_time": time.time(),
        }

        # åˆå¹¶é¡ºåºæ¶ˆè´¹ç»Ÿè®¡ä¿¡æ¯
        stats.update(self._stats)

        # æ·»åŠ é˜Ÿåˆ—ç›¸å…³ä¿¡æ¯
        async with self._assigned_queues_lock:
            stats["assigned_queues_count"] = len(self._assigned_queues)

        return stats
