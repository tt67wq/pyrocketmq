"""
AsyncConcurrentConsumer - å¼‚æ­¥å¹¶å‘æ¶ˆè´¹è€…å®ç°

AsyncConcurrentConsumeræ˜¯pyrocketmqçš„å¼‚æ­¥å¹¶å‘æ¶ˆè´¹è€…å®ç°ï¼Œæ”¯æŒé«˜å¹¶å‘å¼‚æ­¥æ¶ˆæ¯æ¶ˆè´¹ã€‚
å®ƒä½¿ç”¨asyncioå’Œå¼‚æ­¥ä»»åŠ¡æ¥å¹¶è¡Œå¤„ç†å¤šä¸ªé˜Ÿåˆ—çš„æ¶ˆæ¯ï¼Œæä¾›é«˜ååé‡çš„å¼‚æ­¥æ¶ˆæ¯æ¶ˆè´¹èƒ½åŠ›ã€‚

ä¸»è¦ç‰¹æ€§:
- å¼‚æ­¥å¹¶å‘æ¶ˆè´¹ï¼ŒåŸºäºasyncioäº‹ä»¶å¾ªç¯
- è‡ªåŠ¨é‡å¹³è¡¡å’Œé˜Ÿåˆ—åˆ†é…
- å¼‚æ­¥åç§»é‡ç®¡ç†
- å®Œå–„çš„é”™è¯¯å¤„ç†å’Œæ¢å¤
- ä¸°å¯Œçš„ç›‘æ§æŒ‡æ ‡

ä½œè€…: pyrocketmqå¼€å‘å›¢é˜Ÿ
"""

import asyncio
import time
from datetime import datetime

# pyrocketmqå¯¼å…¥
from pyrocketmq.broker import AsyncBrokerClient, MessagePullError
from pyrocketmq.consumer.allocate_queue_strategy import AllocateContext
from pyrocketmq.consumer.async_base_consumer import AsyncBaseConsumer
from pyrocketmq.consumer.async_listener import (
    ConsumeResult,
)
from pyrocketmq.consumer.async_process_queue import AsyncProcessQueue
from pyrocketmq.consumer.config import ConsumerConfig
from pyrocketmq.consumer.errors import (
    ConsumerShutdownError,
    MessageConsumeError,
)
from pyrocketmq.consumer.offset_store import ReadOffsetType
from pyrocketmq.logging import get_logger
from pyrocketmq.model import (
    ConsumeMessageDirectlyHeader,
    ConsumeMessageDirectlyResult,
    MessageExt,
    MessageModel,
    MessageQueue,
    PullMessageResult,
    RemotingCommand,
    RemotingCommandBuilder,
    RequestCode,
    ResponseCode,
    SubscriptionData,
    SubscriptionEntry,
)
from pyrocketmq.remote import AsyncConnectionPool

logger = get_logger(__name__)


# ç±»å‹åˆ«åå®šä¹‰
ProcessQueueItem = tuple[list[MessageExt], MessageQueue]
PullTaskDict = dict[MessageQueue, asyncio.Task[None]]
MessageQueueDict = dict[MessageQueue, int]


class AsyncConcurrentConsumer(AsyncBaseConsumer):
    """
    å¼‚æ­¥å¹¶å‘æ¶ˆè´¹è€…å®ç°

    æ”¯æŒå¼‚æ­¥å¹¶å‘æ¶ˆè´¹æ¶ˆæ¯çš„æ¶ˆè´¹è€…å®ç°ã€‚æ¯ä¸ªé˜Ÿåˆ—éƒ½æœ‰ä¸“é—¨çš„å¼‚æ­¥ä»»åŠ¡è´Ÿè´£æ‹‰å–æ¶ˆæ¯ï¼Œ
    å¹¶ä½¿ç”¨å¼‚æ­¥ä»»åŠ¡æ± æ¥å¹¶è¡Œå¤„ç†æ¶ˆæ¯ï¼Œæä¾›é«˜ååé‡çš„å¼‚æ­¥æ¶ˆæ¯æ¶ˆè´¹èƒ½åŠ›ã€‚

    æ ¸å¿ƒç‰¹æ€§:
    - å¼‚æ­¥å¹¶å‘æ‹‰å–å’Œå¤„ç†æ¶ˆæ¯
    - è‡ªåŠ¨é˜Ÿåˆ—é‡å¹³è¡¡å’Œåˆ†é…
    - æ™ºèƒ½åç§»é‡ç®¡ç†
    - å®Œå–„çš„é”™è¯¯å¤„ç†å’Œé‡è¯•æœºåˆ¶
    - ä¸°å¯Œçš„æ€§èƒ½ç›‘æ§æŒ‡æ ‡

    ä½¿ç”¨ç¤ºä¾‹:
        >>> from pyrocketmq.consumer import AsyncConcurrentConsumer, ConsumerConfig
        >>> from pyrocketmq.consumer.async_listener import AsyncMessageListener, ConsumeResult
        >>>
        >>> class MyAsyncListener(AsyncMessageListener):
        ...     async def consume_message(self, messages, context):
        ...         for msg in messages:
        ...             print(f"Processing: {msg.body.decode()}")
        ...         return ConsumeResult.SUCCESS
        >>>
        >>> config = ConsumerConfig(
        ...     consumer_group="my_group",
        ...     namesrv_addr="localhost:9876"
        ... )
        >>> consumer = AsyncConcurrentConsumer(config)
        >>> await consumer.register_message_listener(MyAsyncListener())
        >>> await consumer.subscribe("test_topic", create_tag_selector("*"))
        >>> await consumer.start()
    """

    # ==================== åˆå§‹åŒ–å’ŒåŸºç¡€é…ç½® ====================

    def __init__(self, config: ConsumerConfig) -> None:
        """
        åˆå§‹åŒ–å¼‚æ­¥å¹¶å‘æ¶ˆè´¹è€…

        Args:
            config (ConsumerConfig): æ¶ˆè´¹è€…é…ç½®å‚æ•°
        """
        super().__init__(config)

        # ==================== å¼‚æ­¥ä»»åŠ¡ç®¡ç† ====================
        self._pull_tasks: PullTaskDict = {}
        self._consume_task: asyncio.Task[None] | None = None
        self._rebalance_task: asyncio.Task[None] | None = None

        # ==================== å¹¶å‘æ§åˆ¶ ====================
        self._consume_semaphore: asyncio.Semaphore = asyncio.Semaphore(
            self._config.consume_thread_max
        )
        self._pull_semaphore: asyncio.Semaphore = asyncio.Semaphore(
            min(self._config.consume_thread_max, 10)
        )

        # ==================== é˜Ÿåˆ—å’Œç¼“å­˜ç®¡ç† ====================
        self._process_queue: asyncio.Queue[ProcessQueueItem] = asyncio.Queue()
        self._assigned_queues: MessageQueueDict = {}

        # ==================== å¼‚æ­¥é” ====================
        self._assigned_queues_lock = (
            asyncio.Lock()
        )  # ğŸ”ä¿æŠ¤_assigned_queueså­—å…¸çš„å¹¶å‘è®¿é—®
        self._stats_lock = asyncio.Lock()
        self._rebalance_lock = asyncio.Lock()

        # ==================== äº‹ä»¶åŒæ­¥ ====================
        self._rebalance_event = asyncio.Event()

        logger.info(
            "AsyncConcurrentConsumer initialized",
            extra={
                "consumer_group": self._config.consumer_group,
                "message_model": self._config.message_model,
                "consume_thread_max": self._config.consume_thread_max,
                "pull_batch_size": self._config.pull_batch_size,
            },
        )

    # ==================== ç”Ÿå‘½å‘¨æœŸç®¡ç†æ¨¡å— ====================
    # åŠŸèƒ½ï¼šè´Ÿè´£æ¶ˆè´¹è€…çš„å¯åŠ¨ã€å…³é—­å’Œèµ„æºæ¸…ç†
    # åŒ…å«ï¼šstart(), shutdown(), ä»¥åŠç›¸å…³çš„è¾…åŠ©æ–¹æ³•
    # ä½œç”¨ï¼šç¡®ä¿æ¶ˆè´¹è€…å®‰å…¨å¯åŠ¨å’Œä¼˜é›…å…³é—­ï¼Œæ­£ç¡®ç®¡ç†èµ„æºç”Ÿå‘½å‘¨æœŸ

    async def start(self) -> None:
        """å¯åŠ¨å¼‚æ­¥å¹¶å‘æ¶ˆè´¹è€…ã€‚

        åˆå§‹åŒ–å¹¶å¯åŠ¨æ¶ˆè´¹è€…çš„æ‰€æœ‰ç»„ä»¶ï¼ŒåŒ…æ‹¬ï¼š
        - å»ºç«‹ä¸NameServerå’ŒBrokerçš„ç½‘ç»œè¿æ¥
        - åˆ›å»ºå¼‚æ­¥æ¶ˆæ¯æ‹‰å–å’Œå¤„ç†ä»»åŠ¡
        - æ‰§è¡Œåˆå§‹é˜Ÿåˆ—åˆ†é…å’Œé‡å¹³è¡¡
        - å¯åŠ¨å¿ƒè·³å’Œé‡å¹³è¡¡åå°ä»»åŠ¡

        å¯åŠ¨å¤±è´¥æ—¶ä¼šè‡ªåŠ¨æ¸…ç†å·²åˆ†é…çš„èµ„æºã€‚

        Raises:
            ConsumerStartError: å½“ä»¥ä¸‹æƒ…å†µå‘ç”Ÿæ—¶æŠ›å‡ºï¼š
                - æœªæ³¨å†Œæ¶ˆæ¯ç›‘å¬å™¨
                - æ¶ˆæ¯ç›‘å¬å™¨ç±»å‹ä¸åŒ¹é…ï¼ˆéœ€è¦AsyncMessageListenerï¼‰
                - ç½‘ç»œè¿æ¥å¤±è´¥
                - å¼‚æ­¥ä»»åŠ¡åˆ›å»ºå¤±è´¥
                - å…¶ä»–åˆå§‹åŒ–é”™è¯¯

        Note:
            æ­¤æ–¹æ³•æ˜¯åç¨‹ï¼Œéœ€è¦åœ¨asyncä¸Šä¸‹æ–‡ä¸­è°ƒç”¨ã€‚
            å¯åŠ¨æˆåŠŸåï¼Œæ¶ˆè´¹è€…ä¼šè‡ªåŠ¨å¼€å§‹æ‹‰å–å’Œå¤„ç†æ¶ˆæ¯ã€‚
        """
        async with self._lock:
            if self._is_running:
                logger.warning("AsyncConsumer is already running")
                return

            try:
                logger.info(
                    "Starting AsyncConcurrentConsumer",
                    extra={
                        "consumer_group": self._config.consumer_group,
                        "namesrv_addr": self._config.namesrv_addr,
                    },
                )

                # å¯åŠ¨AsyncBaseConsumer
                await super().start()

                # æ‰§è¡Œé‡å¹³è¡¡
                await self._do_rebalance()

                # å¯åŠ¨é‡å¹³è¡¡ä»»åŠ¡
                await self._start_rebalance_task()

                # å¯åŠ¨æ¶ˆæ¯å¤„ç†ä»»åŠ¡
                await self._start_consume_tasks()

                # è·å–åˆ†é…é˜Ÿåˆ—æ•°é‡ç”¨äºæ—¥å¿—ç»Ÿè®¡
                async with self._assigned_queues_lock:  # ğŸ”ä¿æŠ¤_assigned_queuesè®¿é—®
                    assigned_queues_count = len(self._assigned_queues)

                logger.info(
                    "AsyncConcurrentConsumer started successfully",
                    extra={
                        "consumer_group": self._config.consumer_group,
                        "assigned_queues": assigned_queues_count,
                        "consume_concurrency": self._config.consume_thread_max,
                        "pull_concurrency": min(self._config.consume_thread_max, 10),
                    },
                )

            except Exception as e:
                logger.error(
                    f"Failed to start AsyncConcurrentConsumer: {e}",
                    extra={
                        "consumer_group": self._config.consumer_group,
                        "error": str(e),
                    },
                    exc_info=True,
                )

    async def shutdown(self) -> None:
        """å…³é—­å¼‚æ­¥å¹¶å‘æ¶ˆè´¹è€…ã€‚

        æŒ‰ç…§ä»¥ä¸‹é¡ºåºå®‰å…¨å…³é—­æ‰€æœ‰ç»„ä»¶ï¼š
        1. åœæ­¢è¿è¡ŒçŠ¶æ€æ ‡è®°
        2. åœæ­¢æ‰€æœ‰æ¶ˆæ¯æ‹‰å–ä»»åŠ¡
        3. åœæ­¢æ¶ˆæ¯å¤„ç†ä»»åŠ¡
        4. ç­‰å¾…æ­£åœ¨å¤„ç†çš„æ¶ˆæ¯å®Œæˆ
        5. æŒä¹…åŒ–åç§»é‡
        6. å…³é—­ç½‘ç»œè¿æ¥

        æ­¤æ–¹æ³•ä¼šç­‰å¾…æ‰€æœ‰æ­£åœ¨å¤„ç†çš„æ¶ˆæ¯å®Œæˆï¼Œç¡®ä¿æ¶ˆæ¯ä¸ä¸¢å¤±ã€‚

        Raises:
            ConsumerShutdownError: å½“å…³é—­è¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯æ—¶æŠ›å‡º

        Note:
            æ­¤æ–¹æ³•æ˜¯åç¨‹ï¼Œéœ€è¦åœ¨asyncä¸Šä¸‹æ–‡ä¸­è°ƒç”¨ã€‚
            è°ƒç”¨æ­¤æ–¹æ³•åï¼Œæ¶ˆè´¹è€…å®ä¾‹ä¸å¯å†æ¬¡ä½¿ç”¨ã€‚
        """
        async with self._lock:
            if not self._is_running:
                logger.warning("AsyncConsumer is not running")
                return

            try:
                logger.info(
                    "Shutting down AsyncConcurrentConsumer",
                    extra={
                        "consumer_group": self._config.consumer_group,
                    },
                )

                # åœæ­¢è¿è¡ŒçŠ¶æ€
                self._is_running = False

                # åœæ­¢é‡å¹³è¡¡äº‹ä»¶
                self._rebalance_event.set()

                # åœæ­¢æ‰€æœ‰æ‹‰å–ä»»åŠ¡
                await self._stop_pull_tasks()

                # åœæ­¢æ¶ˆæ¯å¤„ç†ä»»åŠ¡
                await self._stop_consume_tasks()

                # ç­‰å¾…å¤„ç†å®Œæˆ
                await self._wait_for_processing_completion()

                # å…³é—­AsyncBaseConsumer
                await super().shutdown()

                logger.info(
                    "AsyncConcurrentConsumer shutdown completed",
                    extra={
                        "consumer_group": self._config.consumer_group,
                    },
                )

            except Exception as e:
                logger.error(
                    f"Error during AsyncConcurrentConsumer shutdown: {e}",
                    extra={
                        "consumer_group": self._config.consumer_group,
                        "error": str(e),
                    },
                    exc_info=True,
                )
                raise ConsumerShutdownError(
                    f"Failed to shutdown AsyncConcurrentConsumer: {e}"
                ) from e
            finally:
                # æ¸…ç†èµ„æº
                await self._cleanup_resources()

        # å¦‚æœæ¶ˆè´¹è€…å·²å¯åŠ¨ï¼Œè§¦å‘é‡å¹³è¡¡
        if self._is_running:
            await self._trigger_rebalance()

    # ==================== ä»»åŠ¡è°ƒåº¦ç®¡ç†æ¨¡å— ====================
    # åŠŸèƒ½ï¼šç®¡ç†æ¶ˆè´¹è€…çš„åå°ä»»åŠ¡ï¼ŒåŒ…æ‹¬æ¶ˆæ¯æ‹‰å–ã€æ¶ˆæ¯å¤„ç†å’Œé‡å¹³è¡¡ä»»åŠ¡
    # åŒ…å«ï¼šå„ç±»ä»»åŠ¡çš„å¯åŠ¨ã€åœæ­¢å’Œå¾ªç¯é€»è¾‘
    # ä½œç”¨ï¼šç¡®ä¿æ¶ˆè´¹è€…å„ç»„ä»¶å¼‚æ­¥ä»»åŠ¡æ­£å¸¸è¿è¡Œï¼Œæä¾›å¯é çš„åå°æœåŠ¡

    async def _start_rebalance_task(self) -> None:
        """å¯åŠ¨é‡å¹³è¡¡ä»»åŠ¡"""
        self._rebalance_task = asyncio.create_task(self._rebalance_loop())

    async def _rebalance_loop(self) -> None:
        """é‡å¹³è¡¡å¾ªç¯

        å®šæœŸæ‰§è¡Œé‡å¹³è¡¡æ“ä½œï¼Œå¤„ç†æ¶ˆè´¹è€…ç»„å˜åŒ–å’Œé˜Ÿåˆ—é‡æ–°åˆ†é…ã€‚
        ä½¿ç”¨äº‹ä»¶é©±åŠ¨å’Œè¶…æ—¶æœºåˆ¶ç¡®ä¿åŠæ—¶å“åº”å˜åŒ–ã€‚
        """
        while self._is_running:
            try:
                # ç­‰å¾…é‡å¹³è¡¡äº‹ä»¶æˆ–è¶…æ—¶
                try:
                    await asyncio.wait_for(self._rebalance_event.wait(), timeout=20.0)
                except asyncio.TimeoutError:
                    # è¶…æ—¶ä¹Ÿæ‰§è¡Œé‡å¹³è¡¡
                    pass

                # æ¸…é™¤äº‹ä»¶
                self._rebalance_event.clear()

                # æ‰§è¡Œé‡å¹³è¡¡
                if self._is_running:
                    await self._do_rebalance()

            except asyncio.CancelledError:
                logger.debug("Rebalance loop cancelled")
                break
            except Exception as e:
                logger.error(
                    f"Error in rebalance loop: {e}",
                    extra={"error": str(e)},
                    exc_info=True,
                )
                await asyncio.sleep(5.0)

    async def _start_consume_tasks(self) -> None:
        """å¯åŠ¨æ¶ˆæ¯å¤„ç†ä»»åŠ¡"""
        self._consume_task = asyncio.create_task(self._consume_messages_loop())

    async def _stop_consume_tasks(self) -> None:
        """åœæ­¢æ¶ˆæ¯å¤„ç†ä»»åŠ¡"""
        if self._consume_task and not self._consume_task.done():
            self._consume_task.cancel()
            try:
                await self._consume_task
            except asyncio.CancelledError:
                pass
        self._consume_task = None

    async def _start_pull_tasks_for_queues(self, queues: set[MessageQueue]) -> None:
        """ä¸ºæŒ‡å®šé˜Ÿåˆ—å¯åŠ¨æ‹‰å–ä»»åŠ¡

        ä¸ºæ¯ä¸ªæ–°åˆ†é…çš„é˜Ÿåˆ—åˆ›å»ºç‹¬ç«‹çš„æ‹‰å–ä»»åŠ¡ï¼Œå®ç°å¹¶å‘çš„æ¶ˆæ¯æ‹‰å–ã€‚

        Args:
            queues: éœ€è¦å¯åŠ¨æ‹‰å–ä»»åŠ¡çš„é˜Ÿåˆ—é›†åˆ
        """
        for queue in queues:
            if queue not in self._pull_tasks:
                task = asyncio.create_task(self._pull_messages_loop(queue))
                self._pull_tasks[queue] = task

    async def _stop_pull_tasks(self) -> None:
        """åœæ­¢æ‰€æœ‰æ¶ˆæ¯æ‹‰å–ä»»åŠ¡

        å–æ¶ˆæ‰€æœ‰æ‹‰å–ä»»åŠ¡å¹¶ç­‰å¾…å®Œæˆï¼Œç¡®ä¿ä¼˜é›…å…³é—­ã€‚
        """
        if not self._pull_tasks:
            return

        # å–æ¶ˆæ‰€æœ‰æ‹‰å–ä»»åŠ¡
        for task in self._pull_tasks.values():
            if not task.done():
                task.cancel()

        # ç­‰å¾…æ‰€æœ‰ä»»åŠ¡å®Œæˆ
        if self._pull_tasks:
            await asyncio.gather(*self._pull_tasks.values(), return_exceptions=True)

        self._pull_tasks.clear()

    async def _wait_for_processing_completion(self) -> None:
        """ç­‰å¾…æ­£åœ¨å¤„ç†çš„æ¶ˆæ¯å®Œæˆ

        åœ¨å…³é—­è¿‡ç¨‹ä¸­ç­‰å¾…æ‰€æœ‰æ¶ˆæ¯å¤„ç†å®Œæˆï¼Œç¡®ä¿æ¶ˆæ¯ä¸ä¸¢å¤±ã€‚
        """
        # ç­‰å¾…å¤„ç†é˜Ÿåˆ—ä¸ºç©º
        while not self._process_queue.empty():
            await asyncio.sleep(0.1)

        # ç­‰å¾…æ‰€æœ‰æ‹‰å–ä»»åŠ¡å®Œæˆ
        if self._pull_tasks:
            await asyncio.gather(*self._pull_tasks.values(), return_exceptions=True)

        # ç­‰å¾…æ¶ˆè´¹ä»»åŠ¡å®Œæˆ
        if self._consume_task and not self._consume_task.done():
            await asyncio.wait_for(self._consume_task, timeout=30.0)

    # ==================== æ¶ˆæ¯æ‹‰å–æ¨¡å— ====================
    # åŠŸèƒ½ï¼šè´Ÿè´£ä»Brokeræ‹‰å–æ¶ˆæ¯ï¼ŒåŒ…æ‹¬æ‹‰å–å¾ªç¯ã€å•æ¬¡æ‹‰å–ã€æ¶ˆæ¯å¤„ç†ç­‰
    # åŒ…å«ï¼šæ‹‰å–ä»»åŠ¡çš„å®Œæ•´ç”Ÿå‘½å‘¨æœŸç®¡ç†
    # ä½œç”¨ï¼šæŒç»­ä»Brokerè·å–æ¶ˆæ¯å¹¶æäº¤åˆ°å¤„ç†é˜Ÿåˆ—ï¼Œæ˜¯æ¶ˆæ¯æ¶ˆè´¹çš„æºå¤´

    async def _pull_messages_loop(self, message_queue: MessageQueue) -> None:
        """æŒç»­æ‹‰å–æŒ‡å®šé˜Ÿåˆ—çš„æ¶ˆæ¯ã€‚

        ä¸ºæ¯ä¸ªåˆ†é…çš„é˜Ÿåˆ—åˆ›å»ºç‹¬ç«‹çš„æ‹‰å–å¾ªç¯ï¼ŒæŒç»­ä»Brokeræ‹‰å–æ¶ˆæ¯
        å¹¶æ”¾å…¥å¤„ç†é˜Ÿåˆ—ã€‚è¿™æ˜¯æ¶ˆè´¹è€…æ¶ˆæ¯æ‹‰å–çš„æ ¸å¿ƒæ‰§è¡Œå¾ªç¯ã€‚

        æ‰§è¡Œæµç¨‹ï¼š
        1. ä»é˜Ÿåˆ—çš„å½“å‰åç§»é‡å¼€å§‹æ‹‰å–æ¶ˆæ¯
        2. å¦‚æœæ‹‰å–åˆ°æ¶ˆæ¯ï¼Œæ›´æ–°æœ¬åœ°åç§»é‡è®°å½•
        3. å°†æ¶ˆæ¯å’Œå¤„ç†é˜Ÿåˆ—ä¿¡æ¯æäº¤ç»™æ¶ˆè´¹çº¿ç¨‹æ± 
        4. æ ¹æ®é…ç½®çš„æ‹‰å–é—´éš”è¿›è¡Œä¼‘çœ æ§åˆ¶

        Args:
            message_queue (MessageQueue): è¦æŒç»­æ‹‰å–æ¶ˆæ¯çš„ç›®æ ‡é˜Ÿåˆ—

        Returns:
            None

        Raises:
            None: æ­¤æ–¹æ³•ä¼šæ•è·æ‰€æœ‰å¼‚å¸¸å¹¶è®°å½•æ—¥å¿—ï¼Œä¸ä¼šä¸­æ–­æ‹‰å–å¾ªç¯

        Note:
            - æ¯ä¸ªé˜Ÿåˆ—æœ‰ç‹¬ç«‹çš„æ‹‰å–ä»»åŠ¡ï¼Œé¿å…é˜Ÿåˆ—é—´ç›¸äº’å½±å“
            - åç§»é‡åœ¨æœ¬åœ°ç»´æŠ¤ï¼Œå®šæœŸæˆ–åœ¨æ¶ˆæ¯å¤„ç†æˆåŠŸåæ›´æ–°åˆ°Broker
            - æ‹‰å–å¤±è´¥ä¼šè®°å½•æ—¥å¿—å¹¶ç­‰å¾…é‡è¯•ï¼Œä¸ä¼šå½±å“å…¶ä»–é˜Ÿåˆ—
            - æ¶ˆè´¹è€…åœæ­¢æ—¶æ­¤å¾ªç¯ä¼šè‡ªåŠ¨é€€å‡º
            - æ”¯æŒé€šè¿‡é…ç½®æ§åˆ¶æ‹‰å–é¢‘ç‡
        """
        suggest_broker_id = 0
        while self._is_running:
            pq: AsyncProcessQueue = await self._get_or_create_process_queue(
                message_queue
            )
            if pq.need_flow_control():
                await asyncio.sleep(3.0)
                continue
            try:
                # æ‰§è¡Œå•æ¬¡æ‹‰å–æ“ä½œ
                pull_result: (
                    tuple[list[MessageExt], int, int] | None
                ) = await self._perform_single_pull(message_queue, suggest_broker_id)

                await pq.update_pull_timestamp()

                if pull_result is None:
                    # å¦‚æœè¿”å›Noneï¼Œè¯´æ˜æ²¡æœ‰è®¢é˜…ä¿¡æ¯ï¼Œåœæ­¢æ¶ˆè´¹
                    logger.warning(
                        "No subscription found for topic, stopping pull loop",
                        extra={
                            "consumer_group": self._config.consumer_group,
                            "topic": message_queue.topic,
                            "queue_id": message_queue.queue_id,
                        },
                    )
                    break

                messages, next_begin_offset, next_suggest_id = pull_result
                suggest_broker_id = next_suggest_id

                if messages:
                    await self._handle_pulled_messages(
                        message_queue, messages, next_begin_offset
                    )

                # æ§åˆ¶æ‹‰å–é¢‘ç‡ - ä¼ å…¥æ˜¯å¦æœ‰æ¶ˆæ¯çš„æ ‡å¿—
                await self._apply_pull_interval(len(messages) > 0)

            except asyncio.CancelledError:
                logger.info(
                    "Pull messages loop cancelled, shutting down gracefully",
                    extra={
                        "consumer_group": self._config.consumer_group,
                        "topic": message_queue.topic,
                        "queue_id": message_queue.queue_id,
                    },
                )
                return
            except MessagePullError as e:
                logger.warning(
                    "The pull request is illegal",
                    extra={
                        "consumer_group": self._config.consumer_group,
                        "topic": message_queue.topic,
                        "queue_id": message_queue.queue_id,
                        "error": str(e),
                    },
                )
                break
            except Exception as e:
                logger.error(
                    f"Error in pull messages loop for {message_queue}: {e}",
                    extra={
                        "consumer_group": self._config.consumer_group,
                        "topic": message_queue.topic,
                        "queue_id": message_queue.queue_id,
                        "error": str(e),
                    },
                    exc_info=True,
                )

                # æ‹‰å–å¤±è´¥æ—¶ç­‰å¾…ä¸€æ®µæ—¶é—´å†é‡è¯•
                await asyncio.sleep(3.0)

    async def _perform_single_pull(
        self, message_queue: MessageQueue, suggest_broker_id: int = 0
    ) -> tuple[list[MessageExt], int, int] | None:
        """æ‰§è¡Œå•æ¬¡æ¶ˆæ¯æ‹‰å–æ“ä½œã€‚

        Args:
            message_queue: è¦æ‹‰å–æ¶ˆæ¯çš„é˜Ÿåˆ—
            suggest_broker_id: å»ºè®®çš„Broker ID

        Returns:
            tuple[list[MessageExt], int, int] | None:
                - messages: æ‹‰å–åˆ°çš„æ¶ˆæ¯åˆ—è¡¨
                - next_begin_offset: ä¸‹æ¬¡æ‹‰å–çš„èµ·å§‹åç§»é‡
                - next_suggest_id: ä¸‹æ¬¡å»ºè®®çš„Broker ID
            None: å¦‚æœæ²¡æœ‰è®¢é˜…ä¿¡æ¯

        Raises:
            MessagePullError: å½“æ‹‰å–è¯·æ±‚éæ³•æ—¶æŠ›å‡º
        """
        # è·å–å½“å‰åç§»é‡
        current_offset: int = await self._get_or_initialize_offset(message_queue)

        pull_start_time = time.time()
        # æ‹‰å–æ¶ˆæ¯
        messages, next_begin_offset, next_suggest_id = await self._pull_messages(
            message_queue,
            current_offset,
            suggest_broker_id,
        )

        # æ£€æŸ¥è®¢é˜…ä¿¡æ¯
        sub: SubscriptionEntry | None = self._subscription_manager.get_subscription(
            message_queue.topic
        )
        if sub is None:
            # å¦‚æœæ²¡æœ‰è®¢é˜…ä¿¡æ¯ï¼Œåˆ™åœæ­¢æ¶ˆè´¹
            return None

        sub_data: SubscriptionData = sub.subscription_data

        # æ ¹æ®è®¢é˜…ä¿¡æ¯è¿‡æ»¤æ¶ˆæ¯
        if sub_data.tags_set:
            messages = self._filter_messages_by_tags(messages, sub_data.tags_set)

        # è®°å½•æ‹‰å–ç»Ÿè®¡
        pull_rt = int((time.time() - pull_start_time) * 1000)  # è½¬æ¢ä¸ºæ¯«ç§’
        message_count = len(messages)

        self._stats_manager.increase_pull_rt(
            self._config.consumer_group, message_queue.topic, pull_rt
        )
        self._stats_manager.increase_pull_tps(
            self._config.consumer_group, message_queue.topic, message_count
        )

        return messages, next_begin_offset, next_suggest_id

    async def _handle_pulled_messages(
        self,
        message_queue: MessageQueue,
        messages: list[MessageExt],
        next_begin_offset: int,
    ) -> None:
        """å¤„ç†æ‹‰å–åˆ°çš„æ¶ˆæ¯ã€‚

        åŒ…æ‹¬æ›´æ–°åç§»é‡ã€ç¼“å­˜æ¶ˆæ¯ã€åˆ†æ‰¹å¤„ç†ç­‰ã€‚

        Args:
            message_queue: æ¶ˆæ¯é˜Ÿåˆ—
            messages: æ‹‰å–åˆ°çš„æ¶ˆæ¯åˆ—è¡¨
            next_begin_offset: ä¸‹æ¬¡æ‹‰å–çš„èµ·å§‹åç§»é‡
        """
        # æ›´æ–°åç§»é‡
        async with self._assigned_queues_lock:  # ğŸ”ä¿æŠ¤_assigned_queuesè®¿é—®
            self._assigned_queues[message_queue] = next_begin_offset

        # å°†æ¶ˆæ¯æ·»åŠ åˆ°ç¼“å­˜ä¸­ï¼ˆç”¨äºè§£å†³å¹¶å‘åç§»é‡é—®é¢˜ï¼‰
        await self._add_messages_to_cache(message_queue, messages)

        # å°†æ¶ˆæ¯æŒ‰æ‰¹æ¬¡æ”¾å…¥å¤„ç†é˜Ÿåˆ—
        await self._submit_messages_for_processing(message_queue, messages)

        # æ›´æ–°ç»Ÿè®¡ä¿¡æ¯
        self._stats_manager.increase_pull_tps(
            self._config.consumer_group, message_queue.topic, len(messages)
        )

    async def _submit_messages_for_processing(
        self, message_queue: MessageQueue, messages: list[MessageExt]
    ) -> None:
        """å°†æ¶ˆæ¯æŒ‰æ‰¹æ¬¡æäº¤ç»™å¤„ç†é˜Ÿåˆ—"""
        batch_size = self._config.consume_batch_size
        message_count = len(messages)
        batch_count = (message_count + batch_size - 1) // batch_size

        logger.debug(
            f"Splitting {message_count} messages into {batch_count} batches",
            extra={
                "consumer_group": self._config.consumer_group,
                "topic": message_queue.topic,
                "queue_id": message_queue.queue_id,
                "message_count": message_count,
                "batch_size": batch_size,
                "batch_count": batch_count,
            },
        )

        # å°†æ¶ˆæ¯æŒ‰æ‰¹æ¬¡æ”¾å…¥å¤„ç†é˜Ÿåˆ—
        for i in range(0, message_count, batch_size):
            batch_messages = messages[i : i + batch_size]
            await self._process_queue.put((batch_messages, message_queue))

    async def _apply_pull_interval(self, has_messages: bool = True) -> None:
        """åº”ç”¨æ™ºèƒ½æ‹‰å–é—´éš”æ§åˆ¶"""
        if self._config.pull_interval > 0:
            if has_messages:
                logger.debug(
                    "Messages pulled, continuing without interval",
                    extra={"consumer_group": self._config.consumer_group},
                )
            else:
                sleep_time = self._config.pull_interval / 1000.0
                logger.debug(
                    f"No messages pulled, sleeping for {sleep_time}s",
                    extra={
                        "consumer_group": self._config.consumer_group,
                        "sleep_time": sleep_time,
                    },
                )
                await asyncio.sleep(sleep_time)

    async def _build_sys_flag(self, commit_offset: bool) -> int:
        """æ„å»ºç³»ç»Ÿæ ‡å¿—ä½

        æ ¹æ®Goè¯­è¨€å®ç°ï¼š
        - bit 0 (0x1): commitOffset æ ‡å¿—
        - bit 1 (0x2): suspend æ ‡å¿—
        - bit 2 (0x4): subscription æ ‡å¿—
        - bit 3 (0x8): classFilter æ ‡å¿—

        Args:
            commit_offset (bool): æ˜¯å¦æäº¤åç§»é‡

        Returns:
            int: ç³»ç»Ÿæ ‡å¿—ä½
        """
        flag = 0

        if commit_offset:
            flag |= 0x1 << 0  # bit 0: 0x1

        # suspend: always true
        flag |= 0x1 << 1  # bit 1: 0x2

        # subscription: always true
        flag |= 0x1 << 2  # bit 2: 0x4

        # class_filter: always false
        # flag |= 0x1 << 3  # bit 3: 0x8

        return flag

    async def _pull_messages(
        self, message_queue: MessageQueue, offset: int, suggest_id: int
    ) -> tuple[list[MessageExt], int, int]:
        """ä»æŒ‡å®šé˜Ÿåˆ—æ‹‰å–æ¶ˆæ¯ï¼Œæ”¯æŒåç§»é‡ç®¡ç†å’ŒBrokeré€‰æ‹©ã€‚

        è¯¥æ–¹æ³•æ˜¯å¹¶å‘æ¶ˆè´¹è€…çš„æ ¸å¿ƒæ‹‰å–é€»è¾‘ï¼Œè´Ÿè´£ä»RocketMQ Brokeræ‹‰å–æ¶ˆæ¯ï¼Œ
        å¹¶å¤„ç†ç›¸å…³çš„ç³»ç»Ÿæ ‡å¿—ä½å’Œåç§»é‡ç®¡ç†ã€‚æ”¯æŒä¸»å¤‡Brokerçš„æ™ºèƒ½é€‰æ‹©
        å’Œæ•…éšœè½¬ç§»æœºåˆ¶ã€‚

        æ ¸å¿ƒåŠŸèƒ½:
        - é€šè¿‡NameServerManagerè·å–æœ€ä¼˜Brokeråœ°å€
        - æ„å»ºæ‹‰å–è¯·æ±‚çš„ç³»ç»Ÿæ ‡å¿—ä½
        - å¤„ç†commit offsetçš„æäº¤é€»è¾‘
        - æ”¯æŒæ‰¹é‡æ¶ˆæ¯æ‹‰å–ä»¥æé«˜æ•ˆç‡
        - å®Œå–„çš„é”™è¯¯å¤„ç†å’Œé‡è¯•æœºåˆ¶

        æ‹‰å–ç­–ç•¥:
        1. è·å–ç›®æ ‡Brokeråœ°å€ï¼Œä¼˜å…ˆè¿æ¥master
        2. è¯»å–å½“å‰commit offsetï¼ˆå¦‚æœæœ‰ï¼‰
        3. æ„å»ºåŒ…å«commitæ ‡å¿—çš„ç³»ç»Ÿæ ‡å¿—ä½
        4. å‘é€PULL_MESSAGEè¯·æ±‚åˆ°Broker
        5. è§£æå“åº”å¹¶è¿”å›æ¶ˆæ¯åˆ—è¡¨å’Œä¸‹æ¬¡æ‹‰å–ä½ç½®

        è¿”å›å€¼è¯´æ˜:
        - list[MessageExt]: æ‹‰å–åˆ°çš„æ¶ˆæ¯åˆ—è¡¨ï¼Œå¯èƒ½ä¸ºç©º
        - int: ä¸‹ä¸€æ¬¡æ‹‰å–çš„èµ·å§‹åç§»é‡
        - int: å»ºè®®ä¸‹æ¬¡è¿æ¥çš„Broker IDï¼ˆ0=master, å…¶ä»–=slaveï¼‰

        Args:
            message_queue (MessageQueue): ç›®æ ‡æ¶ˆæ¯é˜Ÿåˆ—ï¼ŒåŒ…å«topicã€brokeråç§°ã€é˜Ÿåˆ—IDç­‰ä¿¡æ¯
            offset (int): æœ¬æ¬¡æ‹‰å–çš„èµ·å§‹åç§»é‡ï¼Œä»è¯¥ä½ç½®å¼€å§‹æ‹‰å–æ¶ˆæ¯
            suggest_id (int): å»ºè®®çš„Broker IDï¼Œç”¨äºè¿æ¥é€‰æ‹©ä¼˜åŒ–ï¼Œ
                            é€šå¸¸ä¸ºä¸Šæ¬¡æ‹‰å–æ—¶è¿”å›çš„å»ºè®®ID

        Returns:
            tuple[list[MessageExt], int, int]: ä¸‰å…ƒç»„åŒ…å«ï¼š
                                            - æ¶ˆæ¯åˆ—è¡¨ï¼ˆå¯èƒ½ä¸ºç©ºï¼‰
                                            - ä¸‹æ¬¡æ‹‰å–çš„èµ·å§‹åç§»é‡
                                            - å»ºè®®çš„ä¸‹æ¬¡Broker ID

        Raises:
            MessageConsumeError: å½“æ‹‰å–è¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯æ—¶æŠ›å‡ºï¼ŒåŒ…å«è¯¦ç»†çš„é”™è¯¯ä¿¡æ¯
            ValueError: å½“æ— æ³•æ‰¾åˆ°æŒ‡å®šbrokerçš„åœ°å€æ—¶æŠ›å‡º

        Example:
            ```python
            # æ‹‰å–æ¶ˆæ¯ç¤ºä¾‹
            messages, next_offset, suggested_broker = await consumer._pull_messages(
                message_queue=MessageQueue("test_topic", "broker-a", 0),
                offset=100,
                suggest_id=0
            )

            if messages:
                for msg in messages:
                    print(f"æ¶ˆæ¯å†…å®¹: {msg.body.decode()}")
                print(f"ä¸‹æ¬¡æ‹‰å–åç§»é‡: {next_offset}")
                if suggested_broker != 0:
                    print(f"å»ºè®®ä¸‹æ¬¡è¿æ¥slave broker: {suggested_broker}")
            ```

        Note:
            - è¯¥æ–¹æ³•ä¼šè¢«_pull_messages_loopå¾ªç¯è°ƒç”¨ï¼Œå®ç°æŒç»­çš„æ¶ˆæ¯æ‹‰å–
            - suggest_idå‚æ•°ç”¨äºBrokeré€‰æ‹©çš„ä¼˜åŒ–ï¼Œé€šå¸¸æ¥è‡ªä¸Šæ¬¡æ‹‰å–å“åº”
            - commit_offsetåªåœ¨è¿æ¥master brokerä¸”å­˜åœ¨å·²æäº¤åç§»é‡æ—¶ä½¿ç”¨
            - æ‹‰å–å¤±è´¥æ—¶ä¼šè®°å½•è¯¦ç»†çš„é”™è¯¯ä¿¡æ¯ï¼Œä¾¿äºé—®é¢˜è¯Šæ–­
            - è¿”å›çš„emptyæ¶ˆæ¯åˆ—è¡¨å¹¶ä¸æ„å‘³ç€é˜Ÿåˆ—ä¸­æ²¡æœ‰æ¶ˆæ¯ï¼Œå¯èƒ½æ˜¯ç”±äºç½‘ç»œå»¶è¿Ÿ
        """
        try:
            broker_info: (
                tuple[str, bool] | None
            ) = await self._nameserver_manager.get_broker_address_in_subscription(
                message_queue.broker_name, suggest_id
            )
            if not broker_info:
                raise ValueError(
                    f"Broker address not found for {message_queue.broker_name}"
                )

            commit_offset: int = await self._offset_store.read_offset(
                message_queue, ReadOffsetType.READ_FROM_MEMORY
            )

            broker_address, is_master = broker_info

            # ä½¿ç”¨BrokerManageræ‹‰å–æ¶ˆæ¯
            pool: AsyncConnectionPool = await self._broker_manager.must_connection_pool(
                broker_address
            )

            # æ³¨å†Œå¼‚æ­¥è¯·æ±‚å¤„ç†å™¨
            await pool.register_request_processor(
                RequestCode.NOTIFY_CONSUMER_IDS_CHANGED,
                self._on_notify_consumer_ids_changed,
            )
            await pool.register_request_processor(
                RequestCode.CONSUME_MESSAGE_DIRECTLY,
                self._on_notify_consume_message_directly,
            )

            async with pool.get_connection(usage="pull_message") as conn:
                broker_client = AsyncBrokerClient(conn)
                result: PullMessageResult = await broker_client.pull_message(
                    consumer_group=self._config.consumer_group,
                    topic=message_queue.topic,
                    queue_id=message_queue.queue_id,
                    queue_offset=offset,
                    max_msg_nums=self._config.pull_batch_size,
                    sys_flag=await self._build_sys_flag(
                        commit_offset=commit_offset > 0 and is_master
                    ),
                    commit_offset=commit_offset,
                    timeout=30,
                )

                if result.messages:
                    return (
                        result.messages,
                        result.next_begin_offset,
                        result.suggest_which_broker_id or 0,
                    )

                return [], offset, 0

        except MessagePullError as e:
            logger.warning(
                "The pull request is illegal",
                extra={
                    "consumer_group": self._config.consumer_group,
                    "topic": message_queue.topic,
                    "queue_id": message_queue.queue_id,
                    "offset": offset,
                    "error": str(e),
                },
            )
            raise e

        except Exception as e:
            logger.warning(
                "Failed to pull messages",
                extra={
                    "consumer_group": self._config.consumer_group,
                    "topic": message_queue.topic,
                    "queue_id": message_queue.queue_id,
                    "offset": offset,
                    "error": str(e),
                },
            )
            raise MessageConsumeError(
                message_queue.topic,
                "Failed to pull messages",
                offset,
                cause=e,
            ) from e

    # ==================== é˜Ÿåˆ—ç®¡ç†æ¨¡å— ====================
    # åŠŸèƒ½ï¼šè´Ÿè´£é˜Ÿåˆ—åˆ†é…ã€é‡å¹³è¡¡ã€Brokerå‘ç°ç­‰é˜Ÿåˆ—ç®¡ç†åŠŸèƒ½
    # åŒ…å«ï¼šé˜Ÿåˆ—é‡å¹³è¡¡ã€åˆ†é…ç®—æ³•ã€Brokeråœ°å€å‘ç°ç­‰
    # ä½œç”¨ï¼šç¡®ä¿é˜Ÿåˆ—åœ¨æ¶ˆè´¹è€…ä¹‹é—´æ­£ç¡®åˆ†é…ï¼Œå®ç°è´Ÿè½½å‡è¡¡å’Œæ•…éšœè½¬ç§»

    async def _collect_and_allocate_queues(self) -> set[MessageQueue]:
        """æ”¶é›†æ‰€æœ‰Topicçš„å¯ç”¨é˜Ÿåˆ—å¹¶æ‰§è¡Œåˆ†é…ã€‚

        éå†æ‰€æœ‰è®¢é˜…çš„Topicï¼Œè·å–æ¯ä¸ªTopicçš„å¯ç”¨é˜Ÿåˆ—ï¼Œ
        å¹¶ä¸ºæ¯ä¸ªTopicæ‰§è¡Œé˜Ÿåˆ—åˆ†é…ç®—æ³•ã€‚

        Returns:
            set[MessageQueue]: åˆ†é…ç»™å½“å‰æ¶ˆè´¹è€…çš„æ‰€æœ‰é˜Ÿåˆ—é›†åˆ

        Raises:
            Exception: è·¯ç”±ä¿¡æ¯æ›´æ–°æˆ–é˜Ÿåˆ—åˆ†é…å¤±è´¥æ—¶æŠ›å‡ºå¼‚å¸¸
        """
        allocated_queues: set[MessageQueue] = set()
        topics = self._subscription_manager.get_topics()

        for topic in topics:
            try:
                # å¼‚æ­¥æ›´æ–°Topicè·¯ç”±ä¿¡æ¯
                _ = await self._update_route_info(topic)

                # è·å–Topicçš„æ‰€æœ‰å¯ç”¨é˜Ÿåˆ—
                all_queues: list[MessageQueue] = [
                    x
                    for (x, _) in self._topic_broker_mapping.get_subscribe_queues(topic)
                ]

                if not all_queues:
                    logger.debug(
                        "No queues available for subscribed topic",
                        extra={"topic": topic},
                    )
                    continue

                # ä¸ºå•ä¸ªTopicæ‰§è¡Œé˜Ÿåˆ—åˆ†é…ï¼ˆå†…éƒ¨ä¼šè·å–è¯¥topicçš„æ¶ˆè´¹è€…åˆ—è¡¨ï¼‰
                topic_allocated_queues = await self._allocate_queues(topic, all_queues)
                allocated_queues.update(topic_allocated_queues)

                logger.debug(
                    "Topic queue allocation completed",
                    extra={
                        "topic": topic,
                        "total_queues": len(all_queues),
                        "allocated_queues": len(topic_allocated_queues),
                    },
                )

            except Exception as e:
                logger.warning(
                    f"Failed to allocate queues for topic {topic}: {e}",
                    extra={"topic": topic, "error": str(e)},
                )
                # ç»§ç»­å¤„ç†å…¶ä»–Topicï¼Œä¸ä¸­æ–­æ•´ä¸ªé‡å¹³è¡¡è¿‡ç¨‹
                continue

        return allocated_queues

    async def _finalize_rebalance(self, new_assigned_queues: set[MessageQueue]) -> None:
        """å®Œæˆé‡å¹³è¡¡åå¤„ç†ã€‚

        æ›´æ–°ç»Ÿè®¡ä¿¡æ¯ï¼Œå¹¶è®°å½•å®Œæˆæ—¥å¿—ã€‚

        Args:
            new_assigned_queues: æ–°åˆ†é…çš„é˜Ÿåˆ—é›†åˆ

        Raises:
            None: æ­¤æ–¹æ³•ä¸ä¼šæŠ›å‡ºå¼‚å¸¸
        """
        # è·å–æ‰€æœ‰è®¢é˜…ä¸»é¢˜
        topic_set = set(self._subscription_manager.get_topics())

        logger.info(
            "Rebalance completed",
            extra={
                "consumer_group": self._config.consumer_group,
                "assigned_queue_count": len(new_assigned_queues),
                "topics": list(topic_set),
            },
        )

    async def _do_rebalance(self) -> None:
        """æ‰§è¡Œé‡å¹³è¡¡æ“ä½œã€‚

        æ ¹æ®å½“å‰è®¢é˜…ä¿¡æ¯å’Œæ¶ˆè´¹è€…ç»„ä¸­çš„æ‰€æœ‰æ¶ˆè´¹è€…ï¼Œé‡æ–°åˆ†é…é˜Ÿåˆ—ã€‚

        æ‰§è¡Œæµç¨‹ï¼š
        1. æ‰§è¡Œé‡å¹³è¡¡å‰ç½®æ£€æŸ¥
        2. æŸ¥æ‰¾å½“å‰æ¶ˆè´¹è€…ç»„ä¸­çš„æ‰€æœ‰æ¶ˆè´¹è€…
        3. æ”¶é›†æ‰€æœ‰Topicçš„å¯ç”¨é˜Ÿåˆ—å¹¶æ‰§è¡Œåˆ†é…
        4. æ›´æ–°åˆ†é…çš„é˜Ÿåˆ—
        5. å®Œæˆé‡å¹³è¡¡åå¤„ç†
        """
        # å‰ç½®æ£€æŸ¥ï¼ˆåŒ…å«é”è·å–ï¼‰
        lock_acquired = await self._pre_rebalance_check()
        if not lock_acquired:
            return

        try:
            await self._rebalance_lock.acquire()
            logger.debug(
                "Starting rebalance",
                extra={"consumer_group": self._config.consumer_group},
            )

            # æ”¶é›†æ‰€æœ‰å¯ç”¨é˜Ÿåˆ—å¹¶æ‰§è¡Œåˆ†é…
            new_assigned_queues = await self._collect_and_allocate_queues()

            # æ›´æ–°åˆ†é…çš„é˜Ÿåˆ—
            await self._update_assigned_queues(new_assigned_queues)

            # å®Œæˆé‡å¹³è¡¡å¤„ç†
            await self._finalize_rebalance(new_assigned_queues)

        except Exception as e:
            logger.error(
                f"Rebalance failed: {e}",
                extra={"consumer_group": self._config.consumer_group, "error": str(e)},
                exc_info=True,
            )

        finally:
            # åªæœ‰æˆåŠŸè·å–é”æ—¶æ‰é‡Šæ”¾é”
            if self._rebalance_lock.locked():
                try:
                    self._rebalance_lock.release()
                    logger.debug(
                        "Rebalance lock released",
                        extra={"consumer_group": self._config.consumer_group},
                    )
                except RuntimeError as e:
                    # é˜²æ­¢å°è¯•é‡Šæ”¾æœªè¢«æŒæœ‰çš„é”
                    logger.warning(
                        f"Failed to release rebalance lock: {e}",
                        extra={"consumer_group": self._config.consumer_group},
                    )

    async def _pre_rebalance_check(self) -> bool:
        """æ‰§è¡Œé‡å¹³è¡¡å‰ç½®æ£€æŸ¥ã€‚

        æ£€æŸ¥æ˜¯å¦å¯ä»¥æ‰§è¡Œé‡å¹³è¡¡æ“ä½œï¼ŒåŒ…æ‹¬é”è·å–å’Œè®¢é˜…çŠ¶æ€æ£€æŸ¥ã€‚

        Returns:
            bool: å¦‚æœå¯ä»¥æ‰§è¡Œé‡å¹³è¡¡è¿”å›Trueï¼Œå¦åˆ™è¿”å›False

        Raises:
            None: æ­¤æ–¹æ³•ä¸ä¼šæŠ›å‡ºå¼‚å¸¸
        """
        # é¦–å…ˆæ£€æŸ¥æ˜¯å¦æœ‰è®¢é˜…çš„Topicï¼Œé¿å…ä¸å¿…è¦çš„é”è·å–
        topics: set[str] = set(self._subscription_manager.get_topics())
        if not topics:
            logger.debug("No topics subscribed, skipping rebalance")
            return False

        # æ£€æŸ¥é”æ˜¯å¦å·²è¢«å ç”¨ï¼Œé¿å…é‡å¤å°è¯•è·å–
        if self._rebalance_lock.locked():
            logger.debug(
                "Rebalance lock already locked, skipping",
                extra={
                    "consumer_group": self._config.consumer_group,
                },
            )
            return False
        return True

    async def _allocate_queues(
        self, topic: str, all_queues: list[MessageQueue]
    ) -> set[MessageQueue]:
        """ä¸ºå•ä¸ªTopicæ‰§è¡Œé˜Ÿåˆ—åˆ†é…ã€‚

        Args:
            topic: ä¸»é¢˜åç§°
            all_queues: è¯¥ä¸»é¢˜çš„æ‰€æœ‰å¯ç”¨é˜Ÿåˆ—åˆ—è¡¨

        Returns:
            set[MessageQueue]: åˆ†é…ç»™å½“å‰æ¶ˆè´¹è€…çš„è¯¥ä¸»é¢˜é˜Ÿåˆ—é›†åˆ
        """
        if self._config.message_model == MessageModel.CLUSTERING:
            # å¼‚æ­¥è·å–è®¢é˜…è¯¥Topicçš„æ‰€æœ‰æ¶ˆè´¹è€…IDåˆ—è¡¨
            consumer_ids = await self._find_consumer_list(topic)
            if not consumer_ids:
                return set()

            return set(
                self._allocate_strategy.allocate(
                    AllocateContext(
                        self._config.consumer_group,
                        self._config.client_id,
                        consumer_ids,
                        all_queues,
                        {},
                    )
                )
            )
        else:
            # å¹¿æ’­æ¨¡å¼ï¼šè¿”å›æ‰€æœ‰é˜Ÿåˆ—
            return set(all_queues)

    async def _find_consumer_list(self, topic: str) -> list[str]:
        """
        æŸ¥æ‰¾æ¶ˆè´¹è€…åˆ—è¡¨

        Args:
            topic: ä¸»é¢˜åç§°

        Returns:
            æ¶ˆè´¹è€…åˆ—è¡¨
        """
        addresses: list[str] = await self._nameserver_manager.get_all_broker_addresses(
            topic
        )
        if not addresses:
            logger.warning(
                "No broker addresses found for topic", extra={"topic": topic}
            )
            return []

        pool: AsyncConnectionPool = await self._broker_manager.must_connection_pool(
            addresses[0]
        )
        async with pool.get_connection(usage="æŸ¥æ‰¾æ¶ˆè´¹è€…åˆ—è¡¨") as conn:
            return await AsyncBrokerClient(conn).get_consumers_by_group(
                self._config.consumer_group
            )

    async def _update_assigned_queues(
        self, new_assigned_queues: set[MessageQueue]
    ) -> None:
        """æ›´æ–°åˆ†é…çš„é˜Ÿåˆ—ã€‚

        Args:
            new_assigned_queues: æ–°åˆ†é…çš„é˜Ÿåˆ—é›†åˆ
        """
        # ä½¿ç”¨_assigned_queues_lockä¿æŠ¤æ•´ä¸ªé˜Ÿåˆ—æ›´æ–°è¿‡ç¨‹
        async with self._assigned_queues_lock:  # ğŸ”ä¿æŠ¤_assigned_queuesçš„å®Œæ•´æ“ä½œ
            # è®°å½•æ—§é˜Ÿåˆ—é›†åˆ
            old_queues = set(self._assigned_queues.keys())

            # ç§»é™¤æ—§é˜Ÿåˆ—çš„åç§»é‡ä¿¡æ¯
            removed_queues = old_queues - new_assigned_queues
            for queue in removed_queues:
                self._assigned_queues.pop(queue, None)

            # åˆå§‹åŒ–æ–°åˆ†é…é˜Ÿåˆ—çš„åç§»é‡
            added_queues = new_assigned_queues - old_queues
            for queue in added_queues:
                self._assigned_queues[queue] = 0  # åˆå§‹åŒ–åç§»é‡ä¸º0ï¼Œåç»­ä¼šæ›´æ–°

        # åœ¨é”å¤–å¤„ç†æ‹‰å–ä»»åŠ¡çš„å¯åœï¼Œé¿å…æ­»é”
        # åœæ­¢å·²ç§»é™¤é˜Ÿåˆ—çš„æ‹‰å–ä»»åŠ¡
        for queue in removed_queues:
            task = self._pull_tasks.pop(queue, None)
            if task and not task.done():
                task.cancel()
                try:
                    await task
                except asyncio.CancelledError:
                    pass

        # å¦‚æœæ¶ˆè´¹è€…æ­£åœ¨è¿è¡Œï¼Œå¯åŠ¨æ–°é˜Ÿåˆ—çš„æ‹‰å–ä»»åŠ¡
        if self._is_running and added_queues:
            await self._start_pull_tasks_for_queues(added_queues)

    async def _trigger_rebalance(self) -> None:
        """è§¦å‘é‡å¹³è¡¡"""
        if self._is_running:
            # å”¤é†’é‡å¹³è¡¡å¾ªç¯ï¼Œä½¿å…¶ç«‹å³æ‰§è¡Œé‡å¹³è¡¡
            self._rebalance_event.set()

    # ==================== æ¶ˆæ¯å¤„ç†æ¨¡å— ====================
    # åŠŸèƒ½ï¼šè´Ÿè´£æ¶ˆæ¯çš„æ¶ˆè´¹å¤„ç†ï¼ŒåŒ…æ‹¬æ¶ˆæ¯ç›‘å¬ã€æ¶ˆè´¹ç»“æœå¤„ç†ã€é‡è¯•æœºåˆ¶ç­‰
    # åŒ…å«ï¼šæ¶ˆæ¯å¤„ç†å¾ªç¯ã€æˆåŠŸå¤±è´¥å¤„ç†ã€ç»Ÿè®¡æ›´æ–°ç­‰
    # ä½œç”¨ï¼šå®é™…æ‰§è¡Œæ¶ˆæ¯ä¸šåŠ¡é€»è¾‘ï¼Œå¤„ç†æ¶ˆè´¹æˆåŠŸå’Œå¤±è´¥çš„åç»­æ“ä½œ

    async def _consume_messages_loop(self) -> None:
        """
        æŒç»­å¤„ç†æ¶ˆæ¯çš„å¼‚æ­¥æ¶ˆè´¹å¾ªç¯ã€‚

        è¿™æ˜¯å¹¶å‘æ¶ˆè´¹è€…æ ¸å¿ƒçš„å¼‚æ­¥æ¶ˆæ¯å¤„ç†å¾ªç¯ï¼Œè¿è¡Œåœ¨ç‹¬ç«‹çš„æ¶ˆè´¹ä»»åŠ¡ä¸­ã€‚
        è¯¥å¾ªç¯è´Ÿè´£ä»æ¶ˆæ¯å¤„ç†é˜Ÿåˆ—ä¸­è·å–æ¶ˆæ¯å¹¶è°ƒç”¨ç”¨æˆ·æ³¨å†Œçš„æ¶ˆæ¯ç›‘å¬å™¨è¿›è¡Œå¤„ç†ã€‚

        ä¸»è¦åŠŸèƒ½:
            - å¼‚æ­¥ä»å¤„ç†é˜Ÿåˆ—é˜»å¡å¼è·å–æ¶ˆæ¯æ‰¹æ¬¡
            - è°ƒç”¨ç”¨æˆ·æ¶ˆæ¯ç›‘å¬å™¨è¿›è¡Œä¸šåŠ¡å¤„ç†
            - æ ¹æ®å¤„ç†ç»“æœæ‰§è¡ŒæˆåŠŸ/å¤±è´¥åçš„å¤„ç†é€»è¾‘
            - æ›´æ–°æ¶ˆè´¹ç»Ÿè®¡ä¿¡æ¯
            - å¤„ç†é‡è¯•æœºåˆ¶å’Œå¼‚å¸¸æƒ…å†µ

        å¤„ç†æµç¨‹:
            1. ä»å¤„ç†é˜Ÿåˆ—è·å–æ¶ˆæ¯æ‰¹æ¬¡ï¼ˆä½¿ç”¨asyncio.wait_forè¶…æ—¶æœºåˆ¶ï¼‰
            2. è°ƒç”¨æ¶ˆæ¯ç›‘å¬å™¨å¤„ç†æ¶ˆæ¯ (_concurrent_consume_message)
            3. æ ¹æ®å¤„ç†ç»“æœæ‰§è¡Œåç»­æ“ä½œï¼š
               - æˆåŠŸ: æ›´æ–°åç§»é‡ï¼Œæ¸…ç†å¤„ç†é˜Ÿåˆ—
               - å¤±è´¥: å‘é€å›brokerè¿›è¡Œé‡è¯•
            4. æ›´æ–°æ¶ˆè´¹ç»Ÿè®¡ä¿¡æ¯
            5. å¯¹å‘é€å›brokerå¤±è´¥çš„æ¶ˆæ¯è¿›è¡Œå»¶è¿Ÿé‡è¯•

        é‡è¯•æœºåˆ¶:
            - æ¶ˆè´¹å¤±è´¥çš„æ¶ˆæ¯ä¼šå°è¯•å‘é€å›brokerè¿›è¡Œé‡è¯•
            - å‘é€å›brokerå¤±è´¥çš„æ¶ˆæ¯ä¼šåœ¨æœ¬åœ°å»¶è¿Ÿ5ç§’åé‡è¯•
            - é¿å…å› brokerè¿æ¥é—®é¢˜å¯¼è‡´çš„æ¶ˆæ¯ä¸¢å¤±

        å¼‚æ­¥ç‰¹æ€§:
            - ä½¿ç”¨asyncio.wait_forå®ç°éé˜»å¡è¶…æ—¶è·å–
            - æ‰€æœ‰IOæ“ä½œéƒ½æ˜¯å¼‚æ­¥çš„ï¼Œä¸é˜»å¡äº‹ä»¶å¾ªç¯
            - æ”¯æŒé«˜å¹¶å‘æ¶ˆæ¯å¤„ç†
            - å¼‚æ­¥é”ä¿æŠ¤å…±äº«çŠ¶æ€

        å¼‚å¸¸å¤„ç†:
            - æ•è·æ‰€æœ‰å¼‚å¸¸ï¼Œç¡®ä¿å•ä¸ªæ¶ˆæ¯å¤„ç†å¤±è´¥ä¸å½±å“æ•´ä¸ªå¾ªç¯
            - è®°å½•è¯¦ç»†çš„é”™è¯¯æ—¥å¿—å’Œå¼‚å¸¸å †æ ˆä¿¡æ¯
            - ä¿æŒå¾ªç¯ç»§ç»­è¿è¡Œï¼Œç¡®ä¿æ¶ˆè´¹è€…æœåŠ¡å¯ç”¨

        Note:
            - è¯¥æ–¹æ³•æ˜¯æ¶ˆè´¹è€…æ¶ˆæ¯å¤„ç†çš„æ ¸å¿ƒé€»è¾‘ï¼Œä¸åº”è¢«å¤–éƒ¨è°ƒç”¨
            - å¾ªç¯ä¼šåœ¨æ¶ˆè´¹è€…å…³é—­æ—¶(_is_running=False)è‡ªåŠ¨é€€å‡º
            - æ¶ˆè´¹å»¶è¿Ÿä¸»è¦å–å†³äºæ¶ˆæ¯å¤„ç†æ—¶é—´å’Œé˜Ÿåˆ—æ·±åº¦
            - ç»Ÿè®¡ä¿¡æ¯ç”¨äºç›‘æ§æ¶ˆè´¹æ€§èƒ½å’Œå¥åº·çŠ¶æ€
        """
        while self._is_running:
            try:
                # ä»å¤„ç†é˜Ÿåˆ—è·å–æ¶ˆæ¯
                try:
                    messages: list[MessageExt]
                    message_queue: MessageQueue
                    messages, message_queue = await asyncio.wait_for(
                        self._process_queue.get(), timeout=1.0
                    )
                except asyncio.TimeoutError:
                    continue

                while messages:
                    # å¤„ç†æ¶ˆæ¯
                    success: bool = await self._process_messages_with_timing(
                        messages, message_queue
                    )

                    # æ ¹æ®å¤„ç†ç»“æœè¿›è¡Œåç»­å¤„ç†
                    if success:
                        await self._handle_successful_consume(messages, message_queue)
                        messages = []
                    else:
                        messages = await self._handle_failed_consume(
                            messages, message_queue
                        )
                        if messages:
                            await asyncio.sleep(5)  # å¼‚æ­¥å»¶è¿Ÿ
                            continue  # è·³è¿‡åç»­å¤„ç†ï¼Œç›´æ¥é‡è¯•

                    # æ›´æ–°ç»Ÿè®¡ä¿¡æ¯

            except Exception as e:
                logger.error(
                    f"Error in async consume messages loop: {e}",
                    extra={
                        "consumer_group": self._config.consumer_group,
                        "error": str(e),
                    },
                    exc_info=True,
                )

    async def _process_messages_with_timing(
        self, messages: list[MessageExt], message_queue: MessageQueue
    ) -> bool:
        """
        å¼‚æ­¥å¤„ç†æ¶ˆæ¯å¹¶è®¡æ—¶

        Args:
            messages: è¦å¤„ç†çš„æ¶ˆæ¯åˆ—è¡¨
            message_queue: æ¶ˆæ¯é˜Ÿåˆ—ä¿¡æ¯

        Returns:
            bool: å¤„ç†æ˜¯å¦æˆåŠŸ
        """
        start_time = time.time()
        try:
            success = await self._concurrent_consume_message(messages, message_queue)
            duration = time.time() - start_time

            # è®°å½•æ¶ˆè´¹ç»Ÿè®¡
            consume_rt = int(duration * 1000)  # è½¬æ¢ä¸ºæ¯«ç§’
            message_count = len(messages)

            self._stats_manager.increase_consume_rt(
                self._config.consumer_group, message_queue.topic, consume_rt
            )

            if success:
                self._stats_manager.increase_consume_ok_tps(
                    self._config.consumer_group, message_queue.topic, message_count
                )
            else:
                self._stats_manager.increase_consume_failed_tps(
                    self._config.consumer_group, message_queue.topic, message_count
                )

            return success
        except Exception as e:
            logger.error(f"Message processing failed: {e}", exc_info=True)
            duration = time.time() - start_time

            # è®°å½•å¤±è´¥ç»Ÿè®¡
            consume_rt = int(duration * 1000)
            message_count = len(messages)
            self._stats_manager.increase_consume_rt(
                self._config.consumer_group, message_queue.topic, consume_rt
            )
            self._stats_manager.increase_consume_failed_tps(
                self._config.consumer_group, message_queue.topic, message_count
            )

            return False

    async def _handle_successful_consume(
        self, messages: list[MessageExt], message_queue: MessageQueue
    ) -> None:
        """
        å¼‚æ­¥å¤„ç†æ¶ˆè´¹æˆåŠŸçš„æƒ…å†µ

        Args:
            messages: æˆåŠŸå¤„ç†çš„æ¶ˆæ¯åˆ—è¡¨
            message_queue: æ¶ˆæ¯é˜Ÿåˆ—ä¿¡æ¯
        """
        try:
            # ä»ç¼“å­˜ä¸­ç§»é™¤å·²å¤„ç†çš„æ¶ˆæ¯ï¼Œå¹¶è·å–å½“å‰æœ€å°offset
            min_offset: int | None = await self._remove_messages_from_cache(
                message_queue, messages
            )

            # ç›´æ¥æ›´æ–°æœ€å°offsetåˆ°offset_storeï¼Œé¿å…é‡å¤æŸ¥è¯¢
            if min_offset is not None:
                try:
                    await self._offset_store.update_offset(message_queue, min_offset)
                    logger.debug(
                        f"Updated offset from cache: {min_offset}",
                        extra={
                            "consumer_group": self._config.consumer_group,
                            "topic": message_queue.topic,
                            "queue_id": message_queue.queue_id,
                            "offset": min_offset,
                            "cache_stats": (
                                await self._get_or_create_process_queue(message_queue)
                            ).get_stats(),
                        },
                    )
                except Exception as e:
                    logger.warning(
                        f"Failed to update offset from cache: {e}",
                        extra={
                            "consumer_group": self._config.consumer_group,
                            "topic": message_queue.topic,
                            "queue_id": message_queue.queue_id,
                            "offset": min_offset,
                            "error": str(e),
                        },
                    )
        except Exception as e:
            logger.warning(
                f"Failed to remove messages from cache: {e}",
                extra={
                    "consumer_group": self._config.consumer_group,
                    "topic": message_queue.topic,
                    "queue_id": message_queue.queue_id,
                    "error": str(e),
                },
            )

    async def _handle_failed_consume(
        self, messages: list[MessageExt], message_queue: MessageQueue
    ) -> list[MessageExt]:
        """
        å¼‚æ­¥å¤„ç†æ¶ˆè´¹å¤±è´¥çš„æ¶ˆæ¯ï¼Œæ ¹æ®æ¶ˆæ¯æ¨¡å¼é‡‡å–ä¸åŒçš„å¤„ç†ç­–ç•¥ã€‚

        Args:
            messages: æ¶ˆè´¹å¤±è´¥çš„æ¶ˆæ¯åˆ—è¡¨
            message_queue: æ¶ˆæ¯é˜Ÿåˆ—ä¿¡æ¯

        Returns:
            list[MessageExt]: å‘é€å›brokerå¤±è´¥çš„æ¶ˆæ¯åˆ—è¡¨ï¼ˆé›†ç¾¤æ¨¡å¼ï¼‰æˆ–ç©ºåˆ—è¡¨ï¼ˆå¹¿æ’­æ¨¡å¼ï¼‰
        """
        if self._config.message_model == MessageModel.CLUSTERING:
            # å°è¯•å°†å¤±è´¥çš„æ¶ˆæ¯å‘é€å›brokerè¿›è¡Œé‡è¯•
            failed_messages: list[MessageExt] = []
            for msg in messages:
                if not await self._send_back_message(message_queue, msg):
                    failed_messages.append(msg)
            return failed_messages
        else:
            # å¹¿æ’­æ¨¡å¼ï¼Œç›´æ¥ä¸¢æ‰æ¶ˆæ¯
            logger.warning("Broadcast mode, discard failed messages")
            return []

    # ==================== åç§»é‡ç®¡ç†æ¨¡å— ====================
    # åŠŸèƒ½ï¼šè´Ÿè´£æ¶ˆæ¯åç§»é‡çš„ç®¡ç†ï¼ŒåŒ…æ‹¬åˆå§‹åŒ–ã€è¯»å–ã€æ›´æ–°å’Œç¼“å­˜æ“ä½œ
    # åŒ…å«ï¼šåç§»é‡åˆå§‹åŒ–ç­–ç•¥ã€ç¼“å­˜ç®¡ç†ã€è¿œç¨‹å­˜å‚¨ç­‰
    # ä½œç”¨ï¼šç¡®ä¿æ¶ˆæ¯æ¶ˆè´¹ä½ç½®çš„å‡†ç¡®æ€§ï¼Œæ”¯æŒæ¶ˆæ¯çš„å¯é æ¶ˆè´¹å’Œæ•…éšœæ¢å¤

    async def _get_or_create_process_queue(
        self, queue: MessageQueue
    ) -> AsyncProcessQueue:
        """è·å–æˆ–åˆ›å»ºæŒ‡å®šé˜Ÿåˆ—çš„ProcessQueue"""
        async with self._cache_lock:
            if queue not in self._msg_cache:
                self._msg_cache[queue] = AsyncProcessQueue(
                    max_cache_count=self._config.max_cache_count_per_queue,
                    max_cache_size_mb=self._config.max_cache_size_per_queue,
                )
            return self._msg_cache[queue]

    async def _add_messages_to_cache(
        self, queue: MessageQueue, messages: list[MessageExt]
    ) -> None:
        """å°†æ¶ˆæ¯æ·»åŠ åˆ°ç¼“å­˜"""
        pq = await self._get_or_create_process_queue(queue)
        _ = await pq.add_batch_messages(messages)

    async def _get_or_initialize_offset(self, queue: MessageQueue) -> int:
        """è·å–æˆ–åˆå§‹åŒ–é˜Ÿåˆ—åç§»é‡

        å¦‚æœæœ¬åœ°ç¼“å­˜çš„åç§»é‡ä¸º0ï¼ˆé¦–æ¬¡æ¶ˆè´¹ï¼‰ï¼Œåˆ™æ ¹æ®é…ç½®çš„æ¶ˆè´¹ç­–ç•¥
        ä»ConsumeFromWhereManagerè·å–æ­£ç¡®çš„åˆå§‹åç§»é‡ã€‚

        Args:
            queue: è¦è·å–åç§»é‡çš„æ¶ˆæ¯é˜Ÿåˆ—

        Returns:
            int: æ¶ˆè´¹åç§»é‡
        """
        # å…ˆä»_assigned_queuesä¸­è¯»å–å½“å‰åç§»é‡
        async with self._assigned_queues_lock:  # ğŸ”ä¿æŠ¤_assigned_queuesè®¿é—®
            current_offset = self._assigned_queues.get(queue, 0)

        # å¦‚æœcurrent_offsetä¸º0ï¼ˆé¦–æ¬¡æ¶ˆè´¹ï¼‰ï¼Œåˆ™ä»_consume_from_where_managerä¸­è·å–æ­£ç¡®çš„åˆå§‹åç§»é‡
        if current_offset == 0:
            try:
                current_offset = (
                    await self._consume_from_where_manager.get_consume_offset(
                        queue, self._config.consume_from_where
                    )
                )
                # æ›´æ–°æœ¬åœ°ç¼“å­˜çš„åç§»é‡
                async with self._assigned_queues_lock:  # ğŸ”ä¿æŠ¤_assigned_queuesä¿®æ”¹
                    self._assigned_queues[queue] = current_offset

                logger.info(
                    f"åˆå§‹åŒ–æ¶ˆè´¹åç§»é‡: {current_offset}",
                    extra={
                        "consumer_group": self._config.consumer_group,
                        "topic": queue.topic,
                        "queue_id": queue.queue_id,
                        "strategy": self._config.consume_from_where,
                        "offset": current_offset,
                    },
                )

            except Exception as e:
                logger.error(
                    f"è·å–åˆå§‹æ¶ˆè´¹åç§»é‡å¤±è´¥ï¼Œä½¿ç”¨é»˜è®¤åç§»é‡0: {e}",
                    extra={
                        "consumer_group": self._config.consumer_group,
                        "topic": queue.topic,
                        "queue_id": queue.queue_id,
                        "strategy": self._config.consume_from_where,
                        "error": str(e),
                    },
                    exc_info=True,
                )
                # ä½¿ç”¨é»˜è®¤åç§»é‡0
                current_offset = 0
                async with self._lock:
                    self._assigned_queues[queue] = current_offset

        return current_offset

    # ==================== é€šä¿¡å›è°ƒæ¨¡å— ====================
    # åŠŸèƒ½ï¼šå¤„ç†æ¥è‡ªBrokerçš„å„ç§é€šçŸ¥å’Œå›è°ƒè¯·æ±‚
    # åŒ…å«ï¼šæ¶ˆè´¹è€…IDå˜æ›´é€šçŸ¥ã€ç›´æ¥æ¶ˆè´¹æ¶ˆæ¯é€šçŸ¥ç­‰
    # ä½œç”¨ï¼šå®ç°ä¸Brokerçš„åŒå‘é€šä¿¡ï¼Œå“åº”å„ç§æ§åˆ¶æŒ‡ä»¤å’Œé€šçŸ¥

    async def _on_notify_consumer_ids_changed(
        self, _remoting_cmd: RemotingCommand, _remote_addr: tuple[str, int]
    ) -> None:
        """å¤„ç†æ¶ˆè´¹è€…IDå˜æ›´é€šçŸ¥"""
        logger.info("Received notification of consumer IDs changed")
        await self._do_rebalance()

    async def _on_notify_consume_message_directly(
        self, command: RemotingCommand, _addr: tuple[str, int]
    ) -> RemotingCommand:
        """å¤„ç†ç›´æ¥æ¶ˆè´¹æ¶ˆæ¯é€šçŸ¥"""
        header: ConsumeMessageDirectlyHeader = ConsumeMessageDirectlyHeader.decode(
            command.ext_fields
        )
        if header.client_id == self._config.client_id:
            return await self._on_notify_consume_message_directly_internal(
                header, command
            )
        else:
            return (
                RemotingCommandBuilder(ResponseCode.ERROR)
                .with_remark(f"Can't find client ID {header.client_id}")
                .build()
            )

    async def _on_notify_consume_message_directly_internal(
        self, header: ConsumeMessageDirectlyHeader, command: RemotingCommand
    ) -> RemotingCommand:
        """å†…éƒ¨å¤„ç†ç›´æ¥æ¶ˆè´¹æ¶ˆæ¯çš„é€»è¾‘"""
        if not command.body:
            return (
                RemotingCommandBuilder(ResponseCode.ERROR)
                .with_remark("No message body")
                .build()
            )

        msgs = MessageExt.decode_messages(command.body)
        if len(msgs) == 0:
            return (
                RemotingCommandBuilder(ResponseCode.ERROR)
                .with_remark("No message")
                .build()
            )

        msg: MessageExt = msgs[0]

        q: MessageQueue
        if msg.queue:
            q = MessageQueue(msg.topic, header.broker_name, msg.queue.queue_id)
        else:
            q = MessageQueue(msg.topic, header.broker_name, 0)

        now = datetime.now()

        for msg in msgs:
            self._reset_retry(msg)

        if await self._concurrent_consume_message(msgs, q):
            res: ConsumeMessageDirectlyResult = ConsumeMessageDirectlyResult(
                order=False,
                auto_commit=True,
                consume_result=ConsumeResult.SUCCESS,
                remark="Message consumed",
                spent_time_mills=int((datetime.now() - now).total_seconds() * 1000),
            )
            return (
                RemotingCommandBuilder(ResponseCode.SUCCESS)
                .with_remark("Message consumed")
                .with_body(res.encode())
                .build()
            )
        else:
            return (
                RemotingCommandBuilder(ResponseCode.ERROR)
                .with_remark("Failed to consume message")
                .build()
            )

    # ==================== ç»Ÿè®¡ç›‘æ§æ¨¡å— ====================
    # åŠŸèƒ½ï¼šè´Ÿè´£æ€§èƒ½ç»Ÿè®¡å’Œç›‘æ§ä¿¡æ¯æ”¶é›†ï¼Œæä¾›æ¶ˆè´¹è€…è¿è¡ŒçŠ¶æ€å’Œæ€§èƒ½æŒ‡æ ‡
    # åŒ…å«ï¼šç»Ÿè®¡ä¿¡æ¯æ”¶é›†ã€æ€§èƒ½ç›‘æ§ã€çŠ¶æ€æŠ¥å‘Šç­‰
    # ä½œç”¨ï¼šä¸ºç³»ç»Ÿç›‘æ§ã€æ€§èƒ½è°ƒä¼˜å’Œæ•…éšœè¯Šæ–­æä¾›æ•°æ®æ”¯æŒ

    async def get_stats_manager(self):
        """å¼‚æ­¥è·å–ç»Ÿè®¡ç®¡ç†å™¨"""
        return self._stats_manager

    async def get_consume_status(self, topic: str):
        """å¼‚æ­¥è·å–æŒ‡å®šä¸»é¢˜çš„æ¶ˆè´¹çŠ¶æ€"""
        return self._stats_manager.get_consume_status(
            self._config.consumer_group, topic
        )

    # ==================== èµ„æºç®¡ç†æ¨¡å— ====================
    # åŠŸèƒ½ï¼šè´Ÿè´£èµ„æºçš„åˆ†é…ã€ç®¡ç†å’Œæ¸…ç†ï¼Œç¡®ä¿ç³»ç»Ÿèµ„æºçš„æ­£ç¡®ä½¿ç”¨å’Œé‡Šæ”¾
    # åŒ…å«ï¼šèµ„æºæ¸…ç†ã€å†…å­˜ç®¡ç†ã€ä»»åŠ¡æ¸…ç†ç­‰
    # ä½œç”¨ï¼šé˜²æ­¢èµ„æºæ³„æ¼ï¼Œç¡®ä¿ç³»ç»Ÿçš„ç¨³å®šæ€§å’Œå¯é æ€§

    async def _cleanup_resources(self) -> None:
        """æ¸…ç†èµ„æº"""
        try:
            # æ¸…ç†ç¼“å­˜
            async with self._cache_lock:
                self._msg_cache.clear()

            async with self._assigned_queues_lock:  # ğŸ”ä¿æŠ¤_assigned_queuesæ¸…ç†
                self._assigned_queues.clear()

            # æ¸…ç†ä»»åŠ¡
            self._pull_tasks.clear()

        except Exception as e:
            logger.error(
                f"Error during resource cleanup: {e}",
                extra={"error": str(e)},
                exc_info=True,
            )
        """
        æŒç»­å¤„ç†æ¶ˆæ¯çš„å¼‚æ­¥æ¶ˆè´¹å¾ªç¯ã€‚

        è¿™æ˜¯å¹¶å‘æ¶ˆè´¹è€…æ ¸å¿ƒçš„å¼‚æ­¥æ¶ˆæ¯å¤„ç†å¾ªç¯ï¼Œè¿è¡Œåœ¨ç‹¬ç«‹çš„æ¶ˆè´¹ä»»åŠ¡ä¸­ã€‚
        è¯¥å¾ªç¯è´Ÿè´£ä»æ¶ˆæ¯å¤„ç†é˜Ÿåˆ—ä¸­è·å–æ¶ˆæ¯å¹¶è°ƒç”¨ç”¨æˆ·æ³¨å†Œçš„æ¶ˆæ¯ç›‘å¬å™¨è¿›è¡Œå¤„ç†ã€‚

        ä¸»è¦åŠŸèƒ½:
            - å¼‚æ­¥ä»å¤„ç†é˜Ÿåˆ—é˜»å¡å¼è·å–æ¶ˆæ¯æ‰¹æ¬¡
            - è°ƒç”¨ç”¨æˆ·æ¶ˆæ¯ç›‘å¬å™¨è¿›è¡Œä¸šåŠ¡å¤„ç†
            - æ ¹æ®å¤„ç†ç»“æœæ‰§è¡ŒæˆåŠŸ/å¤±è´¥åçš„å¤„ç†é€»è¾‘
            - æ›´æ–°æ¶ˆè´¹ç»Ÿè®¡ä¿¡æ¯
            - å¤„ç†é‡è¯•æœºåˆ¶å’Œå¼‚å¸¸æƒ…å†µ

        å¤„ç†æµç¨‹:
            1. ä»å¤„ç†é˜Ÿåˆ—è·å–æ¶ˆæ¯æ‰¹æ¬¡ï¼ˆä½¿ç”¨asyncio.wait_forè¶…æ—¶æœºåˆ¶ï¼‰
            2. è°ƒç”¨æ¶ˆæ¯ç›‘å¬å™¨å¤„ç†æ¶ˆæ¯ (_concurrent_consume_message)
            3. æ ¹æ®å¤„ç†ç»“æœæ‰§è¡Œåç»­æ“ä½œï¼š
               - æˆåŠŸ: æ›´æ–°åç§»é‡ï¼Œæ¸…ç†å¤„ç†é˜Ÿåˆ—
               - å¤±è´¥: å‘é€å›brokerè¿›è¡Œé‡è¯•
            4. æ›´æ–°æ¶ˆè´¹ç»Ÿè®¡ä¿¡æ¯
            5. å¯¹å‘é€å›brokerå¤±è´¥çš„æ¶ˆæ¯è¿›è¡Œå»¶è¿Ÿé‡è¯•

        é‡è¯•æœºåˆ¶:
            - æ¶ˆè´¹å¤±è´¥çš„æ¶ˆæ¯ä¼šå°è¯•å‘é€å›brokerè¿›è¡Œé‡è¯•
            - å‘é€å›brokerå¤±è´¥çš„æ¶ˆæ¯ä¼šåœ¨æœ¬åœ°å»¶è¿Ÿ5ç§’åé‡è¯•
            - é¿å…å› brokerè¿æ¥é—®é¢˜å¯¼è‡´çš„æ¶ˆæ¯ä¸¢å¤±

        å¼‚æ­¥ç‰¹æ€§:
            - ä½¿ç”¨asyncio.wait_forå®ç°éé˜»å¡è¶…æ—¶è·å–
            - æ‰€æœ‰IOæ“ä½œéƒ½æ˜¯å¼‚æ­¥çš„ï¼Œä¸é˜»å¡äº‹ä»¶å¾ªç¯
            - æ”¯æŒé«˜å¹¶å‘æ¶ˆæ¯å¤„ç†
            - å¼‚æ­¥é”ä¿æŠ¤å…±äº«çŠ¶æ€

        å¼‚å¸¸å¤„ç†:
            - æ•è·æ‰€æœ‰å¼‚å¸¸ï¼Œç¡®ä¿å•ä¸ªæ¶ˆæ¯å¤„ç†å¤±è´¥ä¸å½±å“æ•´ä¸ªå¾ªç¯
            - è®°å½•è¯¦ç»†çš„é”™è¯¯æ—¥å¿—å’Œå¼‚å¸¸å †æ ˆä¿¡æ¯
            - ä¿æŒå¾ªç¯ç»§ç»­è¿è¡Œï¼Œç¡®ä¿æ¶ˆè´¹è€…æœåŠ¡å¯ç”¨

        Note:
            - è¯¥æ–¹æ³•æ˜¯æ¶ˆè´¹è€…æ¶ˆæ¯å¤„ç†çš„æ ¸å¿ƒé€»è¾‘ï¼Œä¸åº”è¢«å¤–éƒ¨è°ƒç”¨
            - å¾ªç¯ä¼šåœ¨æ¶ˆè´¹è€…å…³é—­æ—¶(_is_running=False)è‡ªåŠ¨é€€å‡º
            - æ¶ˆè´¹å»¶è¿Ÿä¸»è¦å–å†³äºæ¶ˆæ¯å¤„ç†æ—¶é—´å’Œé˜Ÿåˆ—æ·±åº¦
            - ç»Ÿè®¡ä¿¡æ¯ç”¨äºç›‘æ§æ¶ˆè´¹æ€§èƒ½å’Œå¥åº·çŠ¶æ€
        """
        while self._is_running:
            try:
                # ä»å¤„ç†é˜Ÿåˆ—è·å–æ¶ˆæ¯
                try:
                    messages: list[MessageExt]
                    message_queue: MessageQueue
                    messages, message_queue = await asyncio.wait_for(
                        self._process_queue.get(), timeout=1.0
                    )
                except asyncio.TimeoutError:
                    continue

                while messages:
                    # å¤„ç†æ¶ˆæ¯
                    success: bool = await self._process_messages_with_timing(
                        messages, message_queue
                    )

                    # æ ¹æ®å¤„ç†ç»“æœè¿›è¡Œåç»­å¤„ç†
                    if success:
                        await self._handle_successful_consume(messages, message_queue)
                        messages = []
                    else:
                        messages = await self._handle_failed_consume(
                            messages, message_queue
                        )
                        if messages:
                            await asyncio.sleep(5)  # å¼‚æ­¥å»¶è¿Ÿ
                            continue  # è·³è¿‡åç»­å¤„ç†ï¼Œç›´æ¥é‡è¯•

                    # æ›´æ–°ç»Ÿè®¡ä¿¡æ¯

            except Exception as e:
                logger.error(
                    f"Error in async consume messages loop: {e}",
                    extra={
                        "consumer_group": self._config.consumer_group,
                        "error": str(e),
                    },
                    exc_info=True,
                )

    async def _update_offset_from_cache(self, queue: MessageQueue, offset: int) -> None:
        """ä»ç¼“å­˜æ›´æ–°åç§»é‡"""
        async with self._assigned_queues_lock:  # ğŸ”ä¿æŠ¤_assigned_queuesè®¿é—®
            self._assigned_queues[queue] = offset
        await self._offset_store.update_offset(queue, offset)

    async def _remove_messages_from_cache(
        self, queue: MessageQueue, messages: list[MessageExt]
    ) -> int | None:
        """ä»ç¼“å­˜ä¸­ç§»é™¤æ¶ˆæ¯"""
        pq = await self._get_or_create_process_queue(queue)
        _ = await pq.remove_batch_messages(
            [m.queue_offset for m in messages if m.queue_offset is not None]
        )
        return await pq.get_min_offset()
